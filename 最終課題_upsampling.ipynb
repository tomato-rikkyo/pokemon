{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow 2.3でメモリを指定及び節約して使うためのおまじない。\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(physical_devices[9], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[9], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, smart_resize\n",
    "\n",
    "INPUT_SHAPE = (64, 64)\n",
    "\n",
    "data = np.array([])\n",
    "train_path = os.path.abspath(\"pokemon_jpg/\")\n",
    "images = glob(os.path.join(train_path, \"*.*\"))\n",
    "\n",
    "data = np.stack([img_to_array(load_img(img).resize(INPUT_SHAPE)) for img in images]) / 127.5 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout, ZeroPadding2D, UpSampling2D\n",
    "from keras.layers import Layer\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from keras.utils import plot_model\n",
    "from keras.initializers import RandomNormal\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_input_shape = (64, 64, 3)\n",
    "discriminator_conv_filters = [16,32,64,128]\n",
    "discriminator_conv_kernel_size = [5,5,5,5]\n",
    "discriminator_conv_strides = [2,2,2,2]\n",
    "\n",
    "initial_layer_shape = (4, 4, 128)\n",
    "generator_conv_filters = [64,32,16,3]\n",
    "generator_conv_kernel_size = [5,5,5,5]\n",
    "generator_conv_strides = [1,1,1,1]\n",
    "\n",
    "\n",
    "def generator(z_dim, initial_layer_shape, generator_conv_filters, generator_conv_kernel_size, generator_conv_strides):\n",
    "    generator_input = Input(shape=(z_dim,))\n",
    "    x = generator_input\n",
    "    x = Dense(np.prod(initial_layer_shape), kernel_initializer=\"he_normal\")(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    x = Reshape(initial_layer_shape)(x)\n",
    "    for i in range(len(generator_conv_kernel_size)):\n",
    "        x = UpSampling2D()(x)\n",
    "        x = Conv2D(\n",
    "            filters=generator_conv_filters[i],\n",
    "            kernel_size=generator_conv_kernel_size[i],\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=\"he_normal\")(x)\n",
    "        \n",
    "#         x = Conv2DTranspose(\n",
    "#             filters=generator_conv_filters[i],\n",
    "#             kernel_size=generator_conv_kernel_size[i],\n",
    "#             padding=\"same\",\n",
    "#             strides=generator_conv_strides[i],\n",
    "#             kernel_initializer=\"he_normal\")(x)\n",
    "\n",
    "        if i < len(generator_conv_kernel_size) - 1:\n",
    "            x = BatchNormalization(momentum=0.8)(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "        else:\n",
    "            generator_output = Activation(\"tanh\")(x)\n",
    "            \n",
    "    return Model(generator_input, generator_output)\n",
    "\n",
    "def discriminator(discriminator_input_shape, discriminator_conv_filters, discriminator_conv_kernel_size, discriminator_conv_strides):\n",
    "    discriminator_input = Input(discriminator_input_shape)\n",
    "    x = discriminator_input\n",
    "    \n",
    "    for i in range(len(generator_conv_kernel_size)):\n",
    "        x = Conv2D(filters=discriminator_conv_filters[i],\n",
    "                   kernel_size=discriminator_conv_kernel_size[i],\n",
    "                   padding=\"same\",\n",
    "                   strides=discriminator_conv_strides[i],\n",
    "                   kernel_initializer=\"he_normal\")(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    discriminator_output = Dense(1, activation=None, kernel_initializer=\"he_normal\")(x)\n",
    "    \n",
    "    return Model(discriminator_input, discriminator_output)\n",
    "\n",
    "def wasserstein(y_true, y_pred):\n",
    "        return -K.mean(y_true * y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dims = 100\n",
    "\n",
    "disc = discriminator(discriminator_input_shape, discriminator_conv_filters, discriminator_conv_kernel_size, discriminator_conv_strides)\n",
    "\n",
    "disc.compile(optimizer=Adam(lr=0.0002),\n",
    "             loss=wasserstein)\n",
    "\n",
    "disc.trainable = False\n",
    "\n",
    "gen = generator(z_dims, initial_layer_shape, generator_conv_filters, generator_conv_kernel_size, generator_conv_strides)\n",
    "\n",
    "gan_input = Input(shape=(z_dims,))\n",
    "gan_output = disc(gen(gan_input))\n",
    "gan = Model(gan_input, gan_output)\n",
    "gan.compile(optimizer=Adam(lr=0.0002),\n",
    "            loss=wasserstein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 32, 32, 16)        1216      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 32)        12832     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 64)          51264     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 272,289\n",
      "Trainable params: 0\n",
      "Non-trainable params: 272,289\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "disc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              206848    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 64)          204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 16)        12816     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 64, 64, 3)         1203      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 64, 64, 3)         0         \n",
      "=================================================================\n",
      "Total params: 485,603\n",
      "Trainable params: 481,283\n",
      "Non-trainable params: 4,320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "functional_3 (Functional)    (None, 64, 64, 3)         485603    \n",
      "_________________________________________________________________\n",
      "functional_1 (Functional)    (None, 1)                 272289    \n",
      "=================================================================\n",
      "Total params: 757,892\n",
      "Trainable params: 481,283\n",
      "Non-trainable params: 276,609\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtomato-ai\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.12<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">decent-totem-4</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/tomato-ai/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92_%E6%9C%80%E7%B5%82%E8%AA%B2%E9%A1%8C_upsampling_%E9%AB%98%E8%A7%A3%E5%83%8F%E5%BA%A6\" target=\"_blank\">https://wandb.ai/tomato-ai/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92_%E6%9C%80%E7%B5%82%E8%AA%B2%E9%A1%8C_upsampling_%E9%AB%98%E8%A7%A3%E5%83%8F%E5%BA%A6</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/tomato-ai/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92_%E6%9C%80%E7%B5%82%E8%AA%B2%E9%A1%8C_upsampling_%E9%AB%98%E8%A7%A3%E5%83%8F%E5%BA%A6/runs/1zts47ra\" target=\"_blank\">https://wandb.ai/tomato-ai/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92_%E6%9C%80%E7%B5%82%E8%AA%B2%E9%A1%8C_upsampling_%E9%AB%98%E8%A7%A3%E5%83%8F%E5%BA%A6/runs/1zts47ra</a><br/>\n",
       "                Run data is saved locally in <code>/workdir/homework/deep_NN/pokemon/wandb/run-20210123_001438-1zts47ra</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: (-0.000)(R -0.001, F 0.001)]  [G loss: -0.001] \n",
      "0 [D loss: (-0.000)(R -0.001, F 0.001)]  [G loss: -0.001] \n",
      "1 [D loss: (-0.000)(R -0.001, F 0.000)]  [G loss: -0.001] \n",
      "1 [D loss: (-0.001)(R -0.002, F 0.001)]  [G loss: -0.001] \n",
      "2 [D loss: (-0.001)(R -0.002, F 0.001)]  [G loss: -0.001] \n",
      "2 [D loss: (-0.001)(R -0.002, F 0.001)]  [G loss: -0.000] \n",
      "3 [D loss: (-0.001)(R -0.002, F 0.000)]  [G loss: -0.000] \n",
      "3 [D loss: (-0.001)(R -0.002, F -0.000)]  [G loss: 0.000] \n",
      "4 [D loss: (-0.002)(R -0.002, F -0.001)]  [G loss: 0.001] \n",
      "4 [D loss: (-0.002)(R -0.003, F -0.002)]  [G loss: 0.001] \n",
      "5 [D loss: (-0.003)(R -0.003, F -0.003)]  [G loss: 0.002] \n",
      "5 [D loss: (-0.004)(R -0.004, F -0.004)]  [G loss: 0.003] \n",
      "6 [D loss: (-0.005)(R -0.005, F -0.005)]  [G loss: 0.006] \n",
      "6 [D loss: (-0.005)(R -0.005, F -0.005)]  [G loss: 0.009] \n",
      "7 [D loss: (-0.004)(R -0.006, F -0.003)]  [G loss: 0.012] \n",
      "7 [D loss: (-0.002)(R -0.006, F 0.001)]  [G loss: 0.013] \n",
      "8 [D loss: (-0.000)(R -0.007, F 0.006)]  [G loss: 0.011] \n",
      "8 [D loss: (-0.001)(R -0.010, F 0.008)]  [G loss: 0.008] \n",
      "9 [D loss: (-0.000)(R -0.010, F 0.009)]  [G loss: 0.005] \n",
      "9 [D loss: (-0.001)(R -0.011, F 0.008)]  [G loss: 0.003] \n",
      "10 [D loss: (-0.002)(R -0.011, F 0.007)]  [G loss: 0.002] \n",
      "10 [D loss: (-0.004)(R -0.013, F 0.005)]  [G loss: 0.002] \n",
      "11 [D loss: (-0.006)(R -0.015, F 0.003)]  [G loss: 0.004] \n",
      "11 [D loss: (-0.011)(R -0.021, F -0.002)]  [G loss: 0.007] \n",
      "12 [D loss: (-0.021)(R -0.033, F -0.009)]  [G loss: 0.013] \n",
      "12 [D loss: (-0.043)(R -0.070, F -0.016)]  [G loss: 0.020] \n",
      "13 [D loss: (-0.109)(R -0.214, F -0.004)]  [G loss: -0.002] \n",
      "13 [D loss: (-0.310)(R -0.728, F 0.107)]  [G loss: -0.222] \n",
      "14 [D loss: (-1.008)(R -2.494, F 0.479)]  [G loss: -1.247] \n",
      "14 [D loss: (-3.164)(R -7.596, F 1.268)]  [G loss: -4.746] \n",
      "15 [D loss: (-10.042)(R -21.893, F 1.809)]  [G loss: -14.798] \n",
      "15 [D loss: (-20.981)(R -50.542, F 8.581)]  [G loss: -34.538] \n",
      "16 [D loss: (-24.719)(R -81.048, F 31.611)]  [G loss: -56.608] \n",
      "16 [D loss: (-21.713)(R -106.684, F 63.258)]  [G loss: -73.395] \n",
      "17 [D loss: (-9.601)(R -116.101, F 96.898)]  [G loss: -79.741] \n",
      "17 [D loss: (1.353)(R -110.291, F 112.996)]  [G loss: -79.030] \n",
      "18 [D loss: (4.708)(R -103.700, F 113.115)]  [G loss: -69.797] \n",
      "18 [D loss: (8.509)(R -78.336, F 95.354)]  [G loss: -52.260] \n",
      "19 [D loss: (6.193)(R -55.449, F 67.836)]  [G loss: -36.068] \n",
      "19 [D loss: (3.881)(R -39.802, F 47.564)]  [G loss: -24.814] \n",
      "20 [D loss: (2.035)(R -28.831, F 32.901)]  [G loss: -17.012] \n",
      "20 [D loss: (0.137)(R -22.481, F 22.754)]  [G loss: -11.632] \n",
      "21 [D loss: (-0.670)(R -17.308, F 15.968)]  [G loss: -8.148] \n",
      "21 [D loss: (-1.510)(R -14.187, F 11.168)]  [G loss: -5.507] \n",
      "22 [D loss: (-2.419)(R -12.317, F 7.479)]  [G loss: -3.508] \n",
      "22 [D loss: (-3.034)(R -10.991, F 4.922)]  [G loss: -2.217] \n",
      "23 [D loss: (-3.735)(R -10.167, F 2.696)]  [G loss: -1.045] \n",
      "23 [D loss: (-4.489)(R -9.909, F 0.931)]  [G loss: -0.217] \n",
      "24 [D loss: (-5.062)(R -9.915, F -0.209)]  [G loss: 0.439] \n",
      "24 [D loss: (-5.938)(R -10.272, F -1.605)]  [G loss: 0.807] \n",
      "25 [D loss: (-6.365)(R -10.589, F -2.142)]  [G loss: 1.131] \n",
      "25 [D loss: (-6.912)(R -11.093, F -2.732)]  [G loss: 1.600] \n",
      "26 [D loss: (-7.238)(R -11.604, F -2.872)]  [G loss: 1.863] \n",
      "26 [D loss: (-7.409)(R -12.115, F -2.702)]  [G loss: 1.340] \n",
      "27 [D loss: (-7.512)(R -13.147, F -1.877)]  [G loss: 0.594] \n",
      "27 [D loss: (-7.293)(R -13.809, F -0.777)]  [G loss: -0.305] \n",
      "28 [D loss: (-7.042)(R -15.473, F 1.390)]  [G loss: -2.257] \n",
      "28 [D loss: (-6.325)(R -16.638, F 3.988)]  [G loss: -4.829] \n",
      "29 [D loss: (-5.851)(R -18.905, F 7.204)]  [G loss: -7.838] \n",
      "29 [D loss: (-5.086)(R -19.823, F 9.650)]  [G loss: -11.440] \n",
      "30 [D loss: (-4.407)(R -23.054, F 14.239)]  [G loss: -16.245] \n",
      "30 [D loss: (-4.472)(R -26.694, F 17.751)]  [G loss: -21.484] \n",
      "31 [D loss: (-4.180)(R -30.848, F 22.487)]  [G loss: -27.408] \n",
      "31 [D loss: (-3.759)(R -34.589, F 27.071)]  [G loss: -33.987] \n",
      "32 [D loss: (-4.008)(R -39.307, F 31.292)]  [G loss: -40.648] \n",
      "32 [D loss: (-5.100)(R -45.447, F 35.247)]  [G loss: -47.717] \n",
      "33 [D loss: (-5.585)(R -51.297, F 40.127)]  [G loss: -54.211] \n",
      "33 [D loss: (-6.672)(R -58.737, F 45.392)]  [G loss: -60.385] \n",
      "34 [D loss: (-6.372)(R -64.225, F 51.481)]  [G loss: -66.531] \n",
      "34 [D loss: (-4.625)(R -67.360, F 58.110)]  [G loss: -72.288] \n",
      "35 [D loss: (-2.277)(R -72.178, F 67.624)]  [G loss: -76.260] \n",
      "35 [D loss: (-1.209)(R -72.981, F 70.562)]  [G loss: -77.439] \n",
      "36 [D loss: (3.061)(R -67.597, F 73.718)]  [G loss: -74.032] \n",
      "36 [D loss: (4.232)(R -56.424, F 64.888)]  [G loss: -61.896] \n",
      "37 [D loss: (4.388)(R -38.584, F 47.360)]  [G loss: -43.424] \n",
      "37 [D loss: (1.752)(R -20.483, F 23.987)]  [G loss: -20.102] \n",
      "38 [D loss: (-1.346)(R -1.568, F -1.123)]  [G loss: 1.929] \n",
      "38 [D loss: (-1.082)(R 4.673, F -6.836)]  [G loss: 6.577] \n",
      "39 [D loss: (-1.179)(R 6.282, F -8.641)]  [G loss: 8.213] \n",
      "39 [D loss: (-1.315)(R 7.459, F -10.089)]  [G loss: 9.587] \n",
      "40 [D loss: (-1.624)(R 8.378, F -11.625)]  [G loss: 11.133] \n",
      "40 [D loss: (-2.003)(R 9.263, F -13.270)]  [G loss: 12.667] \n",
      "41 [D loss: (-2.298)(R 10.346, F -14.942)]  [G loss: 14.481] \n",
      "41 [D loss: (-2.376)(R 12.031, F -16.783)]  [G loss: 16.213] \n",
      "42 [D loss: (-2.801)(R 12.864, F -18.467)]  [G loss: 18.105] \n",
      "42 [D loss: (-3.204)(R 14.412, F -20.819)]  [G loss: 20.218] \n",
      "43 [D loss: (-3.902)(R 15.315, F -23.118)]  [G loss: 22.348] \n",
      "43 [D loss: (-3.902)(R 17.578, F -25.381)]  [G loss: 24.573] \n",
      "44 [D loss: (-4.078)(R 19.455, F -27.611)]  [G loss: 27.050] \n",
      "44 [D loss: (-4.646)(R 20.465, F -29.757)]  [G loss: 29.367] \n",
      "45 [D loss: (-4.862)(R 22.630, F -32.355)]  [G loss: 31.528] \n",
      "45 [D loss: (-5.322)(R 23.858, F -34.502)]  [G loss: 33.831] \n",
      "46 [D loss: (-5.155)(R 25.823, F -36.132)]  [G loss: 35.576] \n",
      "46 [D loss: (-5.749)(R 26.031, F -37.529)]  [G loss: 37.289] \n",
      "47 [D loss: (-5.223)(R 27.801, F -38.248)]  [G loss: 38.412] \n",
      "47 [D loss: (-4.826)(R 29.140, F -38.791)]  [G loss: 38.995] \n",
      "48 [D loss: (-4.828)(R 29.408, F -39.064)]  [G loss: 38.963] \n",
      "48 [D loss: (-4.979)(R 27.836, F -37.794)]  [G loss: 38.454] \n",
      "49 [D loss: (-4.538)(R 27.946, F -37.023)]  [G loss: 37.674] \n",
      "49 [D loss: (-4.123)(R 26.482, F -34.728)]  [G loss: 35.598] \n",
      "50 [D loss: (-4.755)(R 23.301, F -32.812)]  [G loss: 32.940] \n",
      "50 [D loss: (-4.185)(R 21.235, F -29.605)]  [G loss: 30.285] \n",
      "51 [D loss: (-4.460)(R 17.251, F -26.171)]  [G loss: 26.977] \n",
      "51 [D loss: (-4.548)(R 13.361, F -22.456)]  [G loss: 23.628] \n",
      "52 [D loss: (-4.898)(R 9.845, F -19.640)]  [G loss: 20.793] \n",
      "52 [D loss: (-5.028)(R 7.801, F -17.856)]  [G loss: 18.362] \n",
      "53 [D loss: (-5.401)(R 5.298, F -16.100)]  [G loss: 17.137] \n",
      "53 [D loss: (-5.959)(R 4.164, F -16.082)]  [G loss: 16.414] \n",
      "54 [D loss: (-6.187)(R 4.111, F -16.485)]  [G loss: 16.687] \n",
      "54 [D loss: (-5.799)(R 4.830, F -16.427)]  [G loss: 17.261] \n",
      "55 [D loss: (-6.875)(R 4.569, F -18.318)]  [G loss: 17.411] \n",
      "55 [D loss: (-6.713)(R 4.261, F -17.687)]  [G loss: 17.132] \n",
      "56 [D loss: (-7.104)(R 3.388, F -17.595)]  [G loss: 16.551] \n",
      "56 [D loss: (-7.042)(R 2.178, F -16.262)]  [G loss: 15.733] \n",
      "57 [D loss: (-7.838)(R 1.222, F -16.898)]  [G loss: 15.604] \n",
      "57 [D loss: (-9.257)(R -0.563, F -17.951)]  [G loss: 16.283] \n",
      "58 [D loss: (-10.912)(R -1.962, F -19.863)]  [G loss: 17.361] \n",
      "58 [D loss: (-12.361)(R -3.255, F -21.466)]  [G loss: 18.878] \n",
      "59 [D loss: (-12.692)(R -3.651, F -21.732)]  [G loss: 21.773] \n",
      "59 [D loss: (-15.931)(R -4.531, F -27.332)]  [G loss: 23.699] \n",
      "60 [D loss: (-16.985)(R -5.962, F -28.007)]  [G loss: 27.343] \n",
      "60 [D loss: (-18.967)(R -6.301, F -31.633)]  [G loss: 30.315] \n",
      "61 [D loss: (-20.279)(R -7.893, F -32.665)]  [G loss: 33.397] \n",
      "61 [D loss: (-20.330)(R -8.760, F -31.900)]  [G loss: 35.440] \n",
      "62 [D loss: (-19.191)(R -9.718, F -28.664)]  [G loss: 36.711] \n",
      "62 [D loss: (-19.709)(R -10.417, F -29.001)]  [G loss: 39.392] \n",
      "63 [D loss: (-18.851)(R -9.632, F -28.070)]  [G loss: 38.074] \n",
      "63 [D loss: (-17.775)(R -9.413, F -26.137)]  [G loss: 36.845] \n",
      "64 [D loss: (-12.048)(R -8.093, F -16.004)]  [G loss: 32.437] \n",
      "64 [D loss: (-9.648)(R -6.140, F -13.156)]  [G loss: 26.506] \n",
      "65 [D loss: (-5.866)(R -4.928, F -6.803)]  [G loss: 20.704] \n",
      "65 [D loss: (-3.669)(R -5.506, F -1.833)]  [G loss: 13.513] \n",
      "66 [D loss: (-1.370)(R -2.646, F -0.094)]  [G loss: 11.401] \n",
      "66 [D loss: (-0.021)(R 2.656, F -2.697)]  [G loss: 9.990] \n",
      "67 [D loss: (-0.339)(R 7.271, F -7.949)]  [G loss: 12.451] \n",
      "67 [D loss: (-1.676)(R 14.042, F -17.395)]  [G loss: 18.313] \n",
      "68 [D loss: (-3.688)(R 22.241, F -29.618)]  [G loss: 27.383] \n",
      "68 [D loss: (-6.584)(R 29.862, F -43.030)]  [G loss: 38.089] \n",
      "69 [D loss: (-8.968)(R 38.095, F -56.032)]  [G loss: 48.517] \n",
      "69 [D loss: (-10.397)(R 46.059, F -66.853)]  [G loss: 58.013] \n",
      "70 [D loss: (-11.604)(R 50.087, F -73.294)]  [G loss: 63.834] \n",
      "70 [D loss: (-10.666)(R 56.584, F -77.916)]  [G loss: 68.047] \n",
      "71 [D loss: (-10.728)(R 57.950, F -79.406)]  [G loss: 70.709] \n",
      "71 [D loss: (-9.023)(R 61.888, F -79.934)]  [G loss: 71.639] \n",
      "72 [D loss: (-7.272)(R 64.090, F -78.634)]  [G loss: 71.404] \n",
      "72 [D loss: (-6.156)(R 65.099, F -77.411)]  [G loss: 70.074] \n",
      "73 [D loss: (-3.247)(R 67.919, F -74.413)]  [G loss: 67.231] \n",
      "73 [D loss: (-1.988)(R 66.641, F -70.618)]  [G loss: 64.194] \n",
      "74 [D loss: (0.770)(R 65.599, F -64.059)]  [G loss: 59.307] \n",
      "74 [D loss: (2.016)(R 63.287, F -59.254)]  [G loss: 54.069] \n",
      "75 [D loss: (3.389)(R 59.200, F -52.422)]  [G loss: 46.552] \n",
      "75 [D loss: (4.186)(R 52.528, F -44.156)]  [G loss: 38.986] \n",
      "76 [D loss: (4.208)(R 44.082, F -35.666)]  [G loss: 31.850] \n",
      "76 [D loss: (3.576)(R 36.334, F -29.182)]  [G loss: 25.013] \n",
      "77 [D loss: (2.981)(R 29.310, F -23.348)]  [G loss: 20.323] \n",
      "77 [D loss: (2.450)(R 23.058, F -18.157)]  [G loss: 16.353] \n",
      "78 [D loss: (1.653)(R 17.489, F -14.182)]  [G loss: 13.368] \n",
      "78 [D loss: (0.738)(R 13.198, F -11.721)]  [G loss: 10.921] \n",
      "79 [D loss: (0.282)(R 9.617, F -9.054)]  [G loss: 9.320] \n",
      "79 [D loss: (-0.646)(R 6.421, F -7.713)]  [G loss: 7.924] \n",
      "80 [D loss: (-0.873)(R 4.372, F -6.118)]  [G loss: 6.522] \n",
      "80 [D loss: (-1.392)(R 1.656, F -4.440)]  [G loss: 5.717] \n",
      "81 [D loss: (-1.635)(R -0.018, F -3.252)]  [G loss: 4.727] \n",
      "81 [D loss: (-2.579)(R -2.826, F -2.331)]  [G loss: 3.583] \n",
      "82 [D loss: (-2.619)(R -3.803, F -1.436)]  [G loss: 2.534] \n",
      "82 [D loss: (-3.840)(R -7.495, F -0.184)]  [G loss: 1.152] \n",
      "83 [D loss: (-4.783)(R -10.479, F 0.914)]  [G loss: -0.254] \n",
      "83 [D loss: (-6.273)(R -14.716, F 2.169)]  [G loss: -1.902] \n",
      "84 [D loss: (-7.536)(R -18.523, F 3.450)]  [G loss: -3.154] \n",
      "84 [D loss: (-10.735)(R -26.058, F 4.587)]  [G loss: -5.882] \n",
      "85 [D loss: (-13.419)(R -33.997, F 7.160)]  [G loss: -8.993] \n",
      "85 [D loss: (-17.765)(R -48.708, F 13.178)]  [G loss: -14.415] \n",
      "86 [D loss: (-23.487)(R -66.506, F 19.532)]  [G loss: -21.885] \n",
      "86 [D loss: (-29.395)(R -87.551, F 28.761)]  [G loss: -31.676] \n",
      "87 [D loss: (-36.468)(R -111.151, F 38.215)]  [G loss: -41.532] \n",
      "87 [D loss: (-39.295)(R -128.236, F 49.645)]  [G loss: -52.082] \n",
      "88 [D loss: (-39.275)(R -135.511, F 56.962)]  [G loss: -61.601] \n",
      "88 [D loss: (-43.234)(R -153.130, F 66.662)]  [G loss: -71.292] \n",
      "89 [D loss: (-44.577)(R -160.691, F 71.536)]  [G loss: -78.029] \n",
      "89 [D loss: (-48.100)(R -168.859, F 72.659)]  [G loss: -81.923] \n",
      "90 [D loss: (-47.862)(R -176.652, F 80.928)]  [G loss: -88.136] \n",
      "90 [D loss: (-48.000)(R -190.251, F 94.251)]  [G loss: -106.479] \n",
      "91 [D loss: (-49.046)(R -214.547, F 116.455)]  [G loss: -132.875] \n",
      "91 [D loss: (-45.149)(R -241.574, F 151.277)]  [G loss: -168.443] \n",
      "92 [D loss: (-35.934)(R -253.400, F 181.532)]  [G loss: -202.573] \n",
      "92 [D loss: (-26.302)(R -259.384, F 206.779)]  [G loss: -228.174] \n",
      "93 [D loss: (-15.896)(R -261.766, F 229.974)]  [G loss: -249.708] \n",
      "93 [D loss: (-2.774)(R -253.580, F 248.031)]  [G loss: -265.048] \n",
      "94 [D loss: (2.690)(R -248.151, F 253.531)]  [G loss: -263.592] \n",
      "94 [D loss: (4.401)(R -213.218, F 222.020)]  [G loss: -227.590] \n",
      "95 [D loss: (6.114)(R -153.823, F 166.052)]  [G loss: -164.567] \n",
      "95 [D loss: (4.257)(R -91.239, F 99.753)]  [G loss: -93.335] \n",
      "96 [D loss: (-0.773)(R -33.125, F 31.578)]  [G loss: -22.947] \n",
      "96 [D loss: (-7.593)(R 19.184, F -34.369)]  [G loss: 42.904] \n",
      "97 [D loss: (-14.092)(R 66.178, F -94.362)]  [G loss: 98.057] \n",
      "97 [D loss: (-16.403)(R 83.620, F -116.426)]  [G loss: 119.047] \n",
      "98 [D loss: (-19.219)(R 97.448, F -135.885)]  [G loss: 138.605] \n",
      "98 [D loss: (-22.692)(R 112.476, F -157.860)]  [G loss: 157.796] \n",
      "99 [D loss: (-25.456)(R 127.452, F -178.364)]  [G loss: 177.274] \n",
      "99 [D loss: (-29.654)(R 138.952, F -198.261)]  [G loss: 197.362] \n",
      "100 [D loss: (-34.653)(R 152.449, F -221.754)]  [G loss: 217.729] \n",
      "100 [D loss: (-37.037)(R 170.696, F -244.769)]  [G loss: 241.189] \n",
      "101 [D loss: (-41.695)(R 189.314, F -272.704)]  [G loss: 265.474] \n",
      "101 [D loss: (-45.729)(R 210.362, F -301.821)]  [G loss: 292.791] \n",
      "102 [D loss: (-53.576)(R 229.017, F -336.170)]  [G loss: 324.345] \n",
      "102 [D loss: (-50.660)(R 269.652, F -370.972)]  [G loss: 359.816] \n",
      "103 [D loss: (-56.235)(R 299.495, F -411.965)]  [G loss: 397.553] \n",
      "103 [D loss: (-62.180)(R 332.864, F -457.225)]  [G loss: 434.890] \n",
      "104 [D loss: (-57.241)(R 379.824, F -494.306)]  [G loss: 472.536] \n",
      "104 [D loss: (-61.843)(R 405.416, F -529.102)]  [G loss: 500.860] \n",
      "105 [D loss: (-57.178)(R 445.777, F -560.133)]  [G loss: 520.797] \n",
      "105 [D loss: (-46.576)(R 481.257, F -574.410)]  [G loss: 531.311] \n",
      "106 [D loss: (-33.366)(R 502.649, F -569.380)]  [G loss: 528.351] \n",
      "106 [D loss: (-26.433)(R 513.289, F -566.154)]  [G loss: 515.536] \n",
      "107 [D loss: (-8.174)(R 526.095, F -542.443)]  [G loss: 493.720] \n",
      "107 [D loss: (1.282)(R 511.596, F -509.033)]  [G loss: 455.998] \n",
      "108 [D loss: (23.849)(R 490.272, F -442.575)]  [G loss: 402.864] \n",
      "108 [D loss: (31.161)(R 447.268, F -384.946)]  [G loss: 339.885] \n",
      "109 [D loss: (34.774)(R 386.849, F -317.301)]  [G loss: 265.661] \n",
      "109 [D loss: (37.685)(R 304.330, F -228.959)]  [G loss: 196.086] \n",
      "110 [D loss: (29.130)(R 201.170, F -142.909)]  [G loss: 120.413] \n",
      "110 [D loss: (15.346)(R 90.986, F -60.293)]  [G loss: 50.405] \n",
      "111 [D loss: (-1.779)(R -13.279, F 9.722)]  [G loss: -11.870] \n",
      "111 [D loss: (-20.893)(R -117.905, F 76.119)]  [G loss: -64.897] \n",
      "112 [D loss: (-39.361)(R -214.459, F 135.737)]  [G loss: -113.388] \n",
      "112 [D loss: (-71.060)(R -323.781, F 181.661)]  [G loss: -157.805] \n",
      "113 [D loss: (-105.717)(R -447.124, F 235.691)]  [G loss: -206.868] \n",
      "113 [D loss: (-135.088)(R -550.961, F 280.784)]  [G loss: -246.873] \n",
      "114 [D loss: (-166.944)(R -667.406, F 333.518)]  [G loss: -291.101] \n",
      "114 [D loss: (-177.679)(R -740.418, F 385.059)]  [G loss: -333.710] \n",
      "115 [D loss: (-180.071)(R -819.908, F 459.767)]  [G loss: -377.584] \n",
      "115 [D loss: (-222.272)(R -916.221, F 471.677)]  [G loss: -437.265] \n",
      "116 [D loss: (-211.600)(R -962.099, F 538.898)]  [G loss: -504.236] \n",
      "116 [D loss: (-226.710)(R -1066.354, F 612.934)]  [G loss: -567.831] \n",
      "117 [D loss: (-236.202)(R -1152.413, F 680.009)]  [G loss: -649.115] \n",
      "117 [D loss: (-198.199)(R -1195.561, F 799.163)]  [G loss: -739.766] \n",
      "118 [D loss: (-214.801)(R -1309.358, F 879.756)]  [G loss: -838.300] \n",
      "118 [D loss: (-223.629)(R -1404.902, F 957.645)]  [G loss: -947.057] \n",
      "119 [D loss: (-179.626)(R -1429.854, F 1070.602)]  [G loss: -1061.535] \n",
      "119 [D loss: (-235.226)(R -1541.594, F 1071.142)]  [G loss: -1159.364] \n",
      "120 [D loss: (-185.282)(R -1653.062, F 1282.498)]  [G loss: -1254.471] \n",
      "120 [D loss: (-187.226)(R -1672.694, F 1298.243)]  [G loss: -1341.430] \n",
      "121 [D loss: (-228.508)(R -1755.991, F 1298.974)]  [G loss: -1461.943] \n",
      "121 [D loss: (-213.909)(R -1782.012, F 1354.194)]  [G loss: -1519.081] \n",
      "122 [D loss: (-138.203)(R -1778.656, F 1502.249)]  [G loss: -1647.884] \n",
      "122 [D loss: (-196.813)(R -1848.648, F 1455.022)]  [G loss: -1737.649] \n",
      "123 [D loss: (-154.724)(R -1905.550, F 1596.101)]  [G loss: -1825.393] \n",
      "123 [D loss: (-90.447)(R -1936.741, F 1755.847)]  [G loss: -1904.682] \n",
      "124 [D loss: (-83.685)(R -1907.579, F 1740.208)]  [G loss: -2021.060] \n",
      "124 [D loss: (-3.808)(R -1943.870, F 1936.254)]  [G loss: -2098.727] \n",
      "125 [D loss: (-6.508)(R -1961.974, F 1948.959)]  [G loss: -2157.609] \n",
      "125 [D loss: (77.467)(R -1925.023, F 2079.957)]  [G loss: -2191.783] \n",
      "126 [D loss: (53.043)(R -1828.958, F 1935.043)]  [G loss: -2123.633] \n",
      "126 [D loss: (96.725)(R -1656.781, F 1850.230)]  [G loss: -1977.200] \n",
      "127 [D loss: (126.865)(R -1463.345, F 1717.076)]  [G loss: -1790.727] \n",
      "127 [D loss: (129.614)(R -1241.087, F 1500.314)]  [G loss: -1534.876] \n",
      "128 [D loss: (108.214)(R -1040.168, F 1256.595)]  [G loss: -1281.789] \n",
      "128 [D loss: (108.822)(R -838.745, F 1056.390)]  [G loss: -1070.594] \n",
      "129 [D loss: (108.023)(R -672.231, F 888.276)]  [G loss: -878.960] \n",
      "129 [D loss: (95.476)(R -545.296, F 736.248)]  [G loss: -718.169] \n",
      "130 [D loss: (87.638)(R -427.776, F 603.051)]  [G loss: -587.311] \n",
      "130 [D loss: (72.579)(R -352.276, F 497.435)]  [G loss: -485.238] \n",
      "131 [D loss: (62.239)(R -286.409, F 410.887)]  [G loss: -401.685] \n",
      "131 [D loss: (51.668)(R -240.471, F 343.808)]  [G loss: -335.084] \n",
      "132 [D loss: (42.404)(R -202.967, F 287.776)]  [G loss: -281.620] \n",
      "132 [D loss: (36.125)(R -171.258, F 243.508)]  [G loss: -239.322] \n",
      "133 [D loss: (30.573)(R -147.954, F 209.099)]  [G loss: -205.764] \n",
      "133 [D loss: (25.201)(R -129.844, F 180.247)]  [G loss: -177.993] \n",
      "134 [D loss: (22.783)(R -109.999, F 155.566)]  [G loss: -154.697] \n",
      "134 [D loss: (18.410)(R -98.896, F 135.717)]  [G loss: -134.548] \n",
      "135 [D loss: (14.809)(R -88.796, F 118.414)]  [G loss: -117.757] \n",
      "135 [D loss: (11.769)(R -79.841, F 103.380)]  [G loss: -102.806] \n",
      "136 [D loss: (10.179)(R -70.464, F 90.822)]  [G loss: -90.000] \n",
      "136 [D loss: (9.125)(R -60.364, F 78.613)]  [G loss: -77.995] \n",
      "137 [D loss: (5.640)(R -56.669, F 67.949)]  [G loss: -67.226] \n",
      "137 [D loss: (4.833)(R -48.826, F 58.492)]  [G loss: -57.716] \n",
      "138 [D loss: (3.295)(R -42.729, F 49.318)]  [G loss: -48.826] \n",
      "138 [D loss: (1.720)(R -37.209, F 40.648)]  [G loss: -39.908] \n",
      "139 [D loss: (0.054)(R -32.200, F 32.308)]  [G loss: -31.608] \n",
      "139 [D loss: (-1.076)(R -26.135, F 23.983)]  [G loss: -23.084] \n",
      "140 [D loss: (-3.114)(R -21.819, F 15.591)]  [G loss: -14.832] \n",
      "140 [D loss: (-4.225)(R -15.737, F 7.286)]  [G loss: -6.331] \n",
      "141 [D loss: (-6.210)(R -10.974, F -1.446)]  [G loss: 2.422] \n",
      "141 [D loss: (-7.866)(R -5.173, F -10.559)]  [G loss: 11.525] \n",
      "142 [D loss: (-9.514)(R 0.806, F -19.833)]  [G loss: 21.174] \n",
      "142 [D loss: (-11.429)(R 7.393, F -30.252)]  [G loss: 31.535] \n",
      "143 [D loss: (-13.220)(R 14.661, F -41.100)]  [G loss: 42.716] \n",
      "143 [D loss: (-16.312)(R 21.130, F -53.754)]  [G loss: 55.025] \n",
      "144 [D loss: (-18.314)(R 30.803, F -67.430)]  [G loss: 69.115] \n",
      "144 [D loss: (-22.372)(R 37.960, F -82.703)]  [G loss: 84.170] \n",
      "145 [D loss: (-25.031)(R 48.476, F -98.539)]  [G loss: 99.973] \n",
      "145 [D loss: (-28.394)(R 59.279, F -116.067)]  [G loss: 116.828] \n",
      "146 [D loss: (-33.282)(R 69.698, F -136.263)]  [G loss: 137.497] \n",
      "146 [D loss: (-39.044)(R 81.256, F -159.345)]  [G loss: 160.883] \n",
      "147 [D loss: (-43.945)(R 97.900, F -185.791)]  [G loss: 187.266] \n",
      "147 [D loss: (-50.005)(R 114.578, F -214.588)]  [G loss: 218.165] \n",
      "148 [D loss: (-59.593)(R 131.063, F -250.249)]  [G loss: 253.305] \n",
      "148 [D loss: (-63.151)(R 166.853, F -293.155)]  [G loss: 295.641] \n",
      "149 [D loss: (-71.988)(R 198.546, F -342.522)]  [G loss: 346.133] \n",
      "149 [D loss: (-82.277)(R 233.410, F -397.965)]  [G loss: 405.750] \n",
      "150 [D loss: (-97.526)(R 275.169, F -470.221)]  [G loss: 469.807] \n",
      "150 [D loss: (-106.182)(R 324.917, F -537.281)]  [G loss: 539.936] \n",
      "151 [D loss: (-124.063)(R 379.956, F -628.082)]  [G loss: 617.572] \n",
      "151 [D loss: (-119.835)(R 447.652, F -687.322)]  [G loss: 706.800] \n",
      "152 [D loss: (-132.567)(R 520.376, F -785.509)]  [G loss: 789.402] \n",
      "152 [D loss: (-149.062)(R 580.731, F -878.855)]  [G loss: 869.957] \n",
      "153 [D loss: (-128.485)(R 674.657, F -931.628)]  [G loss: 952.454] \n",
      "153 [D loss: (-145.009)(R 742.131, F -1032.150)]  [G loss: 1007.278] \n",
      "154 [D loss: (-106.594)(R 823.528, F -1036.715)]  [G loss: 1021.854] \n",
      "154 [D loss: (-102.125)(R 824.658, F -1028.908)]  [G loss: 968.230] \n",
      "155 [D loss: (-100.753)(R 858.542, F -1060.048)]  [G loss: 942.751] \n",
      "155 [D loss: (-36.189)(R 871.731, F -944.109)]  [G loss: 903.653] \n",
      "156 [D loss: (-29.109)(R 864.889, F -923.108)]  [G loss: 869.263] \n",
      "156 [D loss: (-44.292)(R 843.083, F -931.667)]  [G loss: 819.459] \n",
      "157 [D loss: (22.789)(R 830.604, F -785.027)]  [G loss: 775.387] \n",
      "157 [D loss: (-14.781)(R 809.906, F -839.469)]  [G loss: 697.347] \n",
      "158 [D loss: (4.951)(R 769.709, F -759.807)]  [G loss: 651.986] \n",
      "158 [D loss: (51.153)(R 729.590, F -627.283)]  [G loss: 607.331] \n",
      "159 [D loss: (36.158)(R 648.025, F -575.709)]  [G loss: 531.618] \n",
      "159 [D loss: (24.242)(R 568.352, F -519.868)]  [G loss: 474.526] \n",
      "160 [D loss: (21.659)(R 486.573, F -443.255)]  [G loss: 422.702] \n",
      "160 [D loss: (12.623)(R 427.804, F -402.559)]  [G loss: 364.267] \n",
      "161 [D loss: (13.568)(R 361.399, F -334.263)]  [G loss: 323.359] \n",
      "161 [D loss: (-13.853)(R 298.459, F -326.165)]  [G loss: 279.414] \n",
      "162 [D loss: (-7.518)(R 249.526, F -264.562)]  [G loss: 247.243] \n",
      "162 [D loss: (-32.436)(R 172.854, F -237.727)]  [G loss: 215.084] \n",
      "163 [D loss: (-38.265)(R 109.891, F -186.422)]  [G loss: 183.207] \n",
      "163 [D loss: (-52.794)(R 42.068, F -147.656)]  [G loss: 150.933] \n",
      "164 [D loss: (-71.380)(R -37.117, F -105.644)]  [G loss: 113.521] \n",
      "164 [D loss: (-92.823)(R -118.760, F -66.887)]  [G loss: 84.804] \n",
      "165 [D loss: (-108.214)(R -189.255, F -27.173)]  [G loss: 35.029] \n",
      "165 [D loss: (-118.522)(R -273.570, F 36.526)]  [G loss: -5.852] \n",
      "166 [D loss: (-148.510)(R -358.023, F 61.003)]  [G loss: -63.830] \n",
      "166 [D loss: (-156.873)(R -443.777, F 130.031)]  [G loss: -102.071] \n",
      "167 [D loss: (-178.457)(R -534.740, F 177.827)]  [G loss: -173.846] \n",
      "167 [D loss: (-197.388)(R -639.849, F 245.073)]  [G loss: -239.826] \n",
      "168 [D loss: (-225.547)(R -753.456, F 302.362)]  [G loss: -297.360] \n",
      "168 [D loss: (-251.013)(R -870.688, F 368.662)]  [G loss: -362.129] \n",
      "169 [D loss: (-239.072)(R -935.000, F 456.857)]  [G loss: -429.822] \n",
      "169 [D loss: (-276.763)(R -1030.399, F 476.873)]  [G loss: -466.770] \n",
      "170 [D loss: (-294.103)(R -1138.694, F 550.487)]  [G loss: -560.795] \n",
      "170 [D loss: (-279.892)(R -1174.965, F 615.181)]  [G loss: -616.038] \n",
      "171 [D loss: (-285.977)(R -1258.545, F 686.592)]  [G loss: -679.195] \n",
      "171 [D loss: (-300.683)(R -1298.364, F 696.998)]  [G loss: -735.702] \n",
      "172 [D loss: (-296.002)(R -1418.312, F 826.309)]  [G loss: -807.952] \n",
      "172 [D loss: (-303.819)(R -1523.613, F 915.975)]  [G loss: -905.072] \n",
      "173 [D loss: (-311.431)(R -1536.081, F 913.218)]  [G loss: -928.038] \n",
      "173 [D loss: (-315.796)(R -1606.830, F 975.239)]  [G loss: -996.922] \n",
      "174 [D loss: (-356.436)(R -1655.600, F 942.729)]  [G loss: -1056.540] \n",
      "174 [D loss: (-352.243)(R -1730.202, F 1025.716)]  [G loss: -1103.627] \n",
      "175 [D loss: (-339.508)(R -1753.021, F 1074.005)]  [G loss: -1171.001] \n",
      "175 [D loss: (-292.195)(R -1782.631, F 1198.240)]  [G loss: -1245.430] \n",
      "176 [D loss: (-358.477)(R -1878.632, F 1161.677)]  [G loss: -1292.514] \n",
      "176 [D loss: (-319.138)(R -1925.552, F 1287.275)]  [G loss: -1369.397] \n",
      "177 [D loss: (-328.321)(R -1915.702, F 1259.060)]  [G loss: -1439.829] \n",
      "177 [D loss: (-341.591)(R -1988.109, F 1304.927)]  [G loss: -1487.224] \n",
      "178 [D loss: (-279.858)(R -1978.206, F 1418.489)]  [G loss: -1564.077] \n",
      "178 [D loss: (-313.741)(R -2023.325, F 1395.843)]  [G loss: -1684.414] \n",
      "179 [D loss: (-264.634)(R -2054.485, F 1525.218)]  [G loss: -1720.181] \n",
      "179 [D loss: (-275.737)(R -2087.735, F 1536.260)]  [G loss: -1790.466] \n",
      "180 [D loss: (-265.010)(R -2122.573, F 1592.554)]  [G loss: -1866.366] \n",
      "180 [D loss: (-242.531)(R -2128.748, F 1643.686)]  [G loss: -1898.454] \n",
      "181 [D loss: (-220.433)(R -2222.995, F 1782.129)]  [G loss: -1990.777] \n",
      "181 [D loss: (-205.185)(R -2178.559, F 1768.189)]  [G loss: -2001.390] \n",
      "182 [D loss: (-168.385)(R -2196.692, F 1859.921)]  [G loss: -2138.011] \n",
      "182 [D loss: (-170.941)(R -2245.512, F 1903.631)]  [G loss: -2162.302] \n",
      "183 [D loss: (-227.594)(R -2289.402, F 1834.214)]  [G loss: -2248.200] \n",
      "183 [D loss: (-187.037)(R -2243.628, F 1869.553)]  [G loss: -2304.068] \n",
      "184 [D loss: (-119.445)(R -2243.461, F 2004.571)]  [G loss: -2382.092] \n",
      "184 [D loss: (-97.453)(R -2271.625, F 2076.720)]  [G loss: -2403.115] \n",
      "185 [D loss: (10.429)(R -2253.040, F 2273.898)]  [G loss: -2458.522] \n",
      "185 [D loss: (1.309)(R -2256.785, F 2259.403)]  [G loss: -2556.337] \n",
      "186 [D loss: (-13.115)(R -2270.809, F 2244.579)]  [G loss: -2533.859] \n",
      "186 [D loss: (11.772)(R -2279.503, F 2303.047)]  [G loss: -2522.580] \n",
      "187 [D loss: (50.528)(R -2195.472, F 2296.529)]  [G loss: -2523.351] \n",
      "187 [D loss: (87.785)(R -2080.730, F 2256.299)]  [G loss: -2443.083] \n",
      "188 [D loss: (81.440)(R -1931.224, F 2094.105)]  [G loss: -2303.422] \n",
      "188 [D loss: (111.684)(R -1780.680, F 2004.048)]  [G loss: -2176.140] \n",
      "189 [D loss: (112.754)(R -1599.757, F 1825.265)]  [G loss: -1950.410] \n",
      "189 [D loss: (173.148)(R -1429.199, F 1775.495)]  [G loss: -1809.855] \n",
      "190 [D loss: (117.238)(R -1301.053, F 1535.528)]  [G loss: -1645.938] \n",
      "190 [D loss: (106.232)(R -1145.068, F 1357.532)]  [G loss: -1467.739] \n",
      "191 [D loss: (120.193)(R -1030.091, F 1270.476)]  [G loss: -1330.481] \n",
      "191 [D loss: (132.485)(R -910.509, F 1175.479)]  [G loss: -1180.816] \n",
      "192 [D loss: (118.899)(R -795.335, F 1033.134)]  [G loss: -1032.809] \n",
      "192 [D loss: (107.725)(R -690.651, F 906.102)]  [G loss: -920.008] \n",
      "193 [D loss: (104.345)(R -603.076, F 811.765)]  [G loss: -815.055] \n",
      "193 [D loss: (94.271)(R -539.000, F 727.542)]  [G loss: -714.853] \n",
      "194 [D loss: (85.078)(R -471.130, F 641.285)]  [G loss: -634.204] \n",
      "194 [D loss: (77.946)(R -419.163, F 575.054)]  [G loss: -566.609] \n",
      "195 [D loss: (78.092)(R -363.894, F 520.078)]  [G loss: -510.479] \n",
      "195 [D loss: (64.946)(R -331.504, F 461.397)]  [G loss: -457.162] \n",
      "196 [D loss: (59.047)(R -298.988, F 417.081)]  [G loss: -411.107] \n",
      "196 [D loss: (60.184)(R -264.390, F 384.758)]  [G loss: -371.933] \n",
      "197 [D loss: (52.015)(R -238.725, F 342.755)]  [G loss: -334.419] \n",
      "197 [D loss: (49.084)(R -215.698, F 313.865)]  [G loss: -304.647] \n",
      "198 [D loss: (43.029)(R -201.267, F 287.325)]  [G loss: -279.288] \n",
      "198 [D loss: (37.557)(R -186.404, F 261.518)]  [G loss: -256.757] \n",
      "199 [D loss: (36.118)(R -169.838, F 242.074)]  [G loss: -236.441] \n",
      "199 [D loss: (32.845)(R -158.040, F 223.729)]  [G loss: -216.734] \n",
      "200 [D loss: (32.359)(R -140.876, F 205.594)]  [G loss: -200.802] \n",
      "200 [D loss: (29.578)(R -132.561, F 191.718)]  [G loss: -186.618] \n",
      "201 [D loss: (27.220)(R -122.876, F 177.316)]  [G loss: -172.870] \n",
      "201 [D loss: (24.477)(R -115.039, F 163.993)]  [G loss: -161.030] \n",
      "202 [D loss: (23.781)(R -105.668, F 153.231)]  [G loss: -150.178] \n",
      "202 [D loss: (21.723)(R -98.924, F 142.371)]  [G loss: -140.579] \n",
      "203 [D loss: (19.843)(R -93.640, F 133.326)]  [G loss: -131.619] \n",
      "203 [D loss: (18.326)(R -88.963, F 125.614)]  [G loss: -123.644] \n",
      "204 [D loss: (16.430)(R -84.987, F 117.848)]  [G loss: -116.291] \n",
      "204 [D loss: (15.294)(R -79.771, F 110.358)]  [G loss: -109.618] \n",
      "205 [D loss: (15.641)(R -74.078, F 105.359)]  [G loss: -103.598] \n",
      "205 [D loss: (14.901)(R -69.498, F 99.299)]  [G loss: -97.729] \n",
      "206 [D loss: (14.134)(R -65.299, F 93.566)]  [G loss: -92.468] \n",
      "206 [D loss: (12.271)(R -64.029, F 88.572)]  [G loss: -87.891] \n",
      "207 [D loss: (11.785)(R -60.471, F 84.041)]  [G loss: -83.438] \n",
      "207 [D loss: (11.090)(R -57.655, F 79.835)]  [G loss: -79.277] \n",
      "208 [D loss: (10.332)(R -55.591, F 76.256)]  [G loss: -75.425] \n",
      "208 [D loss: (9.991)(R -52.932, F 72.914)]  [G loss: -71.975] \n",
      "209 [D loss: (9.647)(R -49.886, F 69.179)]  [G loss: -68.471] \n",
      "209 [D loss: (8.825)(R -48.201, F 65.850)]  [G loss: -65.404] \n",
      "210 [D loss: (8.293)(R -46.217, F 62.804)]  [G loss: -62.442] \n",
      "210 [D loss: (7.947)(R -44.022, F 59.917)]  [G loss: -59.585] \n",
      "211 [D loss: (7.041)(R -43.174, F 57.256)]  [G loss: -57.043] \n",
      "211 [D loss: (7.499)(R -39.840, F 54.837)]  [G loss: -54.509] \n",
      "212 [D loss: (6.799)(R -38.873, F 52.471)]  [G loss: -52.161] \n",
      "212 [D loss: (6.708)(R -36.854, F 50.270)]  [G loss: -50.012] \n",
      "213 [D loss: (5.757)(R -36.706, F 48.219)]  [G loss: -47.949] \n",
      "213 [D loss: (5.894)(R -34.456, F 46.244)]  [G loss: -45.951] \n",
      "214 [D loss: (5.470)(R -33.430, F 44.369)]  [G loss: -44.196] \n",
      "214 [D loss: (5.029)(R -32.638, F 42.697)]  [G loss: -42.429] \n",
      "215 [D loss: (4.978)(R -31.123, F 41.079)]  [G loss: -40.851] \n",
      "215 [D loss: (4.847)(R -29.853, F 39.546)]  [G loss: -39.287] \n",
      "216 [D loss: (4.251)(R -29.541, F 38.043)]  [G loss: -37.786] \n",
      "216 [D loss: (4.652)(R -27.206, F 36.510)]  [G loss: -36.366] \n",
      "217 [D loss: (3.820)(R -27.558, F 35.197)]  [G loss: -34.931] \n",
      "217 [D loss: (3.713)(R -26.406, F 33.832)]  [G loss: -33.702] \n",
      "218 [D loss: (3.749)(R -25.182, F 32.679)]  [G loss: -32.461] \n",
      "218 [D loss: (3.290)(R -24.882, F 31.461)]  [G loss: -31.232] \n",
      "219 [D loss: (3.051)(R -24.145, F 30.248)]  [G loss: -30.082] \n",
      "219 [D loss: (3.383)(R -22.377, F 29.144)]  [G loss: -29.017] \n",
      "220 [D loss: (3.039)(R -22.002, F 28.080)]  [G loss: -27.934] \n",
      "220 [D loss: (2.930)(R -21.228, F 27.087)]  [G loss: -26.896] \n",
      "221 [D loss: (2.367)(R -21.302, F 26.036)]  [G loss: -25.910] \n",
      "221 [D loss: (2.230)(R -20.581, F 25.040)]  [G loss: -24.946] \n",
      "222 [D loss: (2.150)(R -19.863, F 24.164)]  [G loss: -23.985] \n",
      "222 [D loss: (1.983)(R -19.276, F 23.242)]  [G loss: -23.111] \n",
      "223 [D loss: (2.109)(R -18.131, F 22.349)]  [G loss: -22.244] \n",
      "223 [D loss: (1.866)(R -17.720, F 21.452)]  [G loss: -21.333] \n",
      "224 [D loss: (1.623)(R -17.357, F 20.604)]  [G loss: -20.544] \n",
      "224 [D loss: (1.412)(R -16.982, F 19.807)]  [G loss: -19.724] \n",
      "225 [D loss: (1.243)(R -16.552, F 19.038)]  [G loss: -18.916] \n",
      "225 [D loss: (1.234)(R -15.844, F 18.312)]  [G loss: -18.193] \n",
      "226 [D loss: (1.158)(R -15.224, F 17.539)]  [G loss: -17.415] \n",
      "226 [D loss: (1.021)(R -14.725, F 16.766)]  [G loss: -16.688] \n",
      "227 [D loss: (0.835)(R -14.389, F 16.059)]  [G loss: -15.989] \n",
      "227 [D loss: (0.670)(R -14.021, F 15.361)]  [G loss: -15.277] \n",
      "228 [D loss: (0.560)(R -13.559, F 14.679)]  [G loss: -14.587] \n",
      "228 [D loss: (0.523)(R -12.980, F 14.026)]  [G loss: -13.879] \n",
      "229 [D loss: (0.304)(R -12.696, F 13.305)]  [G loss: -13.213] \n",
      "229 [D loss: (0.313)(R -11.976, F 12.601)]  [G loss: -12.535] \n",
      "230 [D loss: (0.116)(R -11.682, F 11.914)]  [G loss: -11.861] \n",
      "230 [D loss: (0.018)(R -11.223, F 11.260)]  [G loss: -11.174] \n",
      "231 [D loss: (-0.104)(R -10.787, F 10.578)]  [G loss: -10.510] \n",
      "231 [D loss: (-0.124)(R -10.189, F 9.941)]  [G loss: -9.893] \n",
      "232 [D loss: (-0.330)(R -9.950, F 9.290)]  [G loss: -9.204] \n",
      "232 [D loss: (-0.456)(R -9.574, F 8.661)]  [G loss: -8.559] \n",
      "233 [D loss: (-0.558)(R -9.124, F 8.007)]  [G loss: -7.850] \n",
      "233 [D loss: (-0.705)(R -8.646, F 7.236)]  [G loss: -7.229] \n",
      "234 [D loss: (-0.832)(R -8.275, F 6.612)]  [G loss: -6.571] \n",
      "234 [D loss: (-0.913)(R -7.815, F 5.988)]  [G loss: -5.958] \n",
      "235 [D loss: (-1.153)(R -7.505, F 5.200)]  [G loss: -5.177] \n",
      "235 [D loss: (-1.261)(R -7.045, F 4.523)]  [G loss: -4.538] \n",
      "236 [D loss: (-1.375)(R -6.621, F 3.871)]  [G loss: -3.709] \n",
      "236 [D loss: (-1.399)(R -6.123, F 3.325)]  [G loss: -3.049] \n",
      "237 [D loss: (-1.622)(R -5.687, F 2.442)]  [G loss: -2.269] \n",
      "237 [D loss: (-1.812)(R -5.255, F 1.632)]  [G loss: -1.463] \n",
      "238 [D loss: (-2.018)(R -4.846, F 0.811)]  [G loss: -0.838] \n",
      "238 [D loss: (-2.112)(R -4.477, F 0.253)]  [G loss: -0.002] \n",
      "239 [D loss: (-2.420)(R -3.943, F -0.896)]  [G loss: 0.867] \n",
      "239 [D loss: (-2.539)(R -3.433, F -1.644)]  [G loss: 1.713] \n",
      "240 [D loss: (-2.667)(R -2.989, F -2.346)]  [G loss: 2.608] \n",
      "240 [D loss: (-2.893)(R -2.492, F -3.295)]  [G loss: 3.406] \n",
      "241 [D loss: (-3.113)(R -1.967, F -4.258)]  [G loss: 4.389] \n",
      "241 [D loss: (-3.071)(R -1.099, F -5.044)]  [G loss: 5.271] \n",
      "242 [D loss: (-3.409)(R -0.505, F -6.313)]  [G loss: 6.283] \n",
      "242 [D loss: (-3.746)(R -0.038, F -7.453)]  [G loss: 7.303] \n",
      "243 [D loss: (-3.957)(R 0.547, F -8.461)]  [G loss: 8.520] \n",
      "243 [D loss: (-4.088)(R 1.341, F -9.517)]  [G loss: 9.643] \n",
      "244 [D loss: (-4.410)(R 1.877, F -10.697)]  [G loss: 10.925] \n",
      "244 [D loss: (-4.641)(R 2.501, F -11.782)]  [G loss: 12.320] \n",
      "245 [D loss: (-4.746)(R 3.759, F -13.251)]  [G loss: 13.537] \n",
      "245 [D loss: (-5.243)(R 4.080, F -14.567)]  [G loss: 15.081] \n",
      "246 [D loss: (-5.477)(R 5.505, F -16.459)]  [G loss: 16.730] \n",
      "246 [D loss: (-5.982)(R 6.111, F -18.075)]  [G loss: 18.432] \n",
      "247 [D loss: (-6.459)(R 6.934, F -19.852)]  [G loss: 20.153] \n",
      "247 [D loss: (-6.875)(R 8.185, F -21.935)]  [G loss: 22.106] \n",
      "248 [D loss: (-7.189)(R 9.507, F -23.885)]  [G loss: 24.314] \n",
      "248 [D loss: (-7.943)(R 10.567, F -26.454)]  [G loss: 26.489] \n",
      "249 [D loss: (-8.171)(R 12.411, F -28.753)]  [G loss: 28.794] \n",
      "249 [D loss: (-8.662)(R 14.055, F -31.379)]  [G loss: 31.686] \n",
      "250 [D loss: (-9.748)(R 14.464, F -33.959)]  [G loss: 34.315] \n",
      "250 [D loss: (-10.026)(R 16.648, F -36.699)]  [G loss: 37.535] \n",
      "251 [D loss: (-11.244)(R 18.414, F -40.903)]  [G loss: 41.100] \n",
      "251 [D loss: (-11.912)(R 20.423, F -44.247)]  [G loss: 44.751] \n",
      "252 [D loss: (-13.144)(R 22.098, F -48.387)]  [G loss: 48.666] \n",
      "252 [D loss: (-13.781)(R 25.693, F -53.255)]  [G loss: 53.192] \n",
      "253 [D loss: (-15.012)(R 27.721, F -57.745)]  [G loss: 58.376] \n",
      "253 [D loss: (-16.052)(R 31.219, F -63.323)]  [G loss: 64.224] \n",
      "254 [D loss: (-17.590)(R 34.216, F -69.396)]  [G loss: 69.982] \n",
      "254 [D loss: (-19.190)(R 38.412, F -76.791)]  [G loss: 76.435] \n",
      "255 [D loss: (-20.406)(R 42.503, F -83.314)]  [G loss: 83.983] \n",
      "255 [D loss: (-22.441)(R 47.571, F -92.453)]  [G loss: 92.713] \n",
      "256 [D loss: (-23.841)(R 53.694, F -101.376)]  [G loss: 103.312] \n",
      "256 [D loss: (-26.134)(R 59.924, F -112.191)]  [G loss: 113.717] \n",
      "257 [D loss: (-28.359)(R 68.546, F -125.264)]  [G loss: 126.040] \n",
      "257 [D loss: (-33.075)(R 73.327, F -139.477)]  [G loss: 140.245] \n",
      "258 [D loss: (-36.137)(R 83.978, F -156.252)]  [G loss: 158.212] \n",
      "258 [D loss: (-38.765)(R 95.702, F -173.232)]  [G loss: 177.216] \n",
      "259 [D loss: (-45.488)(R 107.527, F -198.503)]  [G loss: 201.141] \n",
      "259 [D loss: (-49.922)(R 124.831, F -224.676)]  [G loss: 228.051] \n",
      "260 [D loss: (-54.922)(R 144.345, F -254.189)]  [G loss: 261.206] \n",
      "260 [D loss: (-65.399)(R 166.871, F -297.668)]  [G loss: 299.078] \n",
      "261 [D loss: (-73.712)(R 192.752, F -340.177)]  [G loss: 346.515] \n",
      "261 [D loss: (-83.023)(R 229.349, F -395.395)]  [G loss: 408.266] \n",
      "262 [D loss: (-100.167)(R 273.358, F -473.692)]  [G loss: 480.478] \n",
      "262 [D loss: (-111.487)(R 323.685, F -546.659)]  [G loss: 572.502] \n",
      "263 [D loss: (-145.756)(R 379.685, F -671.197)]  [G loss: 688.196] \n",
      "263 [D loss: (-170.878)(R 478.389, F -820.146)]  [G loss: 845.544] \n",
      "264 [D loss: (-219.956)(R 576.361, F -1016.273)]  [G loss: 1037.962] \n",
      "264 [D loss: (-256.103)(R 720.756, F -1232.962)]  [G loss: 1268.529] \n",
      "265 [D loss: (-311.125)(R 850.732, F -1472.981)]  [G loss: 1474.161] \n",
      "265 [D loss: (-329.462)(R 991.952, F -1650.875)]  [G loss: 1650.147] \n",
      "266 [D loss: (-343.484)(R 1106.597, F -1793.565)]  [G loss: 1813.310] \n",
      "266 [D loss: (-368.179)(R 1184.471, F -1920.829)]  [G loss: 1935.319] \n",
      "267 [D loss: (-347.225)(R 1327.619, F -2022.068)]  [G loss: 2098.455] \n",
      "267 [D loss: (-376.436)(R 1452.310, F -2205.181)]  [G loss: 2249.439] \n",
      "268 [D loss: (-395.340)(R 1532.063, F -2322.743)]  [G loss: 2399.477] \n",
      "268 [D loss: (-340.235)(R 1711.606, F -2392.076)]  [G loss: 2554.105] \n",
      "269 [D loss: (-293.773)(R 1821.072, F -2408.617)]  [G loss: 2613.461] \n",
      "269 [D loss: (-229.842)(R 2010.908, F -2470.591)]  [G loss: 2673.425] \n",
      "270 [D loss: (-208.242)(R 2008.213, F -2424.697)]  [G loss: 2667.720] \n",
      "270 [D loss: (-171.478)(R 2035.978, F -2378.933)]  [G loss: 2624.090] \n",
      "271 [D loss: (-80.988)(R 2144.325, F -2306.302)]  [G loss: 2571.848] \n",
      "271 [D loss: (5.135)(R 2139.100, F -2128.831)]  [G loss: 2487.773] \n",
      "272 [D loss: (11.700)(R 2075.229, F -2051.829)]  [G loss: 2396.727] \n",
      "272 [D loss: (86.410)(R 2107.386, F -1934.565)]  [G loss: 2271.369] \n",
      "273 [D loss: (76.048)(R 1958.928, F -1806.831)]  [G loss: 2156.205] \n",
      "273 [D loss: (107.056)(R 1892.264, F -1678.152)]  [G loss: 2031.939] \n",
      "274 [D loss: (72.230)(R 1776.927, F -1632.467)]  [G loss: 1830.665] \n",
      "274 [D loss: (101.823)(R 1658.946, F -1455.300)]  [G loss: 1666.776] \n",
      "275 [D loss: (129.637)(R 1572.504, F -1313.230)]  [G loss: 1530.002] \n",
      "275 [D loss: (72.565)(R 1406.543, F -1261.413)]  [G loss: 1380.496] \n",
      "276 [D loss: (125.878)(R 1333.308, F -1081.552)]  [G loss: 1254.519] \n",
      "276 [D loss: (87.886)(R 1181.133, F -1005.361)]  [G loss: 1163.755] \n",
      "277 [D loss: (91.266)(R 1110.224, F -927.692)]  [G loss: 1054.107] \n",
      "277 [D loss: (20.240)(R 1000.329, F -959.850)]  [G loss: 944.987] \n",
      "278 [D loss: (64.947)(R 961.541, F -831.647)]  [G loss: 903.489] \n",
      "278 [D loss: (54.521)(R 884.989, F -775.948)]  [G loss: 806.866] \n",
      "279 [D loss: (69.867)(R 840.358, F -700.625)]  [G loss: 759.988] \n",
      "279 [D loss: (69.147)(R 796.483, F -658.189)]  [G loss: 707.806] \n",
      "280 [D loss: (52.002)(R 717.958, F -613.953)]  [G loss: 652.145] \n",
      "280 [D loss: (37.867)(R 682.343, F -606.608)]  [G loss: 625.190] \n",
      "281 [D loss: (46.618)(R 662.099, F -568.862)]  [G loss: 586.694] \n",
      "281 [D loss: (33.609)(R 624.202, F -556.983)]  [G loss: 539.204] \n",
      "282 [D loss: (45.808)(R 589.140, F -497.523)]  [G loss: 510.232] \n",
      "282 [D loss: (31.849)(R 556.839, F -493.140)]  [G loss: 473.849] \n",
      "283 [D loss: (33.099)(R 508.324, F -442.126)]  [G loss: 461.538] \n",
      "283 [D loss: (42.296)(R 489.602, F -405.010)]  [G loss: 431.339] \n",
      "284 [D loss: (34.317)(R 470.296, F -401.661)]  [G loss: 398.200] \n",
      "284 [D loss: (34.777)(R 438.489, F -368.935)]  [G loss: 368.855] \n",
      "285 [D loss: (34.838)(R 422.689, F -353.014)]  [G loss: 352.406] \n",
      "285 [D loss: (37.458)(R 407.818, F -332.902)]  [G loss: 332.018] \n",
      "286 [D loss: (33.155)(R 381.223, F -314.912)]  [G loss: 317.908] \n",
      "286 [D loss: (32.856)(R 357.092, F -291.380)]  [G loss: 299.130] \n",
      "287 [D loss: (27.415)(R 338.087, F -283.258)]  [G loss: 279.124] \n",
      "287 [D loss: (25.423)(R 312.232, F -261.385)]  [G loss: 256.332] \n",
      "288 [D loss: (37.385)(R 317.507, F -242.737)]  [G loss: 249.891] \n",
      "288 [D loss: (29.014)(R 282.416, F -224.388)]  [G loss: 235.340] \n",
      "289 [D loss: (17.902)(R 264.205, F -228.401)]  [G loss: 228.635] \n",
      "289 [D loss: (20.903)(R 261.659, F -219.853)]  [G loss: 210.407] \n",
      "290 [D loss: (22.723)(R 250.549, F -205.103)]  [G loss: 204.068] \n",
      "290 [D loss: (23.290)(R 241.331, F -194.752)]  [G loss: 189.788] \n",
      "291 [D loss: (24.645)(R 228.106, F -178.816)]  [G loss: 181.746] \n",
      "291 [D loss: (28.265)(R 219.540, F -163.011)]  [G loss: 175.506] \n",
      "292 [D loss: (17.605)(R 205.101, F -169.890)]  [G loss: 162.581] \n",
      "292 [D loss: (22.536)(R 193.616, F -148.544)]  [G loss: 154.651] \n",
      "293 [D loss: (19.900)(R 183.385, F -143.584)]  [G loss: 148.054] \n",
      "293 [D loss: (16.518)(R 171.340, F -138.303)]  [G loss: 138.027] \n",
      "294 [D loss: (18.988)(R 169.065, F -131.090)]  [G loss: 134.932] \n",
      "294 [D loss: (15.043)(R 156.815, F -126.728)]  [G loss: 127.883] \n",
      "295 [D loss: (20.444)(R 156.393, F -115.505)]  [G loss: 120.083] \n",
      "295 [D loss: (14.386)(R 144.851, F -116.079)]  [G loss: 118.273] \n",
      "296 [D loss: (11.882)(R 140.242, F -116.478)]  [G loss: 108.295] \n",
      "296 [D loss: (18.445)(R 139.171, F -102.281)]  [G loss: 104.094] \n",
      "297 [D loss: (13.264)(R 129.846, F -103.318)]  [G loss: 100.852] \n",
      "297 [D loss: (8.956)(R 120.217, F -102.305)]  [G loss: 96.205] \n",
      "298 [D loss: (11.494)(R 116.252, F -93.264)]  [G loss: 89.223] \n",
      "298 [D loss: (15.145)(R 115.578, F -85.288)]  [G loss: 88.144] \n",
      "299 [D loss: (13.189)(R 111.030, F -84.652)]  [G loss: 85.184] \n",
      "299 [D loss: (12.372)(R 107.160, F -82.417)]  [G loss: 80.635] \n",
      "300 [D loss: (12.267)(R 104.533, F -79.998)]  [G loss: 79.877] \n",
      "300 [D loss: (12.867)(R 99.913, F -74.179)]  [G loss: 76.541] \n",
      "301 [D loss: (12.152)(R 94.287, F -69.983)]  [G loss: 73.297] \n",
      "301 [D loss: (12.636)(R 91.787, F -66.515)]  [G loss: 70.273] \n",
      "302 [D loss: (11.552)(R 86.681, F -63.577)]  [G loss: 66.374] \n",
      "302 [D loss: (11.511)(R 85.283, F -62.261)]  [G loss: 63.719] \n",
      "303 [D loss: (10.942)(R 84.935, F -63.051)]  [G loss: 61.840] \n",
      "303 [D loss: (8.160)(R 78.726, F -62.406)]  [G loss: 58.432] \n",
      "304 [D loss: (10.021)(R 75.786, F -55.744)]  [G loss: 56.548] \n",
      "304 [D loss: (8.360)(R 73.078, F -56.357)]  [G loss: 56.367] \n",
      "305 [D loss: (9.053)(R 70.276, F -52.170)]  [G loss: 53.267] \n",
      "305 [D loss: (9.480)(R 69.045, F -50.084)]  [G loss: 51.939] \n",
      "306 [D loss: (8.248)(R 66.230, F -49.733)]  [G loss: 49.513] \n",
      "306 [D loss: (7.884)(R 63.837, F -48.069)]  [G loss: 45.937] \n",
      "307 [D loss: (7.281)(R 61.731, F -47.169)]  [G loss: 45.161] \n",
      "307 [D loss: (6.975)(R 59.006, F -45.056)]  [G loss: 44.656] \n",
      "308 [D loss: (8.436)(R 57.159, F -40.287)]  [G loss: 44.000] \n",
      "308 [D loss: (5.506)(R 53.425, F -42.413)]  [G loss: 40.861] \n",
      "309 [D loss: (5.310)(R 52.190, F -41.570)]  [G loss: 39.105] \n",
      "309 [D loss: (6.966)(R 53.513, F -39.582)]  [G loss: 38.365] \n",
      "310 [D loss: (7.244)(R 52.061, F -37.572)]  [G loss: 39.152] \n",
      "310 [D loss: (6.642)(R 49.613, F -36.330)]  [G loss: 36.295] \n",
      "311 [D loss: (4.892)(R 47.412, F -37.627)]  [G loss: 35.222] \n",
      "311 [D loss: (5.076)(R 46.510, F -36.357)]  [G loss: 33.316] \n",
      "312 [D loss: (6.815)(R 44.652, F -31.022)]  [G loss: 33.017] \n",
      "312 [D loss: (4.665)(R 41.494, F -32.164)]  [G loss: 30.917] \n",
      "313 [D loss: (4.945)(R 41.766, F -31.876)]  [G loss: 30.489] \n",
      "313 [D loss: (6.785)(R 42.367, F -28.797)]  [G loss: 29.672] \n",
      "314 [D loss: (5.548)(R 39.084, F -27.989)]  [G loss: 29.732] \n",
      "314 [D loss: (4.972)(R 37.416, F -27.472)]  [G loss: 27.807] \n",
      "315 [D loss: (4.884)(R 36.032, F -26.265)]  [G loss: 26.435] \n",
      "315 [D loss: (4.034)(R 34.275, F -26.207)]  [G loss: 27.074] \n",
      "316 [D loss: (4.938)(R 35.061, F -25.185)]  [G loss: 25.534] \n",
      "316 [D loss: (4.817)(R 34.367, F -24.733)]  [G loss: 24.372] \n",
      "317 [D loss: (4.328)(R 32.275, F -23.619)]  [G loss: 24.696] \n",
      "317 [D loss: (5.534)(R 32.571, F -21.503)]  [G loss: 22.734] \n",
      "318 [D loss: (4.307)(R 31.825, F -23.211)]  [G loss: 22.731] \n",
      "318 [D loss: (4.743)(R 30.412, F -20.925)]  [G loss: 21.146] \n",
      "319 [D loss: (2.964)(R 28.288, F -22.360)]  [G loss: 21.341] \n",
      "319 [D loss: (4.688)(R 28.757, F -19.381)]  [G loss: 20.613] \n",
      "320 [D loss: (3.547)(R 26.111, F -19.016)]  [G loss: 19.797] \n",
      "320 [D loss: (3.147)(R 25.824, F -19.530)]  [G loss: 19.227] \n",
      "321 [D loss: (3.515)(R 26.229, F -19.200)]  [G loss: 19.750] \n",
      "321 [D loss: (3.532)(R 24.420, F -17.356)]  [G loss: 18.387] \n",
      "322 [D loss: (3.712)(R 24.217, F -16.793)]  [G loss: 17.655] \n",
      "322 [D loss: (3.343)(R 23.680, F -16.993)]  [G loss: 16.703] \n",
      "323 [D loss: (3.669)(R 23.469, F -16.132)]  [G loss: 16.362] \n",
      "323 [D loss: (2.523)(R 22.074, F -17.028)]  [G loss: 16.016] \n",
      "324 [D loss: (2.585)(R 21.148, F -15.977)]  [G loss: 16.027] \n",
      "324 [D loss: (2.797)(R 20.217, F -14.623)]  [G loss: 15.433] \n",
      "325 [D loss: (3.194)(R 20.046, F -13.658)]  [G loss: 14.707] \n",
      "325 [D loss: (3.275)(R 20.124, F -13.573)]  [G loss: 14.772] \n",
      "326 [D loss: (3.721)(R 19.489, F -12.047)]  [G loss: 13.493] \n",
      "326 [D loss: (2.739)(R 19.054, F -13.575)]  [G loss: 13.986] \n",
      "327 [D loss: (2.392)(R 18.149, F -13.365)]  [G loss: 12.865] \n",
      "327 [D loss: (2.588)(R 17.725, F -12.548)]  [G loss: 12.943] \n",
      "328 [D loss: (2.491)(R 17.625, F -12.644)]  [G loss: 12.113] \n",
      "328 [D loss: (2.858)(R 16.884, F -11.168)]  [G loss: 12.128] \n",
      "329 [D loss: (2.059)(R 16.287, F -12.168)]  [G loss: 11.393] \n",
      "329 [D loss: (3.002)(R 16.222, F -10.217)]  [G loss: 11.321] \n",
      "330 [D loss: (2.093)(R 15.352, F -11.166)]  [G loss: 10.774] \n",
      "330 [D loss: (2.721)(R 14.839, F -9.396)]  [G loss: 10.407] \n",
      "331 [D loss: (2.232)(R 14.665, F -10.200)]  [G loss: 9.888] \n",
      "331 [D loss: (1.915)(R 13.784, F -9.954)]  [G loss: 9.547] \n",
      "332 [D loss: (1.979)(R 13.652, F -9.693)]  [G loss: 9.502] \n",
      "332 [D loss: (1.754)(R 12.932, F -9.424)]  [G loss: 9.262] \n",
      "333 [D loss: (1.957)(R 12.466, F -8.552)]  [G loss: 9.000] \n",
      "333 [D loss: (1.900)(R 12.600, F -8.800)]  [G loss: 8.688] \n",
      "334 [D loss: (2.326)(R 12.292, F -7.640)]  [G loss: 8.630] \n",
      "334 [D loss: (1.691)(R 11.346, F -7.963)]  [G loss: 7.961] \n",
      "335 [D loss: (1.760)(R 11.035, F -7.514)]  [G loss: 7.692] \n",
      "335 [D loss: (1.603)(R 10.814, F -7.608)]  [G loss: 7.818] \n",
      "336 [D loss: (2.184)(R 11.291, F -6.923)]  [G loss: 7.327] \n",
      "336 [D loss: (1.325)(R 9.826, F -7.175)]  [G loss: 6.931] \n",
      "337 [D loss: (1.616)(R 9.979, F -6.746)]  [G loss: 6.962] \n",
      "337 [D loss: (1.368)(R 9.472, F -6.737)]  [G loss: 6.793] \n",
      "338 [D loss: (1.308)(R 8.965, F -6.348)]  [G loss: 6.424] \n",
      "338 [D loss: (1.460)(R 9.115, F -6.194)]  [G loss: 6.365] \n",
      "339 [D loss: (1.578)(R 8.986, F -5.830)]  [G loss: 5.987] \n",
      "339 [D loss: (0.877)(R 8.087, F -6.333)]  [G loss: 5.631] \n",
      "340 [D loss: (1.043)(R 7.993, F -5.908)]  [G loss: 5.864] \n",
      "340 [D loss: (1.070)(R 7.730, F -5.590)]  [G loss: 5.542] \n",
      "341 [D loss: (1.115)(R 7.543, F -5.312)]  [G loss: 5.188] \n",
      "341 [D loss: (1.351)(R 7.270, F -4.568)]  [G loss: 4.951] \n",
      "342 [D loss: (1.170)(R 7.333, F -4.994)]  [G loss: 4.720] \n",
      "342 [D loss: (1.177)(R 6.951, F -4.597)]  [G loss: 4.651] \n",
      "343 [D loss: (1.018)(R 6.630, F -4.594)]  [G loss: 4.188] \n",
      "343 [D loss: (1.066)(R 6.362, F -4.231)]  [G loss: 3.838] \n",
      "344 [D loss: (1.190)(R 6.268, F -3.888)]  [G loss: 3.950] \n",
      "344 [D loss: (1.102)(R 5.731, F -3.526)]  [G loss: 3.790] \n",
      "345 [D loss: (1.137)(R 5.375, F -3.100)]  [G loss: 3.624] \n",
      "345 [D loss: (0.807)(R 4.754, F -3.140)]  [G loss: 3.354] \n",
      "346 [D loss: (0.696)(R 5.106, F -3.715)]  [G loss: 3.195] \n",
      "346 [D loss: (0.785)(R 4.826, F -3.256)]  [G loss: 3.250] \n",
      "347 [D loss: (0.912)(R 4.689, F -2.865)]  [G loss: 3.088] \n",
      "347 [D loss: (0.619)(R 4.255, F -3.018)]  [G loss: 2.814] \n",
      "348 [D loss: (0.519)(R 4.107, F -3.068)]  [G loss: 2.637] \n",
      "348 [D loss: (0.500)(R 3.692, F -2.691)]  [G loss: 2.161] \n",
      "349 [D loss: (0.790)(R 3.834, F -2.254)]  [G loss: 2.562] \n",
      "349 [D loss: (0.711)(R 3.610, F -2.189)]  [G loss: 2.414] \n",
      "350 [D loss: (0.750)(R 3.368, F -1.868)]  [G loss: 1.867] \n",
      "350 [D loss: (0.654)(R 3.015, F -1.706)]  [G loss: 1.665] \n",
      "351 [D loss: (0.923)(R 3.235, F -1.388)]  [G loss: 1.707] \n",
      "351 [D loss: (0.724)(R 2.897, F -1.449)]  [G loss: 1.376] \n",
      "352 [D loss: (0.370)(R 2.363, F -1.622)]  [G loss: 1.251] \n",
      "352 [D loss: (0.567)(R 2.183, F -1.049)]  [G loss: 1.305] \n",
      "353 [D loss: (0.768)(R 2.073, F -0.537)]  [G loss: 1.090] \n",
      "353 [D loss: (0.523)(R 1.911, F -0.865)]  [G loss: 0.549] \n",
      "354 [D loss: (0.453)(R 1.564, F -0.657)]  [G loss: 0.689] \n",
      "354 [D loss: (0.553)(R 1.684, F -0.579)]  [G loss: 0.688] \n",
      "355 [D loss: (0.288)(R 1.192, F -0.615)]  [G loss: 0.381] \n",
      "355 [D loss: (0.454)(R 0.891, F 0.016)]  [G loss: 0.187] \n",
      "356 [D loss: (0.413)(R 0.989, F -0.163)]  [G loss: 0.120] \n",
      "356 [D loss: (0.488)(R 0.736, F 0.241)]  [G loss: 0.096] \n",
      "357 [D loss: (0.516)(R 0.634, F 0.397)]  [G loss: -0.241] \n",
      "357 [D loss: (0.422)(R 0.369, F 0.474)]  [G loss: -0.083] \n",
      "358 [D loss: (0.223)(R 0.059, F 0.388)]  [G loss: -0.435] \n",
      "358 [D loss: (0.300)(R 0.087, F 0.513)]  [G loss: -0.609] \n",
      "359 [D loss: (0.322)(R -0.043, F 0.687)]  [G loss: -0.759] \n",
      "359 [D loss: (0.213)(R -0.279, F 0.705)]  [G loss: -1.035] \n",
      "360 [D loss: (0.128)(R -0.584, F 0.841)]  [G loss: -0.962] \n",
      "360 [D loss: (0.359)(R -0.695, F 1.414)]  [G loss: -0.993] \n",
      "361 [D loss: (0.214)(R -0.699, F 1.128)]  [G loss: -1.222] \n",
      "361 [D loss: (0.416)(R -1.037, F 1.868)]  [G loss: -1.345] \n",
      "362 [D loss: (0.074)(R -1.369, F 1.518)]  [G loss: -1.617] \n",
      "362 [D loss: (0.125)(R -1.479, F 1.730)]  [G loss: -1.773] \n",
      "363 [D loss: (-0.004)(R -1.816, F 1.808)]  [G loss: -2.028] \n",
      "363 [D loss: (0.075)(R -2.009, F 2.159)]  [G loss: -2.068] \n",
      "364 [D loss: (0.181)(R -1.977, F 2.338)]  [G loss: -2.076] \n",
      "364 [D loss: (-0.044)(R -2.267, F 2.179)]  [G loss: -2.370] \n",
      "365 [D loss: (0.075)(R -2.381, F 2.531)]  [G loss: -2.606] \n",
      "365 [D loss: (-0.238)(R -2.723, F 2.247)]  [G loss: -2.594] \n",
      "366 [D loss: (-0.021)(R -3.039, F 2.997)]  [G loss: -2.734] \n",
      "366 [D loss: (-0.139)(R -3.053, F 2.776)]  [G loss: -2.933] \n",
      "367 [D loss: (-0.112)(R -3.354, F 3.130)]  [G loss: -3.112] \n",
      "367 [D loss: (-0.099)(R -3.457, F 3.259)]  [G loss: -3.182] \n",
      "368 [D loss: (-0.083)(R -3.582, F 3.415)]  [G loss: -3.174] \n",
      "368 [D loss: (-0.332)(R -3.781, F 3.116)]  [G loss: -3.488] \n",
      "369 [D loss: (-0.340)(R -4.259, F 3.579)]  [G loss: -3.629] \n",
      "369 [D loss: (-0.192)(R -4.385, F 4.002)]  [G loss: -3.818] \n",
      "370 [D loss: (-0.389)(R -4.645, F 3.867)]  [G loss: -3.989] \n",
      "370 [D loss: (-0.361)(R -4.575, F 3.853)]  [G loss: -3.974] \n",
      "371 [D loss: (-0.328)(R -5.013, F 4.357)]  [G loss: -4.387] \n",
      "371 [D loss: (-0.368)(R -5.199, F 4.462)]  [G loss: -4.480] \n",
      "372 [D loss: (-0.278)(R -5.348, F 4.792)]  [G loss: -4.697] \n",
      "372 [D loss: (-0.431)(R -5.732, F 4.869)]  [G loss: -4.720] \n",
      "373 [D loss: (-0.656)(R -6.044, F 4.732)]  [G loss: -5.009] \n",
      "373 [D loss: (-0.388)(R -6.141, F 5.366)]  [G loss: -5.072] \n",
      "374 [D loss: (-0.613)(R -6.366, F 5.140)]  [G loss: -5.377] \n",
      "374 [D loss: (-0.476)(R -6.677, F 5.725)]  [G loss: -5.589] \n",
      "375 [D loss: (-0.610)(R -6.880, F 5.660)]  [G loss: -5.764] \n",
      "375 [D loss: (-0.658)(R -7.126, F 5.809)]  [G loss: -5.875] \n",
      "376 [D loss: (-0.845)(R -7.444, F 5.754)]  [G loss: -6.214] \n",
      "376 [D loss: (-0.685)(R -7.558, F 6.188)]  [G loss: -6.544] \n",
      "377 [D loss: (-0.709)(R -7.957, F 6.540)]  [G loss: -6.737] \n",
      "377 [D loss: (-0.723)(R -8.297, F 6.851)]  [G loss: -6.901] \n",
      "378 [D loss: (-1.032)(R -8.775, F 6.711)]  [G loss: -7.116] \n",
      "378 [D loss: (-0.739)(R -9.007, F 7.529)]  [G loss: -7.327] \n",
      "379 [D loss: (-0.782)(R -9.100, F 7.537)]  [G loss: -7.649] \n",
      "379 [D loss: (-0.908)(R -9.522, F 7.706)]  [G loss: -7.727] \n",
      "380 [D loss: (-1.063)(R -9.832, F 7.706)]  [G loss: -7.769] \n",
      "380 [D loss: (-1.095)(R -10.371, F 8.182)]  [G loss: -8.483] \n",
      "381 [D loss: (-0.816)(R -10.608, F 8.977)]  [G loss: -8.571] \n",
      "381 [D loss: (-1.047)(R -10.716, F 8.623)]  [G loss: -8.634] \n",
      "382 [D loss: (-1.015)(R -11.277, F 9.247)]  [G loss: -9.233] \n",
      "382 [D loss: (-0.969)(R -11.566, F 9.628)]  [G loss: -9.407] \n",
      "383 [D loss: (-1.506)(R -12.188, F 9.176)]  [G loss: -9.646] \n",
      "383 [D loss: (-1.157)(R -12.347, F 10.032)]  [G loss: -10.182] \n",
      "384 [D loss: (-1.330)(R -12.995, F 10.335)]  [G loss: -10.421] \n",
      "384 [D loss: (-1.318)(R -13.265, F 10.630)]  [G loss: -10.841] \n",
      "385 [D loss: (-1.430)(R -14.105, F 11.244)]  [G loss: -11.185] \n",
      "385 [D loss: (-1.300)(R -14.090, F 11.490)]  [G loss: -11.833] \n",
      "386 [D loss: (-1.647)(R -14.830, F 11.537)]  [G loss: -11.654] \n",
      "386 [D loss: (-1.240)(R -14.994, F 12.514)]  [G loss: -11.999] \n",
      "387 [D loss: (-1.074)(R -15.805, F 13.657)]  [G loss: -12.409] \n",
      "387 [D loss: (-1.797)(R -16.428, F 12.834)]  [G loss: -13.089] \n",
      "388 [D loss: (-1.766)(R -17.200, F 13.668)]  [G loss: -13.763] \n",
      "388 [D loss: (-2.174)(R -17.694, F 13.345)]  [G loss: -13.982] \n",
      "389 [D loss: (-1.672)(R -18.057, F 14.714)]  [G loss: -14.528] \n",
      "389 [D loss: (-1.806)(R -18.949, F 15.337)]  [G loss: -15.122] \n",
      "390 [D loss: (-1.863)(R -19.243, F 15.518)]  [G loss: -15.764] \n",
      "390 [D loss: (-2.013)(R -20.293, F 16.267)]  [G loss: -16.386] \n",
      "391 [D loss: (-1.983)(R -20.780, F 16.815)]  [G loss: -16.766] \n",
      "391 [D loss: (-1.876)(R -22.034, F 18.281)]  [G loss: -17.657] \n",
      "392 [D loss: (-2.592)(R -23.198, F 18.014)]  [G loss: -18.277] \n",
      "392 [D loss: (-3.051)(R -24.575, F 18.472)]  [G loss: -19.148] \n",
      "393 [D loss: (-2.891)(R -25.485, F 19.704)]  [G loss: -20.189] \n",
      "393 [D loss: (-2.342)(R -25.601, F 20.917)]  [G loss: -21.228] \n",
      "394 [D loss: (-2.643)(R -26.988, F 21.702)]  [G loss: -21.416] \n",
      "394 [D loss: (-3.034)(R -28.492, F 22.425)]  [G loss: -22.465] \n",
      "395 [D loss: (-2.824)(R -29.118, F 23.471)]  [G loss: -23.875] \n",
      "395 [D loss: (-3.354)(R -31.764, F 25.057)]  [G loss: -24.837] \n",
      "396 [D loss: (-2.862)(R -32.175, F 26.451)]  [G loss: -25.916] \n",
      "396 [D loss: (-3.382)(R -34.048, F 27.284)]  [G loss: -26.754] \n",
      "397 [D loss: (-3.805)(R -35.487, F 27.877)]  [G loss: -27.668] \n",
      "397 [D loss: (-3.421)(R -36.332, F 29.490)]  [G loss: -28.864] \n",
      "398 [D loss: (-3.677)(R -38.429, F 31.075)]  [G loss: -30.297] \n",
      "398 [D loss: (-5.246)(R -41.306, F 30.814)]  [G loss: -31.708] \n",
      "399 [D loss: (-4.893)(R -43.119, F 33.334)]  [G loss: -33.772] \n",
      "399 [D loss: (-4.971)(R -45.774, F 35.833)]  [G loss: -35.926] \n",
      "400 [D loss: (-5.688)(R -46.726, F 35.350)]  [G loss: -37.482] \n",
      "400 [D loss: (-5.576)(R -49.498, F 38.347)]  [G loss: -39.199] \n",
      "401 [D loss: (-5.326)(R -52.823, F 42.171)]  [G loss: -42.395] \n",
      "401 [D loss: (-6.465)(R -56.859, F 43.929)]  [G loss: -44.141] \n",
      "402 [D loss: (-6.180)(R -58.360, F 45.999)]  [G loss: -46.589] \n",
      "402 [D loss: (-8.137)(R -64.291, F 48.017)]  [G loss: -49.468] \n",
      "403 [D loss: (-6.653)(R -67.971, F 54.666)]  [G loss: -53.307] \n",
      "403 [D loss: (-6.750)(R -71.233, F 57.733)]  [G loss: -55.562] \n",
      "404 [D loss: (-8.559)(R -76.884, F 59.766)]  [G loss: -58.573] \n",
      "404 [D loss: (-8.600)(R -82.207, F 65.006)]  [G loss: -63.463] \n",
      "405 [D loss: (-9.977)(R -86.682, F 66.727)]  [G loss: -68.243] \n",
      "405 [D loss: (-8.646)(R -94.528, F 77.236)]  [G loss: -72.731] \n",
      "406 [D loss: (-10.293)(R -98.740, F 78.154)]  [G loss: -78.988] \n",
      "406 [D loss: (-11.239)(R -109.752, F 87.274)]  [G loss: -85.041] \n",
      "407 [D loss: (-9.858)(R -116.140, F 96.424)]  [G loss: -92.691] \n",
      "407 [D loss: (-12.939)(R -128.036, F 102.159)]  [G loss: -100.780] \n",
      "408 [D loss: (-14.355)(R -139.853, F 111.144)]  [G loss: -111.424] \n",
      "408 [D loss: (-17.020)(R -153.082, F 119.041)]  [G loss: -123.584] \n",
      "409 [D loss: (-18.281)(R -172.005, F 135.443)]  [G loss: -136.718] \n",
      "409 [D loss: (-21.998)(R -194.623, F 150.627)]  [G loss: -152.057] \n",
      "410 [D loss: (-28.257)(R -219.081, F 162.567)]  [G loss: -170.577] \n",
      "410 [D loss: (-34.079)(R -249.683, F 181.524)]  [G loss: -192.505] \n",
      "411 [D loss: (-33.912)(R -280.127, F 212.304)]  [G loss: -226.308] \n",
      "411 [D loss: (-38.835)(R -325.243, F 247.573)]  [G loss: -262.527] \n",
      "412 [D loss: (-39.844)(R -380.896, F 301.209)]  [G loss: -303.565] \n",
      "412 [D loss: (-37.768)(R -432.939, F 357.402)]  [G loss: -352.132] \n",
      "413 [D loss: (-49.259)(R -523.699, F 425.182)]  [G loss: -401.222] \n",
      "413 [D loss: (-43.852)(R -592.259, F 504.555)]  [G loss: -467.493] \n",
      "414 [D loss: (-69.146)(R -692.626, F 554.333)]  [G loss: -543.351] \n",
      "414 [D loss: (-90.410)(R -817.587, F 636.767)]  [G loss: -641.936] \n",
      "415 [D loss: (-99.899)(R -976.693, F 776.895)]  [G loss: -764.904] \n",
      "415 [D loss: (-110.535)(R -1141.500, F 920.430)]  [G loss: -912.855] \n",
      "416 [D loss: (-159.680)(R -1333.142, F 1013.782)]  [G loss: -1047.323] \n",
      "416 [D loss: (-164.988)(R -1484.329, F 1154.353)]  [G loss: -1172.407] \n",
      "417 [D loss: (-167.640)(R -1583.545, F 1248.265)]  [G loss: -1228.314] \n",
      "417 [D loss: (-161.169)(R -1648.667, F 1326.328)]  [G loss: -1263.328] \n",
      "418 [D loss: (-164.337)(R -1671.444, F 1342.771)]  [G loss: -1330.706] \n",
      "418 [D loss: (-142.505)(R -1670.885, F 1385.875)]  [G loss: -1326.531] \n",
      "419 [D loss: (-170.536)(R -1697.253, F 1356.181)]  [G loss: -1328.480] \n",
      "419 [D loss: (-180.177)(R -1700.638, F 1340.285)]  [G loss: -1368.163] \n",
      "420 [D loss: (-115.459)(R -1671.493, F 1440.575)]  [G loss: -1382.191] \n",
      "420 [D loss: (-131.485)(R -1709.338, F 1446.369)]  [G loss: -1396.756] \n",
      "421 [D loss: (-107.177)(R -1703.538, F 1489.185)]  [G loss: -1401.400] \n",
      "421 [D loss: (-136.616)(R -1733.715, F 1460.483)]  [G loss: -1436.319] \n",
      "422 [D loss: (-164.109)(R -1738.140, F 1409.923)]  [G loss: -1428.157] \n",
      "422 [D loss: (-94.330)(R -1733.241, F 1544.581)]  [G loss: -1432.117] \n",
      "423 [D loss: (-101.519)(R -1734.414, F 1531.375)]  [G loss: -1473.764] \n",
      "423 [D loss: (-91.438)(R -1716.998, F 1534.123)]  [G loss: -1480.027] \n",
      "424 [D loss: (-122.962)(R -1755.116, F 1509.192)]  [G loss: -1489.978] \n",
      "424 [D loss: (-103.824)(R -1710.617, F 1502.968)]  [G loss: -1513.117] \n",
      "425 [D loss: (-76.349)(R -1693.728, F 1541.029)]  [G loss: -1536.897] \n",
      "425 [D loss: (-91.627)(R -1725.589, F 1542.336)]  [G loss: -1579.832] \n",
      "426 [D loss: (-37.389)(R -1726.556, F 1651.777)]  [G loss: -1579.292] \n",
      "426 [D loss: (-45.192)(R -1733.219, F 1642.835)]  [G loss: -1578.599] \n",
      "427 [D loss: (-39.379)(R -1697.657, F 1618.899)]  [G loss: -1610.316] \n",
      "427 [D loss: (-5.576)(R -1685.249, F 1674.097)]  [G loss: -1622.797] \n",
      "428 [D loss: (-4.373)(R -1711.757, F 1703.010)]  [G loss: -1629.896] \n",
      "428 [D loss: (1.385)(R -1693.667, F 1696.436)]  [G loss: -1639.945] \n",
      "429 [D loss: (-6.861)(R -1682.550, F 1668.828)]  [G loss: -1648.313] \n",
      "429 [D loss: (-3.861)(R -1690.832, F 1683.109)]  [G loss: -1670.366] \n",
      "430 [D loss: (-11.334)(R -1684.416, F 1661.748)]  [G loss: -1668.630] \n",
      "430 [D loss: (49.413)(R -1607.792, F 1706.618)]  [G loss: -1652.683] \n",
      "431 [D loss: (79.171)(R -1553.450, F 1711.791)]  [G loss: -1608.140] \n",
      "431 [D loss: (41.854)(R -1503.235, F 1586.944)]  [G loss: -1567.337] \n",
      "432 [D loss: (62.388)(R -1439.884, F 1564.659)]  [G loss: -1511.407] \n",
      "432 [D loss: (84.708)(R -1355.770, F 1525.186)]  [G loss: -1442.454] \n",
      "433 [D loss: (108.110)(R -1272.774, F 1488.995)]  [G loss: -1384.234] \n",
      "433 [D loss: (80.475)(R -1203.753, F 1364.702)]  [G loss: -1289.161] \n",
      "434 [D loss: (101.199)(R -1118.810, F 1321.209)]  [G loss: -1209.272] \n",
      "434 [D loss: (85.642)(R -1027.936, F 1199.220)]  [G loss: -1137.032] \n",
      "435 [D loss: (96.863)(R -921.782, F 1115.509)]  [G loss: -1056.138] \n",
      "435 [D loss: (83.014)(R -875.586, F 1041.614)]  [G loss: -978.170] \n",
      "436 [D loss: (86.802)(R -786.790, F 960.393)]  [G loss: -902.331] \n",
      "436 [D loss: (95.622)(R -726.196, F 917.440)]  [G loss: -845.998] \n",
      "437 [D loss: (74.639)(R -668.486, F 817.764)]  [G loss: -784.504] \n",
      "437 [D loss: (68.828)(R -613.233, F 750.888)]  [G loss: -713.324] \n",
      "438 [D loss: (50.510)(R -574.594, F 675.613)]  [G loss: -668.680] \n",
      "438 [D loss: (66.876)(R -509.353, F 643.106)]  [G loss: -616.382] \n",
      "439 [D loss: (65.367)(R -462.974, F 593.707)]  [G loss: -574.545] \n",
      "439 [D loss: (53.059)(R -437.380, F 543.499)]  [G loss: -524.080] \n",
      "440 [D loss: (52.320)(R -395.851, F 500.491)]  [G loss: -486.195] \n",
      "440 [D loss: (50.138)(R -360.137, F 460.414)]  [G loss: -446.520] \n",
      "441 [D loss: (49.732)(R -331.967, F 431.431)]  [G loss: -412.419] \n",
      "441 [D loss: (44.588)(R -300.808, F 389.985)]  [G loss: -376.777] \n",
      "442 [D loss: (33.475)(R -281.037, F 347.988)]  [G loss: -346.152] \n",
      "442 [D loss: (35.103)(R -255.001, F 325.207)]  [G loss: -314.242] \n",
      "443 [D loss: (34.472)(R -224.905, F 293.848)]  [G loss: -284.778] \n",
      "443 [D loss: (35.794)(R -199.640, F 271.227)]  [G loss: -260.359] \n",
      "444 [D loss: (28.042)(R -182.390, F 238.473)]  [G loss: -230.387] \n",
      "444 [D loss: (27.792)(R -163.673, F 219.256)]  [G loss: -205.325] \n",
      "445 [D loss: (23.397)(R -142.031, F 188.824)]  [G loss: -180.225] \n",
      "445 [D loss: (16.802)(R -125.554, F 159.159)]  [G loss: -156.643] \n",
      "446 [D loss: (13.653)(R -105.429, F 132.735)]  [G loss: -129.337] \n",
      "446 [D loss: (10.348)(R -89.545, F 110.241)]  [G loss: -104.636] \n",
      "447 [D loss: (6.190)(R -73.146, F 85.527)]  [G loss: -81.119] \n",
      "447 [D loss: (2.027)(R -56.017, F 60.070)]  [G loss: -58.643] \n",
      "448 [D loss: (-1.189)(R -38.239, F 35.861)]  [G loss: -35.004] \n",
      "448 [D loss: (-5.461)(R -23.016, F 12.093)]  [G loss: -9.272] \n",
      "449 [D loss: (-9.273)(R -5.185, F -13.361)]  [G loss: 14.873] \n",
      "449 [D loss: (-12.750)(R 10.585, F -36.085)]  [G loss: 39.001] \n",
      "450 [D loss: (-16.875)(R 28.129, F -61.879)]  [G loss: 63.839] \n",
      "450 [D loss: (-21.267)(R 46.092, F -88.626)]  [G loss: 91.768] \n",
      "451 [D loss: (-25.846)(R 63.449, F -115.142)]  [G loss: 116.937] \n",
      "451 [D loss: (-31.554)(R 78.786, F -141.894)]  [G loss: 143.740] \n",
      "452 [D loss: (-35.339)(R 100.904, F -171.581)]  [G loss: 172.560] \n",
      "452 [D loss: (-42.258)(R 118.426, F -202.942)]  [G loss: 200.385] \n",
      "453 [D loss: (-44.954)(R 140.488, F -230.395)]  [G loss: 231.983] \n",
      "453 [D loss: (-49.645)(R 165.535, F -264.824)]  [G loss: 266.854] \n",
      "454 [D loss: (-60.774)(R 181.352, F -302.900)]  [G loss: 303.840] \n",
      "454 [D loss: (-68.396)(R 212.621, F -349.414)]  [G loss: 347.396] \n",
      "455 [D loss: (-76.109)(R 242.837, F -395.054)]  [G loss: 389.666] \n",
      "455 [D loss: (-79.995)(R 284.415, F -444.404)]  [G loss: 442.336] \n",
      "456 [D loss: (-93.844)(R 310.954, F -498.642)]  [G loss: 496.029] \n",
      "456 [D loss: (-97.677)(R 353.496, F -548.850)]  [G loss: 549.328] \n",
      "457 [D loss: (-110.347)(R 387.276, F -607.970)]  [G loss: 613.341] \n",
      "457 [D loss: (-114.517)(R 437.936, F -666.970)]  [G loss: 682.605] \n",
      "458 [D loss: (-125.121)(R 492.247, F -742.490)]  [G loss: 748.498] \n",
      "458 [D loss: (-143.370)(R 534.292, F -821.031)]  [G loss: 827.232] \n",
      "459 [D loss: (-159.889)(R 580.479, F -900.257)]  [G loss: 907.364] \n",
      "459 [D loss: (-150.324)(R 659.339, F -959.987)]  [G loss: 983.912] \n",
      "460 [D loss: (-159.548)(R 735.609, F -1054.704)]  [G loss: 1063.343] \n",
      "460 [D loss: (-174.556)(R 788.898, F -1138.009)]  [G loss: 1139.427] \n",
      "461 [D loss: (-170.081)(R 836.390, F -1176.553)]  [G loss: 1205.982] \n",
      "461 [D loss: (-189.465)(R 876.871, F -1255.800)]  [G loss: 1250.967] \n",
      "462 [D loss: (-178.201)(R 936.599, F -1293.000)]  [G loss: 1305.265] \n",
      "462 [D loss: (-173.234)(R 935.730, F -1282.199)]  [G loss: 1319.318] \n",
      "463 [D loss: (-184.361)(R 958.852, F -1327.573)]  [G loss: 1340.351] \n",
      "463 [D loss: (-166.383)(R 970.839, F -1303.605)]  [G loss: 1342.651] \n",
      "464 [D loss: (-155.316)(R 1014.987, F -1325.618)]  [G loss: 1347.847] \n",
      "464 [D loss: (-163.224)(R 1009.197, F -1335.644)]  [G loss: 1344.905] \n",
      "465 [D loss: (-142.232)(R 1046.878, F -1331.342)]  [G loss: 1354.795] \n",
      "465 [D loss: (-139.159)(R 1029.201, F -1307.519)]  [G loss: 1348.361] \n",
      "466 [D loss: (-121.724)(R 1038.396, F -1281.843)]  [G loss: 1333.266] \n",
      "466 [D loss: (-135.907)(R 1025.892, F -1297.706)]  [G loss: 1344.671] \n",
      "467 [D loss: (-135.264)(R 1053.529, F -1324.057)]  [G loss: 1346.164] \n",
      "467 [D loss: (-107.028)(R 1073.508, F -1287.564)]  [G loss: 1347.555] \n",
      "468 [D loss: (-84.970)(R 1098.023, F -1267.963)]  [G loss: 1340.167] \n",
      "468 [D loss: (-84.363)(R 1099.450, F -1268.176)]  [G loss: 1335.605] \n",
      "469 [D loss: (-127.066)(R 1050.237, F -1304.369)]  [G loss: 1337.515] \n",
      "469 [D loss: (-82.800)(R 1079.873, F -1245.473)]  [G loss: 1316.708] \n",
      "470 [D loss: (-111.222)(R 1062.380, F -1284.824)]  [G loss: 1315.600] \n",
      "470 [D loss: (-85.136)(R 1117.624, F -1287.897)]  [G loss: 1334.920] \n",
      "471 [D loss: (-94.217)(R 1084.741, F -1273.176)]  [G loss: 1313.872] \n",
      "471 [D loss: (-87.684)(R 1106.114, F -1281.482)]  [G loss: 1319.209] \n",
      "472 [D loss: (-70.567)(R 1121.597, F -1262.731)]  [G loss: 1316.442] \n",
      "472 [D loss: (-53.630)(R 1146.891, F -1254.152)]  [G loss: 1289.285] \n",
      "473 [D loss: (-52.832)(R 1154.448, F -1260.113)]  [G loss: 1301.676] \n",
      "473 [D loss: (-43.398)(R 1153.225, F -1240.021)]  [G loss: 1305.915] \n",
      "474 [D loss: (-59.044)(R 1165.072, F -1283.159)]  [G loss: 1298.297] \n",
      "474 [D loss: (-45.400)(R 1128.371, F -1219.171)]  [G loss: 1287.209] \n",
      "475 [D loss: (-53.128)(R 1158.774, F -1265.030)]  [G loss: 1273.723] \n",
      "475 [D loss: (-53.623)(R 1147.821, F -1255.066)]  [G loss: 1270.621] \n",
      "476 [D loss: (-51.086)(R 1140.586, F -1242.758)]  [G loss: 1290.077] \n",
      "476 [D loss: (-83.678)(R 1124.066, F -1291.422)]  [G loss: 1258.855] \n",
      "477 [D loss: (-10.157)(R 1178.446, F -1198.760)]  [G loss: 1271.729] \n",
      "477 [D loss: (-56.049)(R 1159.503, F -1271.602)]  [G loss: 1257.211] \n",
      "478 [D loss: (-46.998)(R 1173.417, F -1267.412)]  [G loss: 1249.525] \n",
      "478 [D loss: (-61.246)(R 1146.341, F -1268.833)]  [G loss: 1242.765] \n",
      "479 [D loss: (-17.302)(R 1165.627, F -1200.231)]  [G loss: 1234.563] \n",
      "479 [D loss: (11.590)(R 1212.564, F -1189.383)]  [G loss: 1245.857] \n",
      "480 [D loss: (-42.652)(R 1161.041, F -1246.344)]  [G loss: 1192.679] \n",
      "480 [D loss: (20.441)(R 1180.739, F -1139.857)]  [G loss: 1206.465] \n",
      "481 [D loss: (-46.370)(R 1152.452, F -1245.192)]  [G loss: 1180.497] \n",
      "481 [D loss: (25.684)(R 1185.504, F -1134.135)]  [G loss: 1156.578] \n",
      "482 [D loss: (-11.284)(R 1159.704, F -1182.272)]  [G loss: 1178.480] \n",
      "482 [D loss: (15.093)(R 1161.564, F -1131.378)]  [G loss: 1137.131] \n",
      "483 [D loss: (17.442)(R 1106.329, F -1071.445)]  [G loss: 1153.105] \n",
      "483 [D loss: (5.743)(R 1103.995, F -1092.508)]  [G loss: 1114.093] \n",
      "484 [D loss: (17.559)(R 1097.773, F -1062.654)]  [G loss: 1063.861] \n",
      "484 [D loss: (51.341)(R 1120.162, F -1017.481)]  [G loss: 1077.073] \n",
      "485 [D loss: (32.939)(R 1106.566, F -1040.689)]  [G loss: 1025.870] \n",
      "485 [D loss: (38.686)(R 1071.411, F -994.038)]  [G loss: 1006.249] \n",
      "486 [D loss: (22.095)(R 1036.078, F -991.887)]  [G loss: 994.736] \n",
      "486 [D loss: (24.316)(R 1022.575, F -973.944)]  [G loss: 970.194] \n",
      "487 [D loss: (30.285)(R 1005.965, F -945.396)]  [G loss: 936.135] \n",
      "487 [D loss: (43.270)(R 979.022, F -892.482)]  [G loss: 896.413] \n",
      "488 [D loss: (22.132)(R 911.685, F -867.421)]  [G loss: 881.099] \n",
      "488 [D loss: (40.621)(R 921.907, F -840.665)]  [G loss: 827.863] \n",
      "489 [D loss: (13.535)(R 841.777, F -814.708)]  [G loss: 812.091] \n",
      "489 [D loss: (40.183)(R 850.559, F -770.193)]  [G loss: 784.644] \n",
      "490 [D loss: (37.216)(R 814.441, F -740.009)]  [G loss: 758.679] \n",
      "490 [D loss: (43.540)(R 807.464, F -720.383)]  [G loss: 711.156] \n",
      "491 [D loss: (25.363)(R 769.040, F -718.314)]  [G loss: 689.158] \n",
      "491 [D loss: (47.636)(R 760.252, F -664.980)]  [G loss: 683.099] \n",
      "492 [D loss: (56.149)(R 742.259, F -629.960)]  [G loss: 652.922] \n",
      "492 [D loss: (36.438)(R 701.156, F -628.280)]  [G loss: 611.821] \n",
      "493 [D loss: (41.664)(R 676.163, F -592.835)]  [G loss: 615.576] \n",
      "493 [D loss: (30.770)(R 651.329, F -589.789)]  [G loss: 572.075] \n",
      "494 [D loss: (36.601)(R 642.975, F -569.773)]  [G loss: 554.144] \n",
      "494 [D loss: (44.078)(R 608.008, F -519.852)]  [G loss: 525.964] \n",
      "495 [D loss: (49.971)(R 590.640, F -490.699)]  [G loss: 502.992] \n",
      "495 [D loss: (29.200)(R 547.402, F -489.001)]  [G loss: 493.903] \n",
      "496 [D loss: (47.739)(R 557.028, F -461.550)]  [G loss: 469.320] \n",
      "496 [D loss: (38.408)(R 532.032, F -455.215)]  [G loss: 457.763] \n",
      "497 [D loss: (14.135)(R 498.972, F -470.701)]  [G loss: 442.810] \n",
      "497 [D loss: (26.414)(R 486.357, F -433.529)]  [G loss: 424.866] \n",
      "498 [D loss: (39.074)(R 481.531, F -403.382)]  [G loss: 416.198] \n",
      "498 [D loss: (21.488)(R 456.361, F -413.384)]  [G loss: 395.568] \n",
      "499 [D loss: (34.443)(R 445.990, F -377.104)]  [G loss: 390.592] \n",
      "499 [D loss: (37.816)(R 449.668, F -374.035)]  [G loss: 365.615] \n",
      "500 [D loss: (25.188)(R 401.347, F -350.971)]  [G loss: 356.451] \n",
      "500 [D loss: (32.973)(R 410.812, F -344.866)]  [G loss: 340.114] \n",
      "501 [D loss: (39.408)(R 389.057, F -310.241)]  [G loss: 329.132] \n",
      "501 [D loss: (38.131)(R 371.778, F -295.515)]  [G loss: 303.343] \n",
      "502 [D loss: (16.332)(R 346.645, F -313.980)]  [G loss: 298.628] \n",
      "502 [D loss: (32.622)(R 340.403, F -275.159)]  [G loss: 284.708] \n",
      "503 [D loss: (35.834)(R 342.989, F -271.322)]  [G loss: 279.783] \n",
      "503 [D loss: (29.377)(R 326.293, F -267.539)]  [G loss: 270.902] \n",
      "504 [D loss: (23.066)(R 303.412, F -257.279)]  [G loss: 255.566] \n",
      "504 [D loss: (24.814)(R 304.011, F -254.384)]  [G loss: 250.018] \n",
      "505 [D loss: (24.595)(R 288.139, F -238.950)]  [G loss: 240.139] \n",
      "505 [D loss: (23.627)(R 279.471, F -232.218)]  [G loss: 231.815] \n",
      "506 [D loss: (21.472)(R 264.965, F -222.020)]  [G loss: 225.993] \n",
      "506 [D loss: (27.534)(R 257.554, F -202.487)]  [G loss: 217.674] \n",
      "507 [D loss: (23.612)(R 249.662, F -202.438)]  [G loss: 209.831] \n",
      "507 [D loss: (21.422)(R 243.541, F -200.696)]  [G loss: 202.408] \n",
      "508 [D loss: (25.792)(R 239.020, F -187.437)]  [G loss: 188.617] \n",
      "508 [D loss: (14.206)(R 223.412, F -195.000)]  [G loss: 183.988] \n",
      "509 [D loss: (20.254)(R 226.956, F -186.448)]  [G loss: 175.401] \n",
      "509 [D loss: (16.536)(R 209.167, F -176.094)]  [G loss: 167.364] \n",
      "510 [D loss: (23.420)(R 207.725, F -160.885)]  [G loss: 165.677] \n",
      "510 [D loss: (19.037)(R 198.859, F -160.785)]  [G loss: 160.859] \n",
      "511 [D loss: (23.533)(R 195.605, F -148.538)]  [G loss: 150.312] \n",
      "511 [D loss: (17.899)(R 184.887, F -149.089)]  [G loss: 145.359] \n",
      "512 [D loss: (16.250)(R 179.688, F -147.189)]  [G loss: 140.301] \n",
      "512 [D loss: (25.197)(R 175.688, F -125.295)]  [G loss: 139.082] \n",
      "513 [D loss: (16.065)(R 163.229, F -131.098)]  [G loss: 133.108] \n",
      "513 [D loss: (14.982)(R 159.147, F -129.183)]  [G loss: 124.325] \n",
      "514 [D loss: (21.853)(R 158.838, F -115.132)]  [G loss: 124.089] \n",
      "514 [D loss: (15.266)(R 148.814, F -118.281)]  [G loss: 120.185] \n",
      "515 [D loss: (13.074)(R 139.476, F -113.328)]  [G loss: 117.216] \n",
      "515 [D loss: (12.046)(R 140.071, F -115.979)]  [G loss: 113.012] \n",
      "516 [D loss: (13.680)(R 131.311, F -103.950)]  [G loss: 109.919] \n",
      "516 [D loss: (13.656)(R 130.465, F -103.154)]  [G loss: 107.872] \n",
      "517 [D loss: (14.149)(R 126.242, F -97.944)]  [G loss: 104.175] \n",
      "517 [D loss: (14.488)(R 126.731, F -97.754)]  [G loss: 101.300] \n",
      "518 [D loss: (15.977)(R 125.606, F -93.651)]  [G loss: 98.937] \n",
      "518 [D loss: (12.550)(R 120.338, F -95.238)]  [G loss: 91.673] \n",
      "519 [D loss: (16.990)(R 118.888, F -84.909)]  [G loss: 90.581] \n",
      "519 [D loss: (13.519)(R 111.506, F -84.468)]  [G loss: 85.393] \n",
      "520 [D loss: (14.467)(R 111.871, F -82.937)]  [G loss: 85.420] \n",
      "520 [D loss: (14.373)(R 106.388, F -77.643)]  [G loss: 81.920] \n",
      "521 [D loss: (11.395)(R 101.327, F -78.537)]  [G loss: 80.061] \n",
      "521 [D loss: (8.891)(R 94.114, F -76.332)]  [G loss: 77.384] \n",
      "522 [D loss: (6.546)(R 91.951, F -78.858)]  [G loss: 74.344] \n",
      "522 [D loss: (7.998)(R 91.317, F -75.321)]  [G loss: 71.724] \n",
      "523 [D loss: (7.139)(R 88.304, F -74.025)]  [G loss: 69.028] \n",
      "523 [D loss: (9.448)(R 86.835, F -67.938)]  [G loss: 68.195] \n",
      "524 [D loss: (8.609)(R 84.856, F -67.638)]  [G loss: 65.291] \n",
      "524 [D loss: (6.342)(R 81.644, F -68.959)]  [G loss: 61.907] \n",
      "525 [D loss: (8.177)(R 77.654, F -61.300)]  [G loss: 61.887] \n",
      "525 [D loss: (7.792)(R 77.161, F -61.577)]  [G loss: 61.461] \n",
      "526 [D loss: (6.676)(R 74.314, F -60.963)]  [G loss: 58.521] \n",
      "526 [D loss: (8.414)(R 72.306, F -55.479)]  [G loss: 56.941] \n",
      "527 [D loss: (6.899)(R 71.334, F -57.537)]  [G loss: 55.003] \n",
      "527 [D loss: (5.293)(R 68.009, F -57.423)]  [G loss: 53.259] \n",
      "528 [D loss: (6.206)(R 66.612, F -54.201)]  [G loss: 52.363] \n",
      "528 [D loss: (7.822)(R 66.224, F -50.581)]  [G loss: 51.333] \n",
      "529 [D loss: (7.074)(R 62.939, F -48.791)]  [G loss: 50.415] \n",
      "529 [D loss: (7.641)(R 61.045, F -45.764)]  [G loss: 48.462] \n",
      "530 [D loss: (4.890)(R 58.910, F -49.130)]  [G loss: 47.156] \n",
      "530 [D loss: (5.284)(R 55.412, F -44.843)]  [G loss: 45.893] \n",
      "531 [D loss: (5.829)(R 55.318, F -43.659)]  [G loss: 44.689] \n",
      "531 [D loss: (5.749)(R 54.869, F -43.372)]  [G loss: 42.882] \n",
      "532 [D loss: (5.685)(R 53.508, F -42.139)]  [G loss: 40.398] \n",
      "532 [D loss: (6.547)(R 52.208, F -39.114)]  [G loss: 39.925] \n",
      "533 [D loss: (4.398)(R 49.248, F -40.451)]  [G loss: 37.809] \n",
      "533 [D loss: (7.012)(R 48.317, F -34.294)]  [G loss: 37.123] \n",
      "534 [D loss: (5.224)(R 46.935, F -36.487)]  [G loss: 36.873] \n",
      "534 [D loss: (6.344)(R 45.190, F -32.501)]  [G loss: 36.533] \n",
      "535 [D loss: (4.832)(R 42.875, F -33.211)]  [G loss: 33.130] \n",
      "535 [D loss: (4.324)(R 42.449, F -33.802)]  [G loss: 32.173] \n",
      "536 [D loss: (5.099)(R 39.887, F -29.690)]  [G loss: 31.926] \n",
      "536 [D loss: (3.865)(R 38.878, F -31.147)]  [G loss: 29.588] \n",
      "537 [D loss: (4.141)(R 36.968, F -28.687)]  [G loss: 29.300] \n",
      "537 [D loss: (4.283)(R 36.505, F -27.939)]  [G loss: 28.499] \n",
      "538 [D loss: (3.596)(R 35.025, F -27.833)]  [G loss: 27.294] \n",
      "538 [D loss: (4.080)(R 34.292, F -26.132)]  [G loss: 25.639] \n",
      "539 [D loss: (2.379)(R 32.271, F -27.513)]  [G loss: 24.733] \n",
      "539 [D loss: (3.208)(R 32.106, F -25.690)]  [G loss: 24.640] \n",
      "540 [D loss: (2.649)(R 30.239, F -24.940)]  [G loss: 23.231] \n",
      "540 [D loss: (3.678)(R 30.421, F -23.065)]  [G loss: 22.999] \n",
      "541 [D loss: (3.516)(R 28.685, F -21.653)]  [G loss: 21.415] \n",
      "541 [D loss: (3.789)(R 26.577, F -18.998)]  [G loss: 20.415] \n",
      "542 [D loss: (4.021)(R 26.622, F -18.580)]  [G loss: 18.371] \n",
      "542 [D loss: (2.414)(R 24.335, F -19.508)]  [G loss: 18.768] \n",
      "543 [D loss: (2.313)(R 22.446, F -17.821)]  [G loss: 17.223] \n",
      "543 [D loss: (1.936)(R 22.560, F -18.688)]  [G loss: 17.017] \n",
      "544 [D loss: (3.596)(R 22.652, F -15.461)]  [G loss: 16.838] \n",
      "544 [D loss: (2.675)(R 20.259, F -14.910)]  [G loss: 15.560] \n",
      "545 [D loss: (1.984)(R 20.309, F -16.341)]  [G loss: 14.729] \n",
      "545 [D loss: (1.396)(R 18.734, F -15.942)]  [G loss: 13.783] \n",
      "546 [D loss: (1.290)(R 17.464, F -14.883)]  [G loss: 13.259] \n",
      "546 [D loss: (1.922)(R 17.016, F -13.171)]  [G loss: 11.783] \n",
      "547 [D loss: (1.816)(R 15.169, F -11.538)]  [G loss: 10.667] \n",
      "547 [D loss: (1.033)(R 13.892, F -11.826)]  [G loss: 11.125] \n",
      "548 [D loss: (1.752)(R 13.783, F -10.279)]  [G loss: 10.001] \n",
      "548 [D loss: (1.719)(R 13.651, F -10.213)]  [G loss: 8.845] \n",
      "549 [D loss: (1.994)(R 12.457, F -8.469)]  [G loss: 8.790] \n",
      "549 [D loss: (1.846)(R 11.820, F -8.127)]  [G loss: 8.106] \n",
      "550 [D loss: (1.166)(R 10.075, F -7.743)]  [G loss: 6.415] \n",
      "550 [D loss: (1.661)(R 10.132, F -6.810)]  [G loss: 7.083] \n",
      "551 [D loss: (1.394)(R 8.271, F -5.482)]  [G loss: 6.161] \n",
      "551 [D loss: (1.885)(R 7.716, F -3.945)]  [G loss: 5.083] \n",
      "552 [D loss: (1.573)(R 7.890, F -4.744)]  [G loss: 4.411] \n",
      "552 [D loss: (2.169)(R 6.688, F -2.349)]  [G loss: 3.407] \n",
      "553 [D loss: (1.925)(R 5.500, F -1.649)]  [G loss: 2.810] \n",
      "553 [D loss: (1.525)(R 4.215, F -1.164)]  [G loss: 1.475] \n",
      "554 [D loss: (1.815)(R 3.513, F 0.118)]  [G loss: 0.584] \n",
      "554 [D loss: (0.742)(R 2.229, F -0.745)]  [G loss: -0.473] \n",
      "555 [D loss: (1.712)(R 1.978, F 1.446)]  [G loss: 0.330] \n",
      "555 [D loss: (0.429)(R 0.141, F 0.718)]  [G loss: -0.640] \n",
      "556 [D loss: (-0.082)(R -1.027, F 0.862)]  [G loss: -1.658] \n",
      "556 [D loss: (0.463)(R -2.158, F 3.085)]  [G loss: -3.204] \n",
      "557 [D loss: (-0.011)(R -2.749, F 2.727)]  [G loss: -3.265] \n",
      "557 [D loss: (0.738)(R -2.872, F 4.348)]  [G loss: -4.630] \n",
      "558 [D loss: (-0.296)(R -5.282, F 4.690)]  [G loss: -5.509] \n",
      "558 [D loss: (-0.191)(R -5.838, F 5.455)]  [G loss: -4.983] \n",
      "559 [D loss: (-0.481)(R -6.638, F 5.675)]  [G loss: -7.417] \n",
      "559 [D loss: (-0.270)(R -7.484, F 6.943)]  [G loss: -7.502] \n",
      "560 [D loss: (0.354)(R -7.859, F 8.568)]  [G loss: -8.352] \n",
      "560 [D loss: (-0.469)(R -9.702, F 8.763)]  [G loss: -8.970] \n",
      "561 [D loss: (-0.359)(R -10.187, F 9.469)]  [G loss: -9.328] \n",
      "561 [D loss: (-0.642)(R -10.853, F 9.568)]  [G loss: -10.547] \n",
      "562 [D loss: (-0.199)(R -11.655, F 11.257)]  [G loss: -11.114] \n",
      "562 [D loss: (-0.377)(R -12.762, F 12.008)]  [G loss: -12.804] \n",
      "563 [D loss: (-0.688)(R -14.123, F 12.747)]  [G loss: -12.923] \n",
      "563 [D loss: (-0.399)(R -15.061, F 14.264)]  [G loss: -13.926] \n",
      "564 [D loss: (-0.833)(R -15.979, F 14.313)]  [G loss: -14.040] \n",
      "564 [D loss: (-1.722)(R -17.730, F 14.285)]  [G loss: -15.377] \n",
      "565 [D loss: (-0.902)(R -18.474, F 16.669)]  [G loss: -16.481] \n",
      "565 [D loss: (-1.584)(R -19.308, F 16.139)]  [G loss: -16.901] \n",
      "566 [D loss: (-1.340)(R -20.410, F 17.730)]  [G loss: -17.260] \n",
      "566 [D loss: (-1.487)(R -21.009, F 18.035)]  [G loss: -19.008] \n",
      "567 [D loss: (-1.219)(R -23.228, F 20.789)]  [G loss: -20.792] \n",
      "567 [D loss: (-1.808)(R -23.730, F 20.114)]  [G loss: -20.430] \n",
      "568 [D loss: (-1.294)(R -25.182, F 22.593)]  [G loss: -23.239] \n",
      "568 [D loss: (-1.470)(R -25.999, F 23.058)]  [G loss: -23.689] \n",
      "569 [D loss: (-2.383)(R -28.458, F 23.692)]  [G loss: -23.656] \n",
      "569 [D loss: (-2.832)(R -29.851, F 24.187)]  [G loss: -25.282] \n",
      "570 [D loss: (-1.888)(R -30.966, F 27.190)]  [G loss: -26.782] \n",
      "570 [D loss: (-2.228)(R -32.256, F 27.799)]  [G loss: -28.134] \n",
      "571 [D loss: (-2.298)(R -33.972, F 29.377)]  [G loss: -29.372] \n",
      "571 [D loss: (-2.002)(R -34.953, F 30.948)]  [G loss: -30.888] \n",
      "572 [D loss: (-0.926)(R -35.799, F 33.948)]  [G loss: -31.904] \n",
      "572 [D loss: (-3.343)(R -38.288, F 31.602)]  [G loss: -31.458] \n",
      "573 [D loss: (-2.537)(R -39.782, F 34.709)]  [G loss: -34.986] \n",
      "573 [D loss: (-2.100)(R -41.094, F 36.894)]  [G loss: -35.311] \n",
      "574 [D loss: (-2.690)(R -43.209, F 37.830)]  [G loss: -36.683] \n",
      "574 [D loss: (-4.063)(R -45.923, F 37.797)]  [G loss: -38.122] \n",
      "575 [D loss: (-3.046)(R -46.101, F 40.009)]  [G loss: -40.336] \n",
      "575 [D loss: (-2.481)(R -47.243, F 42.282)]  [G loss: -40.803] \n",
      "576 [D loss: (-4.026)(R -52.052, F 44.000)]  [G loss: -42.633] \n",
      "576 [D loss: (-4.545)(R -52.545, F 43.455)]  [G loss: -45.252] \n",
      "577 [D loss: (-3.290)(R -54.747, F 48.166)]  [G loss: -45.939] \n",
      "577 [D loss: (-4.905)(R -57.538, F 47.727)]  [G loss: -48.762] \n",
      "578 [D loss: (-4.385)(R -59.197, F 50.426)]  [G loss: -51.154] \n",
      "578 [D loss: (-3.991)(R -61.911, F 53.930)]  [G loss: -53.952] \n",
      "579 [D loss: (-5.012)(R -67.232, F 57.208)]  [G loss: -55.784] \n",
      "579 [D loss: (-3.636)(R -66.396, F 59.123)]  [G loss: -56.732] \n",
      "580 [D loss: (-6.697)(R -72.339, F 58.946)]  [G loss: -59.642] \n",
      "580 [D loss: (-4.500)(R -72.996, F 63.997)]  [G loss: -62.080] \n",
      "581 [D loss: (-6.656)(R -77.667, F 64.354)]  [G loss: -65.261] \n",
      "581 [D loss: (-5.201)(R -80.342, F 69.940)]  [G loss: -67.178] \n",
      "582 [D loss: (-7.610)(R -86.013, F 70.793)]  [G loss: -72.602] \n",
      "582 [D loss: (-6.525)(R -89.218, F 76.169)]  [G loss: -74.120] \n",
      "583 [D loss: (-5.579)(R -92.531, F 81.373)]  [G loss: -77.507] \n",
      "583 [D loss: (-5.183)(R -94.687, F 84.321)]  [G loss: -79.981] \n",
      "584 [D loss: (-6.691)(R -99.800, F 86.417)]  [G loss: -84.824] \n",
      "584 [D loss: (-7.074)(R -104.526, F 90.378)]  [G loss: -87.700] \n",
      "585 [D loss: (-10.693)(R -110.775, F 89.389)]  [G loss: -93.307] \n",
      "585 [D loss: (-7.973)(R -116.823, F 100.876)]  [G loss: -98.425] \n",
      "586 [D loss: (-11.655)(R -121.396, F 98.086)]  [G loss: -103.119] \n",
      "586 [D loss: (-13.424)(R -132.118, F 105.270)]  [G loss: -109.709] \n",
      "587 [D loss: (-10.742)(R -134.768, F 113.285)]  [G loss: -115.378] \n",
      "587 [D loss: (-11.065)(R -146.921, F 124.790)]  [G loss: -122.795] \n",
      "588 [D loss: (-8.476)(R -150.481, F 133.530)]  [G loss: -129.911] \n",
      "588 [D loss: (-11.493)(R -159.897, F 136.910)]  [G loss: -136.346] \n",
      "589 [D loss: (-17.585)(R -170.608, F 135.438)]  [G loss: -142.769] \n",
      "589 [D loss: (-15.325)(R -181.912, F 151.261)]  [G loss: -148.171] \n",
      "590 [D loss: (-13.515)(R -185.828, F 158.799)]  [G loss: -157.859] \n",
      "590 [D loss: (-12.310)(R -200.655, F 176.035)]  [G loss: -165.783] \n",
      "591 [D loss: (-15.124)(R -209.939, F 179.691)]  [G loss: -175.101] \n",
      "591 [D loss: (-17.617)(R -219.199, F 183.965)]  [G loss: -184.578] \n",
      "592 [D loss: (-17.309)(R -230.922, F 196.304)]  [G loss: -200.526] \n",
      "592 [D loss: (-23.818)(R -258.269, F 210.634)]  [G loss: -217.865] \n",
      "593 [D loss: (-28.605)(R -278.799, F 221.588)]  [G loss: -226.737] \n",
      "593 [D loss: (-23.974)(R -294.837, F 246.888)]  [G loss: -253.963] \n",
      "594 [D loss: (-30.194)(R -317.491, F 257.102)]  [G loss: -268.743] \n",
      "594 [D loss: (-27.913)(R -341.306, F 285.481)]  [G loss: -289.693] \n",
      "595 [D loss: (-28.415)(R -367.708, F 310.878)]  [G loss: -314.501] \n",
      "595 [D loss: (-29.977)(R -389.937, F 329.984)]  [G loss: -337.739] \n",
      "596 [D loss: (-36.023)(R -433.898, F 361.853)]  [G loss: -362.808] \n",
      "596 [D loss: (-33.028)(R -463.390, F 397.333)]  [G loss: -401.925] \n",
      "597 [D loss: (-45.466)(R -523.427, F 432.495)]  [G loss: -435.116] \n",
      "597 [D loss: (-34.052)(R -547.508, F 479.405)]  [G loss: -476.398] \n",
      "598 [D loss: (-49.682)(R -628.857, F 529.494)]  [G loss: -519.867] \n",
      "598 [D loss: (-53.430)(R -679.635, F 572.775)]  [G loss: -572.151] \n",
      "599 [D loss: (-60.493)(R -742.729, F 621.744)]  [G loss: -628.483] \n",
      "599 [D loss: (-57.833)(R -785.885, F 670.218)]  [G loss: -677.111] \n",
      "600 [D loss: (-96.402)(R -888.490, F 695.687)]  [G loss: -725.762] \n",
      "600 [D loss: (-75.189)(R -924.165, F 773.787)]  [G loss: -786.866] \n",
      "601 [D loss: (-83.216)(R -995.717, F 829.286)]  [G loss: -842.649] \n",
      "601 [D loss: (-86.051)(R -1074.485, F 902.384)]  [G loss: -930.333] \n",
      "602 [D loss: (-94.918)(R -1156.658, F 966.821)]  [G loss: -979.679] \n",
      "602 [D loss: (-79.629)(R -1234.224, F 1074.966)]  [G loss: -1038.495] \n",
      "603 [D loss: (-102.506)(R -1335.041, F 1130.028)]  [G loss: -1113.110] \n",
      "603 [D loss: (-115.163)(R -1360.388, F 1130.061)]  [G loss: -1151.788] \n",
      "604 [D loss: (-120.377)(R -1430.724, F 1189.969)]  [G loss: -1197.761] \n",
      "604 [D loss: (-111.687)(R -1449.730, F 1226.356)]  [G loss: -1230.869] \n",
      "605 [D loss: (-74.791)(R -1488.653, F 1339.071)]  [G loss: -1285.209] \n",
      "605 [D loss: (-95.898)(R -1465.697, F 1273.901)]  [G loss: -1326.470] \n",
      "606 [D loss: (-95.321)(R -1531.703, F 1341.061)]  [G loss: -1310.752] \n",
      "606 [D loss: (-83.124)(R -1558.569, F 1392.322)]  [G loss: -1315.698] \n",
      "607 [D loss: (-74.333)(R -1555.616, F 1406.949)]  [G loss: -1369.925] \n",
      "607 [D loss: (-95.261)(R -1592.746, F 1402.225)]  [G loss: -1412.715] \n",
      "608 [D loss: (-70.602)(R -1557.049, F 1415.845)]  [G loss: -1419.560] \n",
      "608 [D loss: (-124.168)(R -1588.381, F 1340.045)]  [G loss: -1417.655] \n",
      "609 [D loss: (-77.103)(R -1584.823, F 1430.618)]  [G loss: -1446.621] \n",
      "609 [D loss: (-56.764)(R -1593.026, F 1479.498)]  [G loss: -1465.657] \n",
      "610 [D loss: (-63.323)(R -1633.339, F 1506.693)]  [G loss: -1479.827] \n",
      "610 [D loss: (-47.354)(R -1622.840, F 1528.131)]  [G loss: -1495.011] \n",
      "611 [D loss: (-17.298)(R -1602.551, F 1567.956)]  [G loss: -1499.196] \n",
      "611 [D loss: (-116.846)(R -1661.182, F 1427.490)]  [G loss: -1510.945] \n",
      "612 [D loss: (-42.527)(R -1623.643, F 1538.588)]  [G loss: -1554.905] \n",
      "612 [D loss: (-27.006)(R -1633.624, F 1579.612)]  [G loss: -1531.975] \n",
      "613 [D loss: (-26.149)(R -1593.207, F 1540.910)]  [G loss: -1570.950] \n",
      "613 [D loss: (-75.856)(R -1664.011, F 1512.299)]  [G loss: -1534.217] \n",
      "614 [D loss: (-37.150)(R -1610.256, F 1535.956)]  [G loss: -1548.081] \n",
      "614 [D loss: (13.988)(R -1559.542, F 1587.519)]  [G loss: -1578.538] \n",
      "615 [D loss: (-29.455)(R -1634.663, F 1575.753)]  [G loss: -1528.997] \n",
      "615 [D loss: (7.216)(R -1586.957, F 1601.390)]  [G loss: -1518.646] \n",
      "616 [D loss: (23.657)(R -1509.240, F 1556.553)]  [G loss: -1536.629] \n",
      "616 [D loss: (-32.566)(R -1558.739, F 1493.608)]  [G loss: -1512.651] \n",
      "617 [D loss: (15.165)(R -1525.538, F 1555.868)]  [G loss: -1539.007] \n",
      "617 [D loss: (26.810)(R -1478.070, F 1531.691)]  [G loss: -1493.100] \n",
      "618 [D loss: (22.780)(R -1482.880, F 1528.441)]  [G loss: -1476.271] \n",
      "618 [D loss: (-19.354)(R -1442.644, F 1403.936)]  [G loss: -1469.401] \n",
      "619 [D loss: (18.227)(R -1430.959, F 1467.412)]  [G loss: -1430.139] \n",
      "619 [D loss: (20.962)(R -1373.287, F 1415.210)]  [G loss: -1435.140] \n",
      "620 [D loss: (31.716)(R -1350.241, F 1413.673)]  [G loss: -1364.761] \n",
      "620 [D loss: (23.968)(R -1315.708, F 1363.644)]  [G loss: -1355.772] \n",
      "621 [D loss: (41.591)(R -1247.721, F 1330.904)]  [G loss: -1332.664] \n",
      "621 [D loss: (19.272)(R -1242.237, F 1280.780)]  [G loss: -1269.252] \n",
      "622 [D loss: (7.760)(R -1207.143, F 1222.664)]  [G loss: -1267.732] \n",
      "622 [D loss: (19.871)(R -1140.932, F 1180.674)]  [G loss: -1214.597] \n",
      "623 [D loss: (19.819)(R -1098.549, F 1138.188)]  [G loss: -1172.645] \n",
      "623 [D loss: (45.147)(R -1059.742, F 1150.036)]  [G loss: -1120.490] \n",
      "624 [D loss: (44.205)(R -1004.387, F 1092.796)]  [G loss: -1084.903] \n",
      "624 [D loss: (49.473)(R -969.052, F 1067.999)]  [G loss: -1047.693] \n",
      "625 [D loss: (32.615)(R -926.482, F 991.713)]  [G loss: -998.384] \n",
      "625 [D loss: (41.133)(R -885.932, F 968.198)]  [G loss: -943.544] \n",
      "626 [D loss: (58.859)(R -821.238, F 938.955)]  [G loss: -903.632] \n",
      "626 [D loss: (57.479)(R -792.438, F 907.396)]  [G loss: -849.090] \n",
      "627 [D loss: (67.252)(R -729.186, F 863.690)]  [G loss: -817.694] \n",
      "627 [D loss: (38.115)(R -691.827, F 768.057)]  [G loss: -779.461] \n",
      "628 [D loss: (74.056)(R -643.335, F 791.446)]  [G loss: -740.969] \n",
      "628 [D loss: (51.240)(R -603.016, F 705.497)]  [G loss: -699.880] \n",
      "629 [D loss: (57.727)(R -577.931, F 693.385)]  [G loss: -649.663] \n",
      "629 [D loss: (37.031)(R -546.841, F 620.902)]  [G loss: -615.489] \n",
      "630 [D loss: (55.861)(R -497.454, F 609.175)]  [G loss: -582.016] \n",
      "630 [D loss: (30.317)(R -472.707, F 533.340)]  [G loss: -530.645] \n",
      "631 [D loss: (40.986)(R -440.977, F 522.948)]  [G loss: -498.304] \n",
      "631 [D loss: (40.685)(R -400.467, F 481.837)]  [G loss: -470.626] \n",
      "632 [D loss: (30.475)(R -387.203, F 448.153)]  [G loss: -451.677] \n",
      "632 [D loss: (44.765)(R -343.955, F 433.484)]  [G loss: -403.125] \n",
      "633 [D loss: (20.803)(R -324.571, F 366.176)]  [G loss: -380.534] \n",
      "633 [D loss: (39.567)(R -294.971, F 374.106)]  [G loss: -341.305] \n",
      "634 [D loss: (20.395)(R -276.372, F 317.162)]  [G loss: -320.487] \n",
      "634 [D loss: (23.047)(R -251.966, F 298.061)]  [G loss: -292.979] \n",
      "635 [D loss: (13.281)(R -234.322, F 260.883)]  [G loss: -268.478] \n",
      "635 [D loss: (18.030)(R -212.169, F 248.228)]  [G loss: -244.225] \n",
      "636 [D loss: (10.412)(R -192.184, F 213.009)]  [G loss: -218.934] \n",
      "636 [D loss: (13.806)(R -176.209, F 203.821)]  [G loss: -193.686] \n",
      "637 [D loss: (2.110)(R -164.945, F 169.165)]  [G loss: -175.192] \n",
      "637 [D loss: (5.458)(R -146.891, F 157.808)]  [G loss: -152.933] \n",
      "638 [D loss: (3.056)(R -131.537, F 137.648)]  [G loss: -132.638] \n",
      "638 [D loss: (-2.653)(R -117.804, F 112.498)]  [G loss: -118.411] \n",
      "639 [D loss: (1.305)(R -104.546, F 107.155)]  [G loss: -93.673] \n",
      "639 [D loss: (-2.109)(R -90.212, F 85.994)]  [G loss: -75.970] \n",
      "640 [D loss: (-8.031)(R -76.059, F 59.997)]  [G loss: -54.591] \n",
      "640 [D loss: (-8.287)(R -60.684, F 44.111)]  [G loss: -36.215] \n",
      "641 [D loss: (-17.594)(R -48.544, F 13.356)]  [G loss: -20.739] \n",
      "641 [D loss: (-24.781)(R -38.185, F -11.376)]  [G loss: -3.042] \n",
      "642 [D loss: (-20.218)(R -24.914, F -15.522)]  [G loss: 14.476] \n",
      "642 [D loss: (-25.265)(R -8.183, F -42.347)]  [G loss: 38.883] \n",
      "643 [D loss: (-25.778)(R 5.585, F -57.141)]  [G loss: 57.023] \n",
      "643 [D loss: (-25.787)(R 20.153, F -71.727)]  [G loss: 78.820] \n",
      "644 [D loss: (-31.535)(R 32.771, F -95.841)]  [G loss: 99.079] \n",
      "644 [D loss: (-36.851)(R 47.675, F -121.377)]  [G loss: 123.712] \n",
      "645 [D loss: (-41.849)(R 61.869, F -145.568)]  [G loss: 147.792] \n",
      "645 [D loss: (-43.357)(R 79.074, F -165.789)]  [G loss: 175.083] \n",
      "646 [D loss: (-46.796)(R 98.920, F -192.511)]  [G loss: 191.793] \n",
      "646 [D loss: (-48.614)(R 120.302, F -217.529)]  [G loss: 219.980] \n",
      "647 [D loss: (-55.921)(R 130.778, F -242.621)]  [G loss: 246.217] \n",
      "647 [D loss: (-58.715)(R 154.732, F -272.162)]  [G loss: 272.728] \n",
      "648 [D loss: (-58.884)(R 170.015, F -287.782)]  [G loss: 309.209] \n",
      "648 [D loss: (-71.543)(R 188.972, F -332.058)]  [G loss: 334.087] \n",
      "649 [D loss: (-72.237)(R 209.920, F -354.394)]  [G loss: 361.335] \n",
      "649 [D loss: (-72.943)(R 228.460, F -374.345)]  [G loss: 393.359] \n",
      "650 [D loss: (-85.622)(R 250.753, F -421.996)]  [G loss: 426.206] \n",
      "650 [D loss: (-86.200)(R 270.712, F -443.113)]  [G loss: 455.118] \n",
      "651 [D loss: (-93.318)(R 292.587, F -479.223)]  [G loss: 481.727] \n",
      "651 [D loss: (-103.780)(R 311.354, F -518.914)]  [G loss: 525.150] \n",
      "652 [D loss: (-96.721)(R 347.181, F -540.622)]  [G loss: 555.140] \n",
      "652 [D loss: (-101.293)(R 364.081, F -566.666)]  [G loss: 582.713] \n",
      "653 [D loss: (-108.689)(R 388.298, F -605.675)]  [G loss: 620.353] \n",
      "653 [D loss: (-118.400)(R 424.978, F -661.777)]  [G loss: 662.249] \n",
      "654 [D loss: (-120.985)(R 434.988, F -676.959)]  [G loss: 695.607] \n",
      "654 [D loss: (-120.073)(R 476.216, F -716.363)]  [G loss: 713.549] \n",
      "655 [D loss: (-138.229)(R 492.348, F -768.807)]  [G loss: 752.390] \n",
      "655 [D loss: (-128.005)(R 520.667, F -776.676)]  [G loss: 781.401] \n",
      "656 [D loss: (-129.482)(R 562.941, F -821.905)]  [G loss: 810.072] \n",
      "656 [D loss: (-140.257)(R 554.002, F -834.516)]  [G loss: 833.495] \n",
      "657 [D loss: (-130.481)(R 584.809, F -845.771)]  [G loss: 884.264] \n",
      "657 [D loss: (-128.730)(R 634.822, F -892.281)]  [G loss: 899.921] \n",
      "658 [D loss: (-148.629)(R 675.141, F -972.399)]  [G loss: 940.265] \n",
      "658 [D loss: (-144.318)(R 663.526, F -952.162)]  [G loss: 958.920] \n",
      "659 [D loss: (-140.319)(R 699.971, F -980.610)]  [G loss: 995.088] \n",
      "659 [D loss: (-172.596)(R 695.535, F -1040.726)]  [G loss: 1013.931] \n",
      "660 [D loss: (-153.104)(R 730.584, F -1036.792)]  [G loss: 1031.828] \n",
      "660 [D loss: (-113.012)(R 791.007, F -1017.030)]  [G loss: 1045.426] \n",
      "661 [D loss: (-148.649)(R 787.914, F -1085.211)]  [G loss: 1074.084] \n",
      "661 [D loss: (-116.419)(R 820.893, F -1053.731)]  [G loss: 1087.594] \n",
      "662 [D loss: (-152.255)(R 789.478, F -1093.989)]  [G loss: 1120.455] \n",
      "662 [D loss: (-136.438)(R 830.282, F -1103.158)]  [G loss: 1128.456] \n",
      "663 [D loss: (-133.418)(R 850.474, F -1117.311)]  [G loss: 1124.014] \n",
      "663 [D loss: (-123.743)(R 841.008, F -1088.495)]  [G loss: 1139.340] \n",
      "664 [D loss: (-140.231)(R 872.905, F -1153.367)]  [G loss: 1163.648] \n",
      "664 [D loss: (-134.290)(R 874.174, F -1142.753)]  [G loss: 1144.208] \n",
      "665 [D loss: (-136.473)(R 922.564, F -1195.510)]  [G loss: 1167.805] \n",
      "665 [D loss: (-128.145)(R 946.923, F -1203.213)]  [G loss: 1186.522] \n",
      "666 [D loss: (-164.702)(R 916.495, F -1245.898)]  [G loss: 1170.849] \n",
      "666 [D loss: (-142.113)(R 931.579, F -1215.804)]  [G loss: 1184.332] \n",
      "667 [D loss: (-136.468)(R 954.556, F -1227.492)]  [G loss: 1203.905] \n",
      "667 [D loss: (-141.515)(R 959.861, F -1242.892)]  [G loss: 1201.545] \n",
      "668 [D loss: (-94.290)(R 978.919, F -1167.500)]  [G loss: 1221.049] \n",
      "668 [D loss: (-154.826)(R 980.138, F -1289.789)]  [G loss: 1217.822] \n",
      "669 [D loss: (-103.443)(R 1006.825, F -1213.712)]  [G loss: 1239.531] \n",
      "669 [D loss: (-154.891)(R 992.654, F -1302.435)]  [G loss: 1219.841] \n",
      "670 [D loss: (-105.691)(R 990.209, F -1201.591)]  [G loss: 1217.844] \n",
      "670 [D loss: (-90.649)(R 1011.293, F -1192.592)]  [G loss: 1167.632] \n",
      "671 [D loss: (-172.971)(R 995.937, F -1341.880)]  [G loss: 1227.925] \n",
      "671 [D loss: (-117.706)(R 1011.008, F -1246.420)]  [G loss: 1215.236] \n",
      "672 [D loss: (-114.543)(R 1016.743, F -1245.829)]  [G loss: 1190.619] \n",
      "672 [D loss: (-123.177)(R 1049.456, F -1295.811)]  [G loss: 1195.273] \n",
      "673 [D loss: (-82.902)(R 1018.123, F -1183.928)]  [G loss: 1182.678] \n",
      "673 [D loss: (-95.086)(R 1013.691, F -1203.863)]  [G loss: 1215.292] \n",
      "674 [D loss: (-32.355)(R 1004.250, F -1068.960)]  [G loss: 1187.013] \n",
      "674 [D loss: (-52.725)(R 1035.901, F -1141.352)]  [G loss: 1186.361] \n",
      "675 [D loss: (-62.106)(R 1057.851, F -1182.063)]  [G loss: 1183.320] \n",
      "675 [D loss: (-58.123)(R 1034.987, F -1151.233)]  [G loss: 1130.777] \n",
      "676 [D loss: (-68.854)(R 1049.628, F -1187.335)]  [G loss: 1152.880] \n",
      "676 [D loss: (-44.893)(R 1053.573, F -1143.359)]  [G loss: 1116.268] \n",
      "677 [D loss: (-92.375)(R 1009.744, F -1194.494)]  [G loss: 1137.652] \n",
      "677 [D loss: (-87.202)(R 1020.685, F -1195.090)]  [G loss: 1095.781] \n",
      "678 [D loss: (-62.994)(R 1035.746, F -1161.735)]  [G loss: 1097.885] \n",
      "678 [D loss: (-32.683)(R 1009.238, F -1074.604)]  [G loss: 1087.190] \n",
      "679 [D loss: (-55.036)(R 1031.877, F -1141.950)]  [G loss: 1071.023] \n",
      "679 [D loss: (-16.563)(R 1032.741, F -1065.867)]  [G loss: 1055.524] \n",
      "680 [D loss: (-42.712)(R 1022.177, F -1107.601)]  [G loss: 1054.215] \n",
      "680 [D loss: (-32.070)(R 991.561, F -1055.701)]  [G loss: 1031.437] \n",
      "681 [D loss: (1.858)(R 996.729, F -993.013)]  [G loss: 1022.592] \n",
      "681 [D loss: (-12.593)(R 995.619, F -1020.804)]  [G loss: 993.237] \n",
      "682 [D loss: (-14.950)(R 974.315, F -1004.215)]  [G loss: 938.736] \n",
      "682 [D loss: (-4.168)(R 974.256, F -982.592)]  [G loss: 901.594] \n",
      "683 [D loss: (-2.457)(R 936.805, F -941.719)]  [G loss: 899.596] \n",
      "683 [D loss: (-9.868)(R 901.593, F -921.329)]  [G loss: 848.779] \n",
      "684 [D loss: (-9.064)(R 880.152, F -898.279)]  [G loss: 842.966] \n",
      "684 [D loss: (25.349)(R 887.819, F -837.121)]  [G loss: 811.311] \n",
      "685 [D loss: (17.028)(R 855.759, F -821.702)]  [G loss: 779.948] \n",
      "685 [D loss: (15.495)(R 829.348, F -798.358)]  [G loss: 782.284] \n",
      "686 [D loss: (-8.669)(R 855.185, F -872.524)]  [G loss: 732.871] \n",
      "686 [D loss: (50.018)(R 777.254, F -677.217)]  [G loss: 689.987] \n",
      "687 [D loss: (-0.947)(R 766.358, F -768.252)]  [G loss: 660.385] \n",
      "687 [D loss: (24.571)(R 700.140, F -650.997)]  [G loss: 622.922] \n",
      "688 [D loss: (3.243)(R 695.580, F -689.095)]  [G loss: 614.484] \n",
      "688 [D loss: (52.610)(R 655.645, F -550.425)]  [G loss: 596.457] \n",
      "689 [D loss: (20.186)(R 616.323, F -575.951)]  [G loss: 546.735] \n",
      "689 [D loss: (42.678)(R 612.013, F -526.657)]  [G loss: 536.816] \n",
      "690 [D loss: (46.780)(R 569.785, F -476.225)]  [G loss: 472.443] \n",
      "690 [D loss: (24.173)(R 534.595, F -486.250)]  [G loss: 471.135] \n",
      "691 [D loss: (36.701)(R 509.671, F -436.269)]  [G loss: 431.732] \n",
      "691 [D loss: (5.181)(R 469.054, F -458.691)]  [G loss: 430.950] \n",
      "692 [D loss: (35.589)(R 448.123, F -376.944)]  [G loss: 409.986] \n",
      "692 [D loss: (-8.204)(R 426.214, F -442.622)]  [G loss: 390.979] \n",
      "693 [D loss: (11.687)(R 395.113, F -371.738)]  [G loss: 375.687] \n",
      "693 [D loss: (17.026)(R 397.165, F -363.114)]  [G loss: 345.023] \n",
      "694 [D loss: (6.364)(R 387.129, F -374.401)]  [G loss: 336.070] \n",
      "694 [D loss: (14.719)(R 331.454, F -302.016)]  [G loss: 331.777] \n",
      "695 [D loss: (13.187)(R 325.329, F -298.955)]  [G loss: 302.992] \n",
      "695 [D loss: (-15.099)(R 287.351, F -317.550)]  [G loss: 287.991] \n",
      "696 [D loss: (0.104)(R 289.683, F -289.475)]  [G loss: 282.298] \n",
      "696 [D loss: (-31.188)(R 272.931, F -335.308)]  [G loss: 277.091] \n",
      "697 [D loss: (-5.123)(R 259.132, F -269.378)]  [G loss: 256.632] \n",
      "697 [D loss: (-22.567)(R 229.275, F -274.410)]  [G loss: 267.365] \n",
      "698 [D loss: (-2.483)(R 240.822, F -245.787)]  [G loss: 253.109] \n",
      "698 [D loss: (-13.675)(R 207.349, F -234.699)]  [G loss: 232.780] \n",
      "699 [D loss: (-29.323)(R 194.111, F -252.758)]  [G loss: 225.062] \n",
      "699 [D loss: (-17.481)(R 182.171, F -217.132)]  [G loss: 228.372] \n",
      "700 [D loss: (-38.345)(R 161.427, F -238.117)]  [G loss: 215.448] \n",
      "700 [D loss: (-35.552)(R 135.811, F -206.915)]  [G loss: 209.067] \n",
      "701 [D loss: (-33.116)(R 134.450, F -200.683)]  [G loss: 202.813] \n",
      "701 [D loss: (-39.272)(R 118.960, F -197.504)]  [G loss: 196.889] \n",
      "702 [D loss: (-54.141)(R 110.630, F -218.912)]  [G loss: 191.518] \n",
      "702 [D loss: (-40.706)(R 96.352, F -177.764)]  [G loss: 191.101] \n",
      "703 [D loss: (-41.270)(R 79.418, F -161.957)]  [G loss: 175.947] \n",
      "703 [D loss: (-65.713)(R 74.542, F -205.969)]  [G loss: 168.801] \n",
      "704 [D loss: (-53.208)(R 60.027, F -166.443)]  [G loss: 167.679] \n",
      "704 [D loss: (-55.622)(R 55.415, F -166.658)]  [G loss: 156.677] \n",
      "705 [D loss: (-58.265)(R 28.457, F -144.987)]  [G loss: 151.286] \n",
      "705 [D loss: (-75.853)(R 11.613, F -163.320)]  [G loss: 150.967] \n",
      "706 [D loss: (-77.996)(R -3.499, F -152.492)]  [G loss: 148.451] \n",
      "706 [D loss: (-78.897)(R -5.760, F -152.035)]  [G loss: 132.688] \n",
      "707 [D loss: (-87.741)(R -18.407, F -157.075)]  [G loss: 124.063] \n",
      "707 [D loss: (-84.163)(R -30.124, F -138.202)]  [G loss: 131.556] \n",
      "708 [D loss: (-90.736)(R -48.927, F -132.545)]  [G loss: 126.405] \n",
      "708 [D loss: (-85.864)(R -55.128, F -116.601)]  [G loss: 121.470] \n",
      "709 [D loss: (-105.855)(R -75.133, F -136.576)]  [G loss: 124.164] \n",
      "709 [D loss: (-97.822)(R -86.551, F -109.093)]  [G loss: 115.656] \n",
      "710 [D loss: (-98.181)(R -93.220, F -103.143)]  [G loss: 93.165] \n",
      "710 [D loss: (-102.884)(R -119.419, F -86.348)]  [G loss: 95.990] \n",
      "711 [D loss: (-103.017)(R -116.340, F -89.693)]  [G loss: 92.977] \n",
      "711 [D loss: (-127.322)(R -140.438, F -114.205)]  [G loss: 74.906] \n",
      "712 [D loss: (-115.892)(R -147.578, F -84.207)]  [G loss: 89.623] \n",
      "712 [D loss: (-114.895)(R -151.626, F -78.163)]  [G loss: 77.980] \n",
      "713 [D loss: (-116.876)(R -169.856, F -63.897)]  [G loss: 79.767] \n",
      "713 [D loss: (-125.903)(R -175.949, F -75.857)]  [G loss: 80.612] \n",
      "714 [D loss: (-122.095)(R -190.317, F -53.874)]  [G loss: 71.828] \n",
      "714 [D loss: (-125.942)(R -196.582, F -55.303)]  [G loss: 63.209] \n",
      "715 [D loss: (-129.272)(R -211.410, F -47.134)]  [G loss: 64.405] \n",
      "715 [D loss: (-134.824)(R -217.950, F -51.697)]  [G loss: 50.219] \n",
      "716 [D loss: (-128.808)(R -224.569, F -33.047)]  [G loss: 40.555] \n",
      "716 [D loss: (-136.376)(R -232.681, F -40.072)]  [G loss: 36.977] \n",
      "717 [D loss: (-141.517)(R -247.084, F -35.951)]  [G loss: 36.975] \n",
      "717 [D loss: (-140.017)(R -254.011, F -26.022)]  [G loss: 28.691] \n",
      "718 [D loss: (-144.043)(R -265.394, F -22.693)]  [G loss: 25.932] \n",
      "718 [D loss: (-136.336)(R -267.499, F -5.174)]  [G loss: 19.085] \n",
      "719 [D loss: (-133.737)(R -266.206, F -1.268)]  [G loss: 11.246] \n",
      "719 [D loss: (-145.318)(R -269.857, F -20.780)]  [G loss: 15.865] \n",
      "720 [D loss: (-139.954)(R -279.837, F -0.072)]  [G loss: 10.581] \n",
      "720 [D loss: (-146.734)(R -291.934, F -1.533)]  [G loss: 11.419] \n",
      "721 [D loss: (-145.418)(R -284.876, F -5.959)]  [G loss: 8.399] \n",
      "721 [D loss: (-141.512)(R -289.405, F 6.381)]  [G loss: 3.811] \n",
      "722 [D loss: (-155.661)(R -294.572, F -16.751)]  [G loss: -14.335] \n",
      "722 [D loss: (-146.934)(R -299.883, F 6.015)]  [G loss: -19.087] \n",
      "723 [D loss: (-144.893)(R -312.957, F 23.170)]  [G loss: -13.648] \n",
      "723 [D loss: (-153.822)(R -304.675, F -2.970)]  [G loss: -23.768] \n",
      "724 [D loss: (-137.771)(R -305.539, F 29.997)]  [G loss: -20.154] \n",
      "724 [D loss: (-143.188)(R -313.163, F 26.787)]  [G loss: -17.133] \n",
      "725 [D loss: (-150.780)(R -320.990, F 19.429)]  [G loss: -28.493] \n",
      "725 [D loss: (-142.522)(R -325.052, F 40.007)]  [G loss: -21.515] \n",
      "726 [D loss: (-149.800)(R -334.719, F 35.120)]  [G loss: -39.700] \n",
      "726 [D loss: (-146.324)(R -328.245, F 35.596)]  [G loss: -33.066] \n",
      "727 [D loss: (-142.141)(R -337.670, F 53.389)]  [G loss: -47.982] \n",
      "727 [D loss: (-138.938)(R -332.546, F 54.671)]  [G loss: -64.265] \n",
      "728 [D loss: (-154.700)(R -345.059, F 35.658)]  [G loss: -49.877] \n",
      "728 [D loss: (-148.666)(R -353.075, F 55.742)]  [G loss: -59.296] \n",
      "729 [D loss: (-158.858)(R -370.177, F 52.461)]  [G loss: -77.297] \n",
      "729 [D loss: (-156.657)(R -374.415, F 61.102)]  [G loss: -58.863] \n",
      "730 [D loss: (-142.355)(R -364.253, F 79.544)]  [G loss: -66.019] \n",
      "730 [D loss: (-151.849)(R -367.564, F 63.867)]  [G loss: -71.650] \n",
      "731 [D loss: (-154.431)(R -381.460, F 72.599)]  [G loss: -72.121] \n",
      "731 [D loss: (-163.256)(R -388.964, F 62.452)]  [G loss: -69.552] \n",
      "732 [D loss: (-155.836)(R -393.590, F 81.918)]  [G loss: -90.743] \n",
      "732 [D loss: (-147.222)(R -392.371, F 97.926)]  [G loss: -85.815] \n",
      "733 [D loss: (-149.979)(R -394.127, F 94.168)]  [G loss: -93.298] \n",
      "733 [D loss: (-151.160)(R -397.792, F 95.472)]  [G loss: -93.763] \n",
      "734 [D loss: (-143.362)(R -400.407, F 113.684)]  [G loss: -107.833] \n",
      "734 [D loss: (-159.650)(R -406.883, F 87.583)]  [G loss: -102.261] \n",
      "735 [D loss: (-142.618)(R -408.359, F 123.122)]  [G loss: -109.223] \n",
      "735 [D loss: (-145.595)(R -412.385, F 121.195)]  [G loss: -120.788] \n",
      "736 [D loss: (-153.716)(R -410.712, F 103.281)]  [G loss: -117.773] \n",
      "736 [D loss: (-157.812)(R -422.539, F 106.914)]  [G loss: -137.492] \n",
      "737 [D loss: (-145.065)(R -426.400, F 136.271)]  [G loss: -115.944] \n",
      "737 [D loss: (-152.402)(R -439.101, F 134.297)]  [G loss: -136.244] \n",
      "738 [D loss: (-156.931)(R -435.342, F 121.480)]  [G loss: -145.945] \n",
      "738 [D loss: (-141.331)(R -427.247, F 144.585)]  [G loss: -139.922] \n",
      "739 [D loss: (-148.139)(R -436.789, F 140.510)]  [G loss: -143.516] \n",
      "739 [D loss: (-152.028)(R -442.025, F 137.969)]  [G loss: -152.462] \n",
      "740 [D loss: (-149.202)(R -437.533, F 139.128)]  [G loss: -145.922] \n",
      "740 [D loss: (-143.610)(R -434.177, F 146.956)]  [G loss: -161.710] \n",
      "741 [D loss: (-144.420)(R -443.015, F 154.174)]  [G loss: -159.701] \n",
      "741 [D loss: (-150.695)(R -444.344, F 142.954)]  [G loss: -168.631] \n",
      "742 [D loss: (-148.319)(R -448.727, F 152.088)]  [G loss: -165.198] \n",
      "742 [D loss: (-142.330)(R -452.404, F 167.744)]  [G loss: -167.528] \n",
      "743 [D loss: (-134.259)(R -455.148, F 186.631)]  [G loss: -176.970] \n",
      "743 [D loss: (-143.942)(R -448.064, F 160.179)]  [G loss: -182.069] \n",
      "744 [D loss: (-132.817)(R -440.792, F 175.157)]  [G loss: -185.788] \n",
      "744 [D loss: (-131.279)(R -455.660, F 193.102)]  [G loss: -201.113] \n",
      "745 [D loss: (-126.824)(R -457.953, F 204.304)]  [G loss: -190.185] \n",
      "745 [D loss: (-140.543)(R -459.836, F 178.750)]  [G loss: -214.930] \n",
      "746 [D loss: (-140.976)(R -470.647, F 188.695)]  [G loss: -211.607] \n",
      "746 [D loss: (-131.776)(R -463.787, F 200.235)]  [G loss: -205.602] \n",
      "747 [D loss: (-132.055)(R -467.956, F 203.847)]  [G loss: -215.109] \n",
      "747 [D loss: (-122.142)(R -465.473, F 221.189)]  [G loss: -226.086] \n",
      "748 [D loss: (-114.627)(R -470.414, F 241.159)]  [G loss: -241.350] \n",
      "748 [D loss: (-142.120)(R -494.626, F 210.386)]  [G loss: -236.527] \n",
      "749 [D loss: (-130.819)(R -486.609, F 224.972)]  [G loss: -229.031] \n",
      "749 [D loss: (-117.412)(R -481.267, F 246.443)]  [G loss: -243.052] \n",
      "750 [D loss: (-118.354)(R -487.201, F 250.493)]  [G loss: -254.392] \n",
      "750 [D loss: (-105.564)(R -476.304, F 265.175)]  [G loss: -270.756] \n",
      "751 [D loss: (-119.828)(R -491.044, F 251.388)]  [G loss: -282.882] \n",
      "751 [D loss: (-113.834)(R -492.407, F 264.739)]  [G loss: -281.727] \n",
      "752 [D loss: (-124.515)(R -502.753, F 253.722)]  [G loss: -295.190] \n",
      "752 [D loss: (-110.592)(R -514.989, F 293.805)]  [G loss: -293.710] \n",
      "753 [D loss: (-118.292)(R -512.948, F 276.363)]  [G loss: -298.559] \n",
      "753 [D loss: (-114.407)(R -510.291, F 281.477)]  [G loss: -302.695] \n",
      "754 [D loss: (-94.809)(R -500.150, F 310.533)]  [G loss: -311.529] \n",
      "754 [D loss: (-108.815)(R -504.367, F 286.737)]  [G loss: -315.417] \n",
      "755 [D loss: (-115.452)(R -512.379, F 281.475)]  [G loss: -329.380] \n",
      "755 [D loss: (-105.446)(R -527.931, F 317.039)]  [G loss: -340.152] \n",
      "756 [D loss: (-105.445)(R -527.238, F 316.347)]  [G loss: -345.221] \n",
      "756 [D loss: (-106.734)(R -534.606, F 321.138)]  [G loss: -353.867] \n",
      "757 [D loss: (-85.687)(R -521.282, F 349.908)]  [G loss: -359.884] \n",
      "757 [D loss: (-90.559)(R -535.833, F 354.715)]  [G loss: -371.736] \n",
      "758 [D loss: (-89.295)(R -537.509, F 358.919)]  [G loss: -362.430] \n",
      "758 [D loss: (-85.080)(R -540.478, F 370.319)]  [G loss: -383.096] \n",
      "759 [D loss: (-91.369)(R -545.613, F 362.875)]  [G loss: -392.915] \n",
      "759 [D loss: (-75.582)(R -538.839, F 387.675)]  [G loss: -385.901] \n",
      "760 [D loss: (-82.777)(R -546.603, F 381.050)]  [G loss: -413.646] \n",
      "760 [D loss: (-78.614)(R -556.618, F 399.391)]  [G loss: -411.685] \n",
      "761 [D loss: (-69.069)(R -549.783, F 411.645)]  [G loss: -409.833] \n",
      "761 [D loss: (-66.246)(R -547.138, F 414.646)]  [G loss: -418.971] \n",
      "762 [D loss: (-65.100)(R -560.824, F 430.625)]  [G loss: -421.518] \n",
      "762 [D loss: (-58.878)(R -557.671, F 439.914)]  [G loss: -434.985] \n",
      "763 [D loss: (-71.309)(R -557.095, F 414.478)]  [G loss: -435.645] \n",
      "763 [D loss: (-59.338)(R -556.201, F 437.524)]  [G loss: -454.076] \n",
      "764 [D loss: (-60.612)(R -561.735, F 440.512)]  [G loss: -456.572] \n",
      "764 [D loss: (-52.830)(R -564.308, F 458.649)]  [G loss: -461.625] \n",
      "765 [D loss: (-47.051)(R -572.453, F 478.350)]  [G loss: -471.430] \n",
      "765 [D loss: (-46.083)(R -571.893, F 479.726)]  [G loss: -480.932] \n",
      "766 [D loss: (-56.436)(R -572.257, F 459.386)]  [G loss: -492.072] \n",
      "766 [D loss: (-31.857)(R -556.121, F 492.406)]  [G loss: -483.740] \n",
      "767 [D loss: (-42.560)(R -580.721, F 495.601)]  [G loss: -508.989] \n",
      "767 [D loss: (-49.091)(R -582.852, F 484.670)]  [G loss: -508.174] \n",
      "768 [D loss: (-24.734)(R -568.994, F 519.526)]  [G loss: -504.262] \n",
      "768 [D loss: (-27.028)(R -571.509, F 517.453)]  [G loss: -518.778] \n",
      "769 [D loss: (-33.663)(R -593.443, F 526.117)]  [G loss: -528.320] \n",
      "769 [D loss: (-29.169)(R -585.585, F 527.248)]  [G loss: -540.731] \n",
      "770 [D loss: (-22.246)(R -590.286, F 545.794)]  [G loss: -550.224] \n",
      "770 [D loss: (-27.698)(R -585.995, F 530.600)]  [G loss: -566.957] \n",
      "771 [D loss: (-5.535)(R -590.290, F 579.221)]  [G loss: -558.319] \n",
      "771 [D loss: (-10.587)(R -599.624, F 578.450)]  [G loss: -569.466] \n",
      "772 [D loss: (-18.486)(R -602.028, F 565.056)]  [G loss: -578.161] \n",
      "772 [D loss: (-25.776)(R -601.194, F 549.642)]  [G loss: -591.437] \n",
      "773 [D loss: (-11.441)(R -614.957, F 592.074)]  [G loss: -596.789] \n",
      "773 [D loss: (-9.357)(R -619.510, F 600.795)]  [G loss: -614.358] \n",
      "774 [D loss: (-6.092)(R -622.627, F 610.442)]  [G loss: -611.067] \n",
      "774 [D loss: (-3.264)(R -612.654, F 606.126)]  [G loss: -623.077] \n",
      "775 [D loss: (-0.774)(R -615.980, F 614.432)]  [G loss: -622.940] \n",
      "775 [D loss: (-5.029)(R -633.246, F 623.188)]  [G loss: -634.664] \n",
      "776 [D loss: (-11.258)(R -635.706, F 613.190)]  [G loss: -634.367] \n",
      "776 [D loss: (7.319)(R -624.049, F 638.687)]  [G loss: -630.069] \n",
      "777 [D loss: (2.653)(R -624.866, F 630.173)]  [G loss: -633.033] \n",
      "777 [D loss: (5.140)(R -627.043, F 637.323)]  [G loss: -651.559] \n",
      "778 [D loss: (-6.417)(R -642.737, F 629.903)]  [G loss: -648.494] \n",
      "778 [D loss: (11.450)(R -620.245, F 643.145)]  [G loss: -654.812] \n",
      "779 [D loss: (6.300)(R -632.834, F 645.433)]  [G loss: -658.344] \n",
      "779 [D loss: (3.308)(R -637.027, F 643.642)]  [G loss: -660.247] \n",
      "780 [D loss: (2.297)(R -638.238, F 642.832)]  [G loss: -664.708] \n",
      "780 [D loss: (15.347)(R -620.594, F 651.288)]  [G loss: -664.047] \n",
      "781 [D loss: (18.803)(R -626.786, F 664.392)]  [G loss: -662.056] \n",
      "781 [D loss: (17.608)(R -622.969, F 658.184)]  [G loss: -656.838] \n",
      "782 [D loss: (15.455)(R -626.758, F 657.668)]  [G loss: -656.362] \n",
      "782 [D loss: (9.966)(R -619.748, F 639.681)]  [G loss: -646.581] \n",
      "783 [D loss: (8.817)(R -619.795, F 637.429)]  [G loss: -649.523] \n",
      "783 [D loss: (29.466)(R -595.099, F 654.031)]  [G loss: -646.372] \n",
      "784 [D loss: (32.325)(R -603.922, F 668.573)]  [G loss: -645.510] \n",
      "784 [D loss: (20.410)(R -601.424, F 642.245)]  [G loss: -637.190] \n",
      "785 [D loss: (19.581)(R -592.435, F 631.598)]  [G loss: -627.979] \n",
      "785 [D loss: (15.639)(R -603.662, F 634.940)]  [G loss: -626.388] \n",
      "786 [D loss: (14.680)(R -597.679, F 627.038)]  [G loss: -617.404] \n",
      "786 [D loss: (22.590)(R -571.051, F 616.231)]  [G loss: -605.381] \n",
      "787 [D loss: (22.153)(R -567.109, F 611.415)]  [G loss: -601.505] \n",
      "787 [D loss: (16.114)(R -558.735, F 590.963)]  [G loss: -595.205] \n",
      "788 [D loss: (28.447)(R -541.185, F 598.078)]  [G loss: -587.306] \n",
      "788 [D loss: (22.613)(R -539.560, F 584.786)]  [G loss: -573.755] \n",
      "789 [D loss: (12.687)(R -535.983, F 561.356)]  [G loss: -565.473] \n",
      "789 [D loss: (21.957)(R -514.317, F 558.230)]  [G loss: -558.084] \n",
      "790 [D loss: (34.337)(R -499.341, F 568.014)]  [G loss: -548.717] \n",
      "790 [D loss: (18.552)(R -501.776, F 538.881)]  [G loss: -534.344] \n",
      "791 [D loss: (21.769)(R -488.921, F 532.459)]  [G loss: -527.002] \n",
      "791 [D loss: (27.984)(R -475.760, F 531.728)]  [G loss: -518.275] \n",
      "792 [D loss: (29.358)(R -456.706, F 515.422)]  [G loss: -507.651] \n",
      "792 [D loss: (30.787)(R -457.647, F 519.222)]  [G loss: -495.398] \n",
      "793 [D loss: (21.139)(R -448.195, F 490.472)]  [G loss: -481.416] \n",
      "793 [D loss: (16.020)(R -431.939, F 463.979)]  [G loss: -468.074] \n",
      "794 [D loss: (22.854)(R -420.362, F 466.070)]  [G loss: -454.712] \n",
      "794 [D loss: (21.621)(R -409.716, F 452.958)]  [G loss: -452.037] \n",
      "795 [D loss: (25.332)(R -396.614, F 447.278)]  [G loss: -441.330] \n",
      "795 [D loss: (29.159)(R -381.075, F 439.392)]  [G loss: -428.594] \n",
      "796 [D loss: (22.204)(R -381.028, F 425.437)]  [G loss: -417.101] \n",
      "796 [D loss: (22.222)(R -369.923, F 414.367)]  [G loss: -406.538] \n",
      "797 [D loss: (22.779)(R -364.428, F 409.986)]  [G loss: -397.682] \n",
      "797 [D loss: (17.567)(R -354.505, F 389.640)]  [G loss: -390.554] \n",
      "798 [D loss: (17.808)(R -341.790, F 377.407)]  [G loss: -377.458] \n",
      "798 [D loss: (19.251)(R -331.954, F 370.455)]  [G loss: -369.042] \n",
      "799 [D loss: (21.711)(R -319.428, F 362.851)]  [G loss: -360.552] \n",
      "799 [D loss: (20.168)(R -313.696, F 354.033)]  [G loss: -351.673] \n",
      "800 [D loss: (20.981)(R -304.516, F 346.477)]  [G loss: -340.335] \n",
      "800 [D loss: (14.595)(R -295.835, F 325.025)]  [G loss: -333.036] \n",
      "801 [D loss: (16.590)(R -288.470, F 321.651)]  [G loss: -323.827] \n",
      "801 [D loss: (21.944)(R -276.908, F 320.796)]  [G loss: -317.055] \n",
      "802 [D loss: (20.226)(R -276.610, F 317.063)]  [G loss: -310.804] \n",
      "802 [D loss: (17.462)(R -265.459, F 300.382)]  [G loss: -298.411] \n",
      "803 [D loss: (15.209)(R -261.409, F 291.826)]  [G loss: -292.560] \n",
      "803 [D loss: (20.082)(R -252.705, F 292.870)]  [G loss: -286.367] \n",
      "804 [D loss: (20.294)(R -244.023, F 284.610)]  [G loss: -278.734] \n",
      "804 [D loss: (19.112)(R -236.316, F 274.539)]  [G loss: -271.424] \n",
      "805 [D loss: (18.519)(R -232.152, F 269.190)]  [G loss: -263.128] \n",
      "805 [D loss: (16.835)(R -227.919, F 261.589)]  [G loss: -259.219] \n",
      "806 [D loss: (13.354)(R -224.218, F 250.927)]  [G loss: -251.676] \n",
      "806 [D loss: (15.869)(R -216.168, F 247.907)]  [G loss: -246.783] \n",
      "807 [D loss: (15.870)(R -210.040, F 241.780)]  [G loss: -241.449] \n",
      "807 [D loss: (14.483)(R -207.567, F 236.534)]  [G loss: -235.233] \n",
      "808 [D loss: (16.717)(R -200.298, F 233.731)]  [G loss: -229.939] \n",
      "808 [D loss: (15.650)(R -194.978, F 226.278)]  [G loss: -222.366] \n",
      "809 [D loss: (14.126)(R -190.876, F 219.127)]  [G loss: -218.612] \n",
      "809 [D loss: (15.084)(R -184.586, F 214.755)]  [G loss: -211.768] \n",
      "810 [D loss: (15.601)(R -180.216, F 211.418)]  [G loss: -206.639] \n",
      "810 [D loss: (12.749)(R -175.260, F 200.757)]  [G loss: -202.703] \n",
      "811 [D loss: (15.194)(R -171.035, F 201.423)]  [G loss: -197.464] \n",
      "811 [D loss: (14.604)(R -165.698, F 194.906)]  [G loss: -193.913] \n",
      "812 [D loss: (12.227)(R -164.052, F 188.506)]  [G loss: -188.253] \n",
      "812 [D loss: (12.627)(R -158.395, F 183.648)]  [G loss: -182.559] \n",
      "813 [D loss: (12.409)(R -153.813, F 178.630)]  [G loss: -178.399] \n",
      "813 [D loss: (12.471)(R -151.457, F 176.398)]  [G loss: -174.819] \n",
      "814 [D loss: (8.572)(R -151.944, F 169.088)]  [G loss: -171.330] \n",
      "814 [D loss: (12.275)(R -143.826, F 168.376)]  [G loss: -167.118] \n",
      "815 [D loss: (12.653)(R -141.926, F 167.233)]  [G loss: -163.132] \n",
      "815 [D loss: (12.907)(R -136.134, F 161.949)]  [G loss: -159.532] \n",
      "816 [D loss: (11.650)(R -133.503, F 156.803)]  [G loss: -156.290] \n",
      "816 [D loss: (11.054)(R -132.043, F 154.152)]  [G loss: -151.948] \n",
      "817 [D loss: (9.366)(R -129.640, F 148.372)]  [G loss: -148.924] \n",
      "817 [D loss: (8.998)(R -125.847, F 143.843)]  [G loss: -145.593] \n",
      "818 [D loss: (9.791)(R -122.207, F 141.789)]  [G loss: -141.876] \n",
      "818 [D loss: (9.553)(R -119.726, F 138.833)]  [G loss: -139.189] \n",
      "819 [D loss: (11.013)(R -118.072, F 140.098)]  [G loss: -136.703] \n",
      "819 [D loss: (9.501)(R -114.714, F 133.716)]  [G loss: -132.964] \n",
      "820 [D loss: (8.008)(R -112.906, F 128.922)]  [G loss: -129.975] \n",
      "820 [D loss: (9.132)(R -110.044, F 128.308)]  [G loss: -127.870] \n",
      "821 [D loss: (8.251)(R -109.090, F 125.591)]  [G loss: -124.411] \n",
      "821 [D loss: (8.115)(R -105.561, F 121.792)]  [G loss: -121.290] \n",
      "822 [D loss: (9.812)(R -101.996, F 121.619)]  [G loss: -119.419] \n",
      "822 [D loss: (9.258)(R -100.272, F 118.788)]  [G loss: -117.730] \n",
      "823 [D loss: (7.602)(R -99.572, F 114.776)]  [G loss: -115.193] \n",
      "823 [D loss: (8.534)(R -97.833, F 114.902)]  [G loss: -112.864] \n",
      "824 [D loss: (7.908)(R -95.097, F 110.914)]  [G loss: -109.965] \n",
      "824 [D loss: (8.245)(R -91.181, F 107.671)]  [G loss: -108.525] \n",
      "825 [D loss: (7.562)(R -92.141, F 107.266)]  [G loss: -106.119] \n",
      "825 [D loss: (7.562)(R -90.075, F 105.199)]  [G loss: -103.904] \n",
      "826 [D loss: (6.595)(R -88.635, F 101.824)]  [G loss: -102.589] \n",
      "826 [D loss: (7.037)(R -86.726, F 100.800)]  [G loss: -100.174] \n",
      "827 [D loss: (6.159)(R -85.697, F 98.014)]  [G loss: -99.390] \n",
      "827 [D loss: (6.650)(R -82.932, F 96.231)]  [G loss: -96.111] \n",
      "828 [D loss: (6.289)(R -82.043, F 94.621)]  [G loss: -95.385] \n",
      "828 [D loss: (6.431)(R -80.502, F 93.363)]  [G loss: -92.761] \n",
      "829 [D loss: (6.620)(R -78.240, F 91.479)]  [G loss: -90.591] \n",
      "829 [D loss: (6.745)(R -76.832, F 90.322)]  [G loss: -88.673] \n",
      "830 [D loss: (5.569)(R -75.724, F 86.862)]  [G loss: -88.054] \n",
      "830 [D loss: (6.479)(R -73.379, F 86.337)]  [G loss: -85.970] \n",
      "831 [D loss: (5.099)(R -73.868, F 84.067)]  [G loss: -84.069] \n",
      "831 [D loss: (5.169)(R -71.829, F 82.167)]  [G loss: -81.825] \n",
      "832 [D loss: (4.683)(R -71.529, F 80.894)]  [G loss: -80.343] \n",
      "832 [D loss: (5.427)(R -67.981, F 78.834)]  [G loss: -79.648] \n",
      "833 [D loss: (5.476)(R -66.055, F 77.007)]  [G loss: -76.650] \n",
      "833 [D loss: (5.302)(R -65.146, F 75.749)]  [G loss: -76.602] \n",
      "834 [D loss: (3.855)(R -66.244, F 73.954)]  [G loss: -75.143] \n",
      "834 [D loss: (6.331)(R -62.074, F 74.735)]  [G loss: -73.899] \n",
      "835 [D loss: (5.295)(R -62.152, F 72.742)]  [G loss: -71.864] \n",
      "835 [D loss: (5.325)(R -60.665, F 71.315)]  [G loss: -70.140] \n",
      "836 [D loss: (5.652)(R -60.056, F 71.359)]  [G loss: -69.198] \n",
      "836 [D loss: (3.900)(R -59.856, F 67.657)]  [G loss: -68.765] \n",
      "837 [D loss: (3.861)(R -58.853, F 66.575)]  [G loss: -67.035] \n",
      "837 [D loss: (4.875)(R -57.322, F 67.072)]  [G loss: -65.546] \n",
      "838 [D loss: (3.758)(R -56.209, F 63.725)]  [G loss: -64.830] \n",
      "838 [D loss: (3.915)(R -56.062, F 63.893)]  [G loss: -62.910] \n",
      "839 [D loss: (4.652)(R -53.889, F 63.193)]  [G loss: -62.122] \n",
      "839 [D loss: (4.372)(R -53.010, F 61.754)]  [G loss: -62.104] \n",
      "840 [D loss: (4.180)(R -52.287, F 60.648)]  [G loss: -59.946] \n",
      "840 [D loss: (4.339)(R -51.232, F 59.910)]  [G loss: -59.564] \n",
      "841 [D loss: (3.814)(R -50.130, F 57.758)]  [G loss: -58.222] \n",
      "841 [D loss: (3.748)(R -50.128, F 57.623)]  [G loss: -57.172] \n",
      "842 [D loss: (3.577)(R -49.327, F 56.481)]  [G loss: -56.311] \n",
      "842 [D loss: (3.620)(R -48.244, F 55.483)]  [G loss: -55.251] \n",
      "843 [D loss: (3.944)(R -47.472, F 55.361)]  [G loss: -54.160] \n",
      "843 [D loss: (3.477)(R -47.292, F 54.245)]  [G loss: -54.155] \n",
      "844 [D loss: (3.502)(R -46.000, F 53.004)]  [G loss: -52.476] \n",
      "844 [D loss: (3.616)(R -45.822, F 53.054)]  [G loss: -51.192] \n",
      "845 [D loss: (3.234)(R -44.639, F 51.107)]  [G loss: -50.370] \n",
      "845 [D loss: (3.602)(R -43.389, F 50.594)]  [G loss: -50.078] \n",
      "846 [D loss: (3.813)(R -42.581, F 50.207)]  [G loss: -48.869] \n",
      "846 [D loss: (2.965)(R -42.920, F 48.850)]  [G loss: -48.750] \n",
      "847 [D loss: (2.900)(R -42.201, F 48.001)]  [G loss: -47.687] \n",
      "847 [D loss: (3.052)(R -40.954, F 47.057)]  [G loss: -47.261] \n",
      "848 [D loss: (3.115)(R -40.612, F 46.841)]  [G loss: -46.516] \n",
      "848 [D loss: (2.954)(R -40.080, F 45.988)]  [G loss: -46.002] \n",
      "849 [D loss: (2.655)(R -39.085, F 44.394)]  [G loss: -44.773] \n",
      "849 [D loss: (2.643)(R -38.669, F 43.955)]  [G loss: -44.438] \n",
      "850 [D loss: (2.437)(R -38.174, F 43.047)]  [G loss: -43.449] \n",
      "850 [D loss: (2.323)(R -37.444, F 42.090)]  [G loss: -43.061] \n",
      "851 [D loss: (2.250)(R -37.470, F 41.969)]  [G loss: -42.097] \n",
      "851 [D loss: (2.801)(R -36.622, F 42.223)]  [G loss: -41.304] \n",
      "852 [D loss: (2.357)(R -36.312, F 41.026)]  [G loss: -41.327] \n",
      "852 [D loss: (2.779)(R -34.949, F 40.507)]  [G loss: -40.309] \n",
      "853 [D loss: (1.926)(R -35.073, F 38.926)]  [G loss: -39.787] \n",
      "853 [D loss: (2.239)(R -34.765, F 39.242)]  [G loss: -39.638] \n",
      "854 [D loss: (2.266)(R -33.908, F 38.440)]  [G loss: -38.777] \n",
      "854 [D loss: (2.251)(R -33.398, F 37.900)]  [G loss: -38.697] \n",
      "855 [D loss: (2.284)(R -33.153, F 37.722)]  [G loss: -37.637] \n",
      "855 [D loss: (2.484)(R -32.265, F 37.234)]  [G loss: -37.063] \n",
      "856 [D loss: (2.347)(R -31.428, F 36.122)]  [G loss: -36.376] \n",
      "856 [D loss: (1.878)(R -31.390, F 35.146)]  [G loss: -35.849] \n",
      "857 [D loss: (1.937)(R -31.196, F 35.070)]  [G loss: -35.436] \n",
      "857 [D loss: (1.852)(R -30.834, F 34.537)]  [G loss: -35.031] \n",
      "858 [D loss: (2.014)(R -29.963, F 33.990)]  [G loss: -33.719] \n",
      "858 [D loss: (1.928)(R -29.938, F 33.794)]  [G loss: -33.657] \n",
      "859 [D loss: (1.946)(R -29.278, F 33.171)]  [G loss: -33.139] \n",
      "859 [D loss: (2.163)(R -29.149, F 33.474)]  [G loss: -32.337] \n",
      "860 [D loss: (1.882)(R -28.772, F 32.537)]  [G loss: -32.416] \n",
      "860 [D loss: (2.550)(R -27.426, F 32.526)]  [G loss: -31.518] \n",
      "861 [D loss: (1.726)(R -27.733, F 31.185)]  [G loss: -31.065] \n",
      "861 [D loss: (1.880)(R -27.349, F 31.109)]  [G loss: -30.551] \n",
      "862 [D loss: (1.775)(R -26.932, F 30.482)]  [G loss: -30.118] \n",
      "862 [D loss: (1.790)(R -26.303, F 29.883)]  [G loss: -30.222] \n",
      "863 [D loss: (1.562)(R -25.949, F 29.072)]  [G loss: -29.223] \n",
      "863 [D loss: (1.556)(R -25.788, F 28.900)]  [G loss: -29.133] \n",
      "864 [D loss: (1.656)(R -25.202, F 28.515)]  [G loss: -28.835] \n",
      "864 [D loss: (1.602)(R -24.477, F 27.680)]  [G loss: -28.362] \n",
      "865 [D loss: (1.481)(R -25.165, F 28.127)]  [G loss: -27.465] \n",
      "865 [D loss: (1.829)(R -24.217, F 27.875)]  [G loss: -27.912] \n",
      "866 [D loss: (1.331)(R -24.431, F 27.092)]  [G loss: -26.578] \n",
      "866 [D loss: (2.056)(R -23.506, F 27.617)]  [G loss: -26.769] \n",
      "867 [D loss: (1.305)(R -23.513, F 26.123)]  [G loss: -26.291] \n",
      "867 [D loss: (1.654)(R -22.896, F 26.203)]  [G loss: -26.085] \n",
      "868 [D loss: (1.585)(R -22.364, F 25.533)]  [G loss: -25.537] \n",
      "868 [D loss: (1.525)(R -22.305, F 25.354)]  [G loss: -25.290] \n",
      "869 [D loss: (1.564)(R -21.832, F 24.960)]  [G loss: -24.313] \n",
      "869 [D loss: (1.144)(R -21.886, F 24.174)]  [G loss: -24.324] \n",
      "870 [D loss: (1.422)(R -21.318, F 24.162)]  [G loss: -23.930] \n",
      "870 [D loss: (1.642)(R -20.711, F 23.994)]  [G loss: -24.045] \n",
      "871 [D loss: (1.032)(R -20.822, F 22.887)]  [G loss: -23.050] \n",
      "871 [D loss: (1.289)(R -20.244, F 22.822)]  [G loss: -23.111] \n",
      "872 [D loss: (1.339)(R -20.355, F 23.032)]  [G loss: -22.703] \n",
      "872 [D loss: (1.437)(R -19.916, F 22.791)]  [G loss: -21.837] \n",
      "873 [D loss: (1.015)(R -19.957, F 21.987)]  [G loss: -22.135] \n",
      "873 [D loss: (1.187)(R -19.603, F 21.977)]  [G loss: -21.243] \n",
      "874 [D loss: (0.842)(R -19.433, F 21.117)]  [G loss: -21.699] \n",
      "874 [D loss: (1.334)(R -18.508, F 21.177)]  [G loss: -21.026] \n",
      "875 [D loss: (0.922)(R -18.907, F 20.751)]  [G loss: -20.599] \n",
      "875 [D loss: (0.758)(R -18.388, F 19.904)]  [G loss: -20.328] \n",
      "876 [D loss: (1.206)(R -18.007, F 20.419)]  [G loss: -19.829] \n",
      "876 [D loss: (1.409)(R -17.432, F 20.251)]  [G loss: -19.777] \n",
      "877 [D loss: (0.990)(R -17.375, F 19.354)]  [G loss: -19.676] \n",
      "877 [D loss: (0.898)(R -17.178, F 18.974)]  [G loss: -19.084] \n",
      "878 [D loss: (0.767)(R -16.876, F 18.410)]  [G loss: -18.861] \n",
      "878 [D loss: (0.879)(R -16.521, F 18.279)]  [G loss: -18.502] \n",
      "879 [D loss: (0.669)(R -16.753, F 18.091)]  [G loss: -18.136] \n",
      "879 [D loss: (0.799)(R -16.153, F 17.750)]  [G loss: -17.930] \n",
      "880 [D loss: (0.911)(R -15.837, F 17.658)]  [G loss: -17.682] \n",
      "880 [D loss: (0.955)(R -15.405, F 17.315)]  [G loss: -16.895] \n",
      "881 [D loss: (0.693)(R -15.644, F 17.030)]  [G loss: -17.017] \n",
      "881 [D loss: (0.978)(R -15.089, F 17.045)]  [G loss: -16.419] \n",
      "882 [D loss: (0.616)(R -14.940, F 16.172)]  [G loss: -16.481] \n",
      "882 [D loss: (0.681)(R -14.764, F 16.127)]  [G loss: -15.636] \n",
      "883 [D loss: (0.432)(R -14.704, F 15.568)]  [G loss: -15.373] \n",
      "883 [D loss: (0.341)(R -14.705, F 15.386)]  [G loss: -15.364] \n",
      "884 [D loss: (0.371)(R -14.266, F 15.009)]  [G loss: -15.067] \n",
      "884 [D loss: (0.475)(R -13.751, F 14.700)]  [G loss: -15.410] \n",
      "885 [D loss: (0.573)(R -13.591, F 14.737)]  [G loss: -14.872] \n",
      "885 [D loss: (0.290)(R -13.608, F 14.189)]  [G loss: -14.295] \n",
      "886 [D loss: (0.250)(R -13.268, F 13.769)]  [G loss: -14.212] \n",
      "886 [D loss: (0.558)(R -12.735, F 13.851)]  [G loss: -14.006] \n",
      "887 [D loss: (0.598)(R -12.868, F 14.064)]  [G loss: -13.782] \n",
      "887 [D loss: (0.750)(R -12.338, F 13.838)]  [G loss: -13.603] \n",
      "888 [D loss: (0.745)(R -12.181, F 13.670)]  [G loss: -13.124] \n",
      "888 [D loss: (0.489)(R -12.351, F 13.329)]  [G loss: -13.321] \n",
      "889 [D loss: (0.284)(R -11.781, F 12.348)]  [G loss: -12.731] \n",
      "889 [D loss: (0.646)(R -11.375, F 12.666)]  [G loss: -12.530] \n",
      "890 [D loss: (0.406)(R -11.233, F 12.046)]  [G loss: -12.510] \n",
      "890 [D loss: (0.393)(R -11.144, F 11.929)]  [G loss: -11.575] \n",
      "891 [D loss: (0.302)(R -11.262, F 11.867)]  [G loss: -11.882] \n",
      "891 [D loss: (0.118)(R -10.705, F 10.940)]  [G loss: -11.903] \n",
      "892 [D loss: (0.494)(R -10.781, F 11.769)]  [G loss: -11.124] \n",
      "892 [D loss: (0.366)(R -10.203, F 10.934)]  [G loss: -10.771] \n",
      "893 [D loss: (0.577)(R -9.801, F 10.954)]  [G loss: -10.482] \n",
      "893 [D loss: (-0.002)(R -10.256, F 10.252)]  [G loss: -10.740] \n",
      "894 [D loss: (0.560)(R -9.490, F 10.610)]  [G loss: -10.220] \n",
      "894 [D loss: (0.511)(R -9.347, F 10.370)]  [G loss: -9.862] \n",
      "895 [D loss: (0.282)(R -9.513, F 10.077)]  [G loss: -9.586] \n",
      "895 [D loss: (-0.130)(R -9.275, F 9.015)]  [G loss: -9.430] \n",
      "896 [D loss: (0.521)(R -8.859, F 9.902)]  [G loss: -9.182] \n",
      "896 [D loss: (0.077)(R -8.683, F 8.838)]  [G loss: -9.462] \n",
      "897 [D loss: (0.674)(R -8.332, F 9.680)]  [G loss: -8.567] \n",
      "897 [D loss: (0.457)(R -8.082, F 8.995)]  [G loss: -8.698] \n",
      "898 [D loss: (0.145)(R -7.602, F 7.893)]  [G loss: -8.137] \n",
      "898 [D loss: (-0.141)(R -8.101, F 7.819)]  [G loss: -7.813] \n",
      "899 [D loss: (0.139)(R -7.686, F 7.963)]  [G loss: -7.650] \n",
      "899 [D loss: (0.302)(R -7.275, F 7.880)]  [G loss: -7.435] \n",
      "900 [D loss: (-0.188)(R -7.590, F 7.213)]  [G loss: -6.816] \n",
      "900 [D loss: (0.142)(R -7.040, F 7.324)]  [G loss: -7.531] \n",
      "901 [D loss: (0.007)(R -7.350, F 7.365)]  [G loss: -6.904] \n",
      "901 [D loss: (-0.314)(R -7.167, F 6.539)]  [G loss: -6.592] \n",
      "902 [D loss: (0.063)(R -6.204, F 6.330)]  [G loss: -6.228] \n",
      "902 [D loss: (-0.513)(R -6.748, F 5.723)]  [G loss: -6.566] \n",
      "903 [D loss: (-0.401)(R -6.271, F 5.470)]  [G loss: -6.234] \n",
      "903 [D loss: (-0.179)(R -6.049, F 5.691)]  [G loss: -5.513] \n",
      "904 [D loss: (0.278)(R -5.320, F 5.877)]  [G loss: -5.271] \n",
      "904 [D loss: (-0.082)(R -5.606, F 5.442)]  [G loss: -5.239] \n",
      "905 [D loss: (0.232)(R -4.952, F 5.416)]  [G loss: -4.671] \n",
      "905 [D loss: (-0.081)(R -4.903, F 4.741)]  [G loss: -4.805] \n",
      "906 [D loss: (0.021)(R -4.829, F 4.872)]  [G loss: -4.385] \n",
      "906 [D loss: (-0.114)(R -4.502, F 4.274)]  [G loss: -3.851] \n",
      "907 [D loss: (-0.243)(R -4.437, F 3.951)]  [G loss: -4.100] \n",
      "907 [D loss: (-0.342)(R -4.262, F 3.578)]  [G loss: -3.470] \n",
      "908 [D loss: (-0.077)(R -3.602, F 3.447)]  [G loss: -3.369] \n",
      "908 [D loss: (-0.422)(R -3.647, F 2.803)]  [G loss: -2.851] \n",
      "909 [D loss: (-0.421)(R -3.740, F 2.898)]  [G loss: -3.110] \n",
      "909 [D loss: (-0.441)(R -2.931, F 2.049)]  [G loss: -2.765] \n",
      "910 [D loss: (-0.604)(R -3.359, F 2.151)]  [G loss: -2.126] \n",
      "910 [D loss: (-0.354)(R -3.002, F 2.293)]  [G loss: -1.857] \n",
      "911 [D loss: (-0.240)(R -2.742, F 2.262)]  [G loss: -1.946] \n",
      "911 [D loss: (-0.670)(R -2.767, F 1.426)]  [G loss: -1.429] \n",
      "912 [D loss: (-0.548)(R -2.268, F 1.173)]  [G loss: -1.236] \n",
      "912 [D loss: (-0.489)(R -2.119, F 1.141)]  [G loss: -0.751] \n",
      "913 [D loss: (-0.576)(R -1.870, F 0.718)]  [G loss: -0.859] \n",
      "913 [D loss: (-0.332)(R -1.187, F 0.522)]  [G loss: -0.155] \n",
      "914 [D loss: (-0.750)(R -1.459, F -0.041)]  [G loss: 0.296] \n",
      "914 [D loss: (-0.580)(R -1.007, F -0.152)]  [G loss: 0.321] \n",
      "915 [D loss: (-0.246)(R -0.396, F -0.096)]  [G loss: 0.497] \n",
      "915 [D loss: (-0.602)(R -0.817, F -0.387)]  [G loss: 1.319] \n",
      "916 [D loss: (-0.328)(R -0.158, F -0.499)]  [G loss: 1.031] \n",
      "916 [D loss: (-0.361)(R 0.593, F -1.315)]  [G loss: 1.666] \n",
      "917 [D loss: (-0.440)(R 0.927, F -1.806)]  [G loss: 1.831] \n",
      "917 [D loss: (-0.387)(R 0.786, F -1.560)]  [G loss: 1.829] \n",
      "918 [D loss: (-0.802)(R 0.755, F -2.359)]  [G loss: 2.548] \n",
      "918 [D loss: (-0.826)(R 0.740, F -2.392)]  [G loss: 2.757] \n",
      "919 [D loss: (-0.447)(R 1.479, F -2.374)]  [G loss: 3.031] \n",
      "919 [D loss: (-0.202)(R 1.970, F -2.374)]  [G loss: 3.614] \n",
      "920 [D loss: (-0.745)(R 1.994, F -3.484)]  [G loss: 3.895] \n",
      "920 [D loss: (-1.067)(R 2.085, F -4.219)]  [G loss: 3.731] \n",
      "921 [D loss: (-0.415)(R 2.401, F -3.231)]  [G loss: 4.567] \n",
      "921 [D loss: (-0.639)(R 2.450, F -3.728)]  [G loss: 4.958] \n",
      "922 [D loss: (-0.986)(R 3.005, F -4.977)]  [G loss: 5.365] \n",
      "922 [D loss: (-1.312)(R 2.922, F -5.545)]  [G loss: 4.853] \n",
      "923 [D loss: (-0.649)(R 4.166, F -5.464)]  [G loss: 5.714] \n",
      "923 [D loss: (-0.465)(R 4.475, F -5.406)]  [G loss: 5.999] \n",
      "924 [D loss: (-0.950)(R 4.917, F -6.816)]  [G loss: 6.259] \n",
      "924 [D loss: (-0.841)(R 5.382, F -7.064)]  [G loss: 7.560] \n",
      "925 [D loss: (-0.977)(R 5.569, F -7.524)]  [G loss: 7.411] \n",
      "925 [D loss: (-1.442)(R 5.107, F -7.991)]  [G loss: 7.494] \n",
      "926 [D loss: (-1.627)(R 5.587, F -8.841)]  [G loss: 8.470] \n",
      "926 [D loss: (-0.819)(R 7.112, F -8.751)]  [G loss: 8.303] \n",
      "927 [D loss: (-1.534)(R 6.146, F -9.213)]  [G loss: 8.434] \n",
      "927 [D loss: (-0.859)(R 7.344, F -9.063)]  [G loss: 10.370] \n",
      "928 [D loss: (-1.059)(R 7.838, F -9.956)]  [G loss: 9.966] \n",
      "928 [D loss: (-0.906)(R 7.934, F -9.747)]  [G loss: 10.516] \n",
      "929 [D loss: (-1.026)(R 8.406, F -10.457)]  [G loss: 11.512] \n",
      "929 [D loss: (-1.330)(R 8.719, F -11.380)]  [G loss: 11.394] \n",
      "930 [D loss: (-1.009)(R 9.497, F -11.516)]  [G loss: 12.023] \n",
      "930 [D loss: (-1.855)(R 8.822, F -12.532)]  [G loss: 12.472] \n",
      "931 [D loss: (-0.930)(R 10.081, F -11.942)]  [G loss: 12.787] \n",
      "931 [D loss: (-1.852)(R 9.815, F -13.519)]  [G loss: 13.549] \n",
      "932 [D loss: (-1.268)(R 11.311, F -13.847)]  [G loss: 14.019] \n",
      "932 [D loss: (-1.347)(R 11.897, F -14.590)]  [G loss: 14.235] \n",
      "933 [D loss: (-2.008)(R 11.507, F -15.523)]  [G loss: 15.192] \n",
      "933 [D loss: (-1.626)(R 12.864, F -16.116)]  [G loss: 16.111] \n",
      "934 [D loss: (-1.846)(R 12.311, F -16.004)]  [G loss: 15.994] \n",
      "934 [D loss: (-1.912)(R 13.054, F -16.878)]  [G loss: 16.986] \n",
      "935 [D loss: (-1.791)(R 15.013, F -18.594)]  [G loss: 17.605] \n",
      "935 [D loss: (-1.460)(R 15.098, F -18.019)]  [G loss: 18.642] \n",
      "936 [D loss: (-1.280)(R 16.427, F -18.988)]  [G loss: 19.015] \n",
      "936 [D loss: (-2.237)(R 15.761, F -20.235)]  [G loss: 18.757] \n",
      "937 [D loss: (-2.274)(R 16.139, F -20.688)]  [G loss: 20.480] \n",
      "937 [D loss: (-1.698)(R 17.335, F -20.731)]  [G loss: 21.524] \n",
      "938 [D loss: (-2.064)(R 17.061, F -21.190)]  [G loss: 22.075] \n",
      "938 [D loss: (-1.639)(R 19.112, F -22.390)]  [G loss: 22.348] \n",
      "939 [D loss: (-2.239)(R 18.708, F -23.186)]  [G loss: 23.089] \n",
      "939 [D loss: (-2.144)(R 19.617, F -23.904)]  [G loss: 23.937] \n",
      "940 [D loss: (-1.902)(R 21.215, F -25.019)]  [G loss: 25.419] \n",
      "940 [D loss: (-2.286)(R 21.712, F -26.285)]  [G loss: 26.149] \n",
      "941 [D loss: (-1.991)(R 23.225, F -27.206)]  [G loss: 26.754] \n",
      "941 [D loss: (-1.835)(R 23.726, F -27.396)]  [G loss: 28.816] \n",
      "942 [D loss: (-2.480)(R 23.470, F -28.429)]  [G loss: 28.844] \n",
      "942 [D loss: (-1.891)(R 26.111, F -29.893)]  [G loss: 30.689] \n",
      "943 [D loss: (-1.822)(R 25.786, F -29.430)]  [G loss: 30.287] \n",
      "943 [D loss: (-2.007)(R 26.099, F -30.112)]  [G loss: 32.111] \n",
      "944 [D loss: (-2.186)(R 28.083, F -32.455)]  [G loss: 32.149] \n",
      "944 [D loss: (-2.495)(R 27.665, F -32.656)]  [G loss: 33.864] \n",
      "945 [D loss: (-1.176)(R 29.257, F -31.609)]  [G loss: 34.942] \n",
      "945 [D loss: (-3.021)(R 29.757, F -35.800)]  [G loss: 36.133] \n",
      "946 [D loss: (-3.252)(R 31.027, F -37.531)]  [G loss: 38.209] \n",
      "946 [D loss: (-3.191)(R 31.555, F -37.937)]  [G loss: 37.686] \n",
      "947 [D loss: (-3.049)(R 31.624, F -37.722)]  [G loss: 38.593] \n",
      "947 [D loss: (-4.514)(R 33.783, F -42.810)]  [G loss: 40.152] \n",
      "948 [D loss: (-2.793)(R 34.284, F -39.870)]  [G loss: 41.482] \n",
      "948 [D loss: (-4.179)(R 35.701, F -44.058)]  [G loss: 41.919] \n",
      "949 [D loss: (-3.359)(R 36.959, F -43.677)]  [G loss: 43.513] \n",
      "949 [D loss: (-2.281)(R 39.452, F -44.014)]  [G loss: 45.471] \n",
      "950 [D loss: (-2.687)(R 41.212, F -46.586)]  [G loss: 46.381] \n",
      "950 [D loss: (-3.703)(R 40.084, F -47.491)]  [G loss: 47.930] \n",
      "951 [D loss: (-3.631)(R 41.494, F -48.756)]  [G loss: 50.725] \n",
      "951 [D loss: (-2.710)(R 42.478, F -47.898)]  [G loss: 51.713] \n",
      "952 [D loss: (-3.329)(R 43.622, F -50.280)]  [G loss: 52.151] \n",
      "952 [D loss: (-4.262)(R 45.609, F -54.134)]  [G loss: 53.195] \n",
      "953 [D loss: (-1.647)(R 49.264, F -52.558)]  [G loss: 55.083] \n",
      "953 [D loss: (-4.353)(R 46.828, F -55.534)]  [G loss: 57.192] \n",
      "954 [D loss: (-3.604)(R 50.842, F -58.050)]  [G loss: 59.980] \n",
      "954 [D loss: (-2.437)(R 53.045, F -57.919)]  [G loss: 59.803] \n",
      "955 [D loss: (-1.935)(R 54.510, F -58.380)]  [G loss: 61.245] \n",
      "955 [D loss: (-3.869)(R 54.200, F -61.938)]  [G loss: 62.518] \n",
      "956 [D loss: (-2.614)(R 57.650, F -62.878)]  [G loss: 64.158] \n",
      "956 [D loss: (-3.579)(R 57.912, F -65.070)]  [G loss: 65.113] \n",
      "957 [D loss: (-4.185)(R 59.286, F -67.655)]  [G loss: 68.579] \n",
      "957 [D loss: (-6.326)(R 58.158, F -70.810)]  [G loss: 71.392] \n",
      "958 [D loss: (-5.274)(R 64.171, F -74.720)]  [G loss: 72.500] \n",
      "958 [D loss: (-4.795)(R 65.842, F -75.433)]  [G loss: 74.728] \n",
      "959 [D loss: (-3.741)(R 67.475, F -74.958)]  [G loss: 77.975] \n",
      "959 [D loss: (-3.680)(R 70.806, F -78.166)]  [G loss: 77.594] \n",
      "960 [D loss: (-4.058)(R 72.390, F -80.506)]  [G loss: 79.196] \n",
      "960 [D loss: (-4.501)(R 74.679, F -83.681)]  [G loss: 83.878] \n",
      "961 [D loss: (-6.612)(R 74.026, F -87.250)]  [G loss: 86.481] \n",
      "961 [D loss: (-4.944)(R 78.851, F -88.740)]  [G loss: 88.362] \n",
      "962 [D loss: (-6.934)(R 79.699, F -93.567)]  [G loss: 90.838] \n",
      "962 [D loss: (-5.638)(R 82.286, F -93.562)]  [G loss: 93.439] \n",
      "963 [D loss: (-6.592)(R 82.577, F -95.761)]  [G loss: 94.981] \n",
      "963 [D loss: (-3.695)(R 87.042, F -94.431)]  [G loss: 99.048] \n",
      "964 [D loss: (-7.340)(R 84.458, F -99.138)]  [G loss: 99.603] \n",
      "964 [D loss: (-4.513)(R 94.010, F -103.036)]  [G loss: 105.020] \n",
      "965 [D loss: (-5.368)(R 94.415, F -105.152)]  [G loss: 107.224] \n",
      "965 [D loss: (-3.682)(R 98.415, F -105.778)]  [G loss: 109.386] \n",
      "966 [D loss: (-6.269)(R 103.915, F -116.454)]  [G loss: 112.565] \n",
      "966 [D loss: (-6.708)(R 100.407, F -113.824)]  [G loss: 114.595] \n",
      "967 [D loss: (-6.646)(R 103.597, F -116.890)]  [G loss: 117.413] \n",
      "967 [D loss: (-4.297)(R 108.021, F -116.615)]  [G loss: 121.233] \n",
      "968 [D loss: (-6.216)(R 109.924, F -122.355)]  [G loss: 124.064] \n",
      "968 [D loss: (-1.904)(R 118.420, F -122.227)]  [G loss: 124.795] \n",
      "969 [D loss: (-3.265)(R 114.738, F -121.268)]  [G loss: 127.482] \n",
      "969 [D loss: (-3.275)(R 117.601, F -124.151)]  [G loss: 129.237] \n",
      "970 [D loss: (-4.616)(R 120.685, F -129.917)]  [G loss: 131.908] \n",
      "970 [D loss: (-6.815)(R 120.640, F -134.269)]  [G loss: 133.751] \n",
      "971 [D loss: (-4.518)(R 123.455, F -132.491)]  [G loss: 135.452] \n",
      "971 [D loss: (-5.340)(R 122.705, F -133.386)]  [G loss: 137.061] \n",
      "972 [D loss: (-6.143)(R 124.102, F -136.388)]  [G loss: 138.113] \n",
      "972 [D loss: (-5.095)(R 122.110, F -132.300)]  [G loss: 140.207] \n",
      "973 [D loss: (-10.964)(R 120.419, F -142.347)]  [G loss: 140.670] \n",
      "973 [D loss: (-2.860)(R 131.167, F -136.887)]  [G loss: 143.561] \n",
      "974 [D loss: (-2.451)(R 133.632, F -138.533)]  [G loss: 143.942] \n",
      "974 [D loss: (-4.417)(R 134.509, F -143.343)]  [G loss: 143.336] \n",
      "975 [D loss: (-3.659)(R 136.788, F -144.106)]  [G loss: 146.200] \n",
      "975 [D loss: (-4.131)(R 136.229, F -144.491)]  [G loss: 145.934] \n",
      "976 [D loss: (-5.054)(R 135.605, F -145.712)]  [G loss: 146.035] \n",
      "976 [D loss: (-6.432)(R 136.679, F -149.544)]  [G loss: 152.047] \n",
      "977 [D loss: (-5.088)(R 138.463, F -148.639)]  [G loss: 151.758] \n",
      "977 [D loss: (-6.288)(R 137.433, F -150.010)]  [G loss: 152.285] \n",
      "978 [D loss: (-7.700)(R 138.266, F -153.667)]  [G loss: 153.717] \n",
      "978 [D loss: (-4.493)(R 142.275, F -151.261)]  [G loss: 156.017] \n",
      "979 [D loss: (-7.489)(R 141.400, F -156.378)]  [G loss: 157.331] \n",
      "979 [D loss: (-5.852)(R 148.225, F -159.929)]  [G loss: 160.011] \n",
      "980 [D loss: (-7.008)(R 145.390, F -159.406)]  [G loss: 163.030] \n",
      "980 [D loss: (-4.995)(R 150.997, F -160.987)]  [G loss: 162.427] \n",
      "981 [D loss: (-8.209)(R 152.191, F -168.609)]  [G loss: 162.632] \n",
      "981 [D loss: (-6.018)(R 151.979, F -164.015)]  [G loss: 165.014] \n",
      "982 [D loss: (-8.419)(R 148.388, F -165.227)]  [G loss: 166.034] \n",
      "982 [D loss: (-4.390)(R 153.686, F -162.467)]  [G loss: 166.235] \n",
      "983 [D loss: (-7.122)(R 149.979, F -164.224)]  [G loss: 166.144] \n",
      "983 [D loss: (-9.705)(R 153.212, F -172.621)]  [G loss: 167.356] \n",
      "984 [D loss: (-6.912)(R 154.808, F -168.632)]  [G loss: 170.858] \n",
      "984 [D loss: (-5.442)(R 157.182, F -168.065)]  [G loss: 171.271] \n",
      "985 [D loss: (-5.904)(R 158.403, F -170.212)]  [G loss: 171.689] \n",
      "985 [D loss: (-4.516)(R 160.492, F -169.524)]  [G loss: 173.087] \n",
      "986 [D loss: (-3.481)(R 157.405, F -164.367)]  [G loss: 175.009] \n",
      "986 [D loss: (-7.851)(R 160.240, F -175.942)]  [G loss: 174.171] \n",
      "987 [D loss: (-8.835)(R 157.402, F -175.071)]  [G loss: 179.730] \n",
      "987 [D loss: (-6.225)(R 160.221, F -172.671)]  [G loss: 175.056] \n",
      "988 [D loss: (-5.552)(R 161.150, F -172.253)]  [G loss: 178.964] \n",
      "988 [D loss: (-2.836)(R 162.796, F -168.467)]  [G loss: 178.917] \n",
      "989 [D loss: (-9.344)(R 158.303, F -176.991)]  [G loss: 179.784] \n",
      "989 [D loss: (-6.076)(R 160.207, F -172.360)]  [G loss: 180.461] \n",
      "990 [D loss: (-10.794)(R 164.653, F -186.241)]  [G loss: 181.318] \n",
      "990 [D loss: (-8.410)(R 166.561, F -183.381)]  [G loss: 179.890] \n",
      "991 [D loss: (-11.212)(R 159.331, F -181.756)]  [G loss: 183.351] \n",
      "991 [D loss: (-5.002)(R 167.084, F -177.088)]  [G loss: 184.325] \n",
      "992 [D loss: (-12.536)(R 161.808, F -186.880)]  [G loss: 182.843] \n",
      "992 [D loss: (-6.270)(R 164.164, F -176.704)]  [G loss: 182.852] \n",
      "993 [D loss: (-6.222)(R 167.631, F -180.075)]  [G loss: 186.721] \n",
      "993 [D loss: (-13.027)(R 164.786, F -190.839)]  [G loss: 185.775] \n",
      "994 [D loss: (-10.314)(R 165.266, F -185.893)]  [G loss: 184.838] \n",
      "994 [D loss: (-9.692)(R 167.528, F -186.913)]  [G loss: 184.361] \n",
      "995 [D loss: (-12.933)(R 161.694, F -187.561)]  [G loss: 187.292] \n",
      "995 [D loss: (-10.738)(R 167.854, F -189.330)]  [G loss: 189.740] \n",
      "996 [D loss: (-9.981)(R 167.421, F -187.384)]  [G loss: 189.832] \n",
      "996 [D loss: (-9.668)(R 171.603, F -190.939)]  [G loss: 186.238] \n",
      "997 [D loss: (-8.410)(R 167.257, F -184.077)]  [G loss: 187.507] \n",
      "997 [D loss: (-8.185)(R 168.161, F -184.531)]  [G loss: 190.525] \n",
      "998 [D loss: (-7.975)(R 170.788, F -186.737)]  [G loss: 190.262] \n",
      "998 [D loss: (-7.531)(R 166.560, F -181.622)]  [G loss: 193.333] \n",
      "999 [D loss: (-9.128)(R 162.379, F -180.635)]  [G loss: 189.977] \n",
      "999 [D loss: (-5.630)(R 169.190, F -180.449)]  [G loss: 190.720] \n",
      "1000 [D loss: (-4.107)(R 170.010, F -178.223)]  [G loss: 190.948] \n",
      "1000 [D loss: (-8.493)(R 164.974, F -181.959)]  [G loss: 184.251] \n",
      "1001 [D loss: (-8.877)(R 167.738, F -185.491)]  [G loss: 185.637] \n",
      "1001 [D loss: (-7.654)(R 162.250, F -177.557)]  [G loss: 179.786] \n",
      "1002 [D loss: (-9.884)(R 162.124, F -181.892)]  [G loss: 183.106] \n",
      "1002 [D loss: (-6.136)(R 161.646, F -173.918)]  [G loss: 178.708] \n",
      "1003 [D loss: (-4.825)(R 164.236, F -173.886)]  [G loss: 177.340] \n",
      "1003 [D loss: (-11.976)(R 153.786, F -177.738)]  [G loss: 176.290] \n",
      "1004 [D loss: (-9.286)(R 157.037, F -175.609)]  [G loss: 174.675] \n",
      "1004 [D loss: (-9.822)(R 154.115, F -173.758)]  [G loss: 177.351] \n",
      "1005 [D loss: (-11.957)(R 153.931, F -177.846)]  [G loss: 169.885] \n",
      "1005 [D loss: (-7.384)(R 147.986, F -162.755)]  [G loss: 167.422] \n",
      "1006 [D loss: (-8.375)(R 146.573, F -163.323)]  [G loss: 167.633] \n",
      "1006 [D loss: (-7.618)(R 148.371, F -163.607)]  [G loss: 166.141] \n",
      "1007 [D loss: (-10.464)(R 143.544, F -164.472)]  [G loss: 163.132] \n",
      "1007 [D loss: (-11.553)(R 138.149, F -161.255)]  [G loss: 164.199] \n",
      "1008 [D loss: (-8.609)(R 139.073, F -156.292)]  [G loss: 160.710] \n",
      "1008 [D loss: (-11.103)(R 134.029, F -156.235)]  [G loss: 159.858] \n",
      "1009 [D loss: (-8.705)(R 135.133, F -152.543)]  [G loss: 149.574] \n",
      "1009 [D loss: (-9.095)(R 134.359, F -152.550)]  [G loss: 156.542] \n",
      "1010 [D loss: (-10.417)(R 129.329, F -150.163)]  [G loss: 144.935] \n",
      "1010 [D loss: (-7.936)(R 124.998, F -140.870)]  [G loss: 151.214] \n",
      "1011 [D loss: (-11.473)(R 122.248, F -145.194)]  [G loss: 147.807] \n",
      "1011 [D loss: (-10.081)(R 123.248, F -143.410)]  [G loss: 139.129] \n",
      "1012 [D loss: (-6.787)(R 118.921, F -132.494)]  [G loss: 140.906] \n",
      "1012 [D loss: (-9.007)(R 115.464, F -133.479)]  [G loss: 134.726] \n",
      "1013 [D loss: (-4.344)(R 117.219, F -125.906)]  [G loss: 131.337] \n",
      "1013 [D loss: (-6.554)(R 114.944, F -128.052)]  [G loss: 130.593] \n",
      "1014 [D loss: (-6.682)(R 114.069, F -127.433)]  [G loss: 125.386] \n",
      "1014 [D loss: (-3.636)(R 107.985, F -115.256)]  [G loss: 126.665] \n",
      "1015 [D loss: (-12.094)(R 103.033, F -127.222)]  [G loss: 122.298] \n",
      "1015 [D loss: (-7.513)(R 100.369, F -115.395)]  [G loss: 120.549] \n",
      "1016 [D loss: (-10.573)(R 101.048, F -122.194)]  [G loss: 120.307] \n",
      "1016 [D loss: (-8.025)(R 95.153, F -111.203)]  [G loss: 109.078] \n",
      "1017 [D loss: (-7.304)(R 93.861, F -108.469)]  [G loss: 92.731] \n",
      "1017 [D loss: (-6.863)(R 90.308, F -104.034)]  [G loss: 100.665] \n",
      "1018 [D loss: (-6.854)(R 87.899, F -101.608)]  [G loss: 97.046] \n",
      "1018 [D loss: (-3.494)(R 84.817, F -91.804)]  [G loss: 105.828] \n",
      "1019 [D loss: (-8.212)(R 78.886, F -95.309)]  [G loss: 98.690] \n",
      "1019 [D loss: (-5.114)(R 78.733, F -88.960)]  [G loss: 95.620] \n",
      "1020 [D loss: (-4.991)(R 73.548, F -83.530)]  [G loss: 88.958] \n",
      "1020 [D loss: (-9.188)(R 71.856, F -90.231)]  [G loss: 84.328] \n",
      "1021 [D loss: (0.133)(R 73.275, F -73.008)]  [G loss: 93.419] \n",
      "1021 [D loss: (-3.017)(R 69.129, F -75.163)]  [G loss: 83.577] \n",
      "1022 [D loss: (-3.082)(R 65.117, F -71.281)]  [G loss: 83.391] \n",
      "1022 [D loss: (-4.015)(R 64.979, F -73.010)]  [G loss: 78.196] \n",
      "1023 [D loss: (-6.902)(R 62.727, F -76.531)]  [G loss: 78.778] \n",
      "1023 [D loss: (-6.965)(R 55.761, F -69.691)]  [G loss: 73.546] \n",
      "1024 [D loss: (-6.765)(R 57.124, F -70.655)]  [G loss: 68.271] \n",
      "1024 [D loss: (-4.482)(R 54.409, F -63.372)]  [G loss: 73.533] \n",
      "1025 [D loss: (-8.128)(R 49.838, F -66.094)]  [G loss: 70.464] \n",
      "1025 [D loss: (-6.945)(R 51.500, F -65.390)]  [G loss: 56.750] \n",
      "1026 [D loss: (-8.638)(R 44.099, F -61.375)]  [G loss: 62.443] \n",
      "1026 [D loss: (-2.060)(R 47.990, F -52.110)]  [G loss: 50.440] \n",
      "1027 [D loss: (-14.101)(R 43.329, F -71.530)]  [G loss: 53.412] \n",
      "1027 [D loss: (-7.683)(R 38.118, F -53.484)]  [G loss: 50.066] \n",
      "1028 [D loss: (-9.215)(R 32.301, F -50.731)]  [G loss: 47.662] \n",
      "1028 [D loss: (-10.410)(R 33.515, F -54.336)]  [G loss: 48.805] \n",
      "1029 [D loss: (-9.552)(R 25.237, F -44.340)]  [G loss: 43.305] \n",
      "1029 [D loss: (-4.927)(R 27.328, F -37.182)]  [G loss: 43.189] \n",
      "1030 [D loss: (-6.886)(R 25.312, F -39.083)]  [G loss: 44.381] \n",
      "1030 [D loss: (-12.318)(R 18.076, F -42.712)]  [G loss: 37.344] \n",
      "1031 [D loss: (-11.321)(R 14.973, F -37.614)]  [G loss: 40.940] \n",
      "1031 [D loss: (-5.357)(R 11.276, F -21.991)]  [G loss: 28.315] \n",
      "1032 [D loss: (-6.951)(R 15.794, F -29.697)]  [G loss: 30.836] \n",
      "1032 [D loss: (-7.858)(R 10.238, F -25.954)]  [G loss: 32.658] \n",
      "1033 [D loss: (-11.016)(R 10.170, F -32.202)]  [G loss: 26.136] \n",
      "1033 [D loss: (-8.027)(R 8.054, F -24.107)]  [G loss: 21.160] \n",
      "1034 [D loss: (-13.204)(R 2.759, F -29.167)]  [G loss: 17.454] \n",
      "1034 [D loss: (-9.856)(R 1.050, F -20.762)]  [G loss: 22.014] \n",
      "1035 [D loss: (-14.138)(R 1.980, F -30.257)]  [G loss: 15.017] \n",
      "1035 [D loss: (-12.723)(R -5.806, F -19.640)]  [G loss: 16.532] \n",
      "1036 [D loss: (-8.243)(R -2.869, F -13.617)]  [G loss: 10.139] \n",
      "1036 [D loss: (-11.664)(R -4.609, F -18.718)]  [G loss: 16.310] \n",
      "1037 [D loss: (-9.836)(R -9.685, F -9.987)]  [G loss: -0.211] \n",
      "1037 [D loss: (-6.655)(R -7.499, F -5.811)]  [G loss: 0.464] \n",
      "1038 [D loss: (-6.858)(R -11.909, F -1.807)]  [G loss: 3.672] \n",
      "1038 [D loss: (-13.168)(R -24.676, F -1.660)]  [G loss: 1.310] \n",
      "1039 [D loss: (-7.156)(R -14.177, F -0.135)]  [G loss: 2.644] \n",
      "1039 [D loss: (-10.070)(R -21.144, F 1.003)]  [G loss: 7.787] \n",
      "1040 [D loss: (-8.847)(R -19.317, F 1.624)]  [G loss: -1.128] \n",
      "1040 [D loss: (-9.041)(R -26.094, F 8.012)]  [G loss: 0.739] \n",
      "1041 [D loss: (-14.527)(R -31.887, F 2.834)]  [G loss: 1.213] \n",
      "1041 [D loss: (-12.029)(R -27.117, F 3.060)]  [G loss: -10.266] \n",
      "1042 [D loss: (-11.492)(R -29.183, F 6.199)]  [G loss: -6.723] \n",
      "1042 [D loss: (-15.094)(R -30.037, F -0.150)]  [G loss: -6.306] \n",
      "1043 [D loss: (-9.555)(R -34.393, F 15.284)]  [G loss: -6.409] \n",
      "1043 [D loss: (-5.656)(R -28.113, F 16.802)]  [G loss: -15.812] \n",
      "1044 [D loss: (-14.474)(R -37.808, F 8.860)]  [G loss: -10.485] \n",
      "1044 [D loss: (-11.745)(R -38.816, F 15.326)]  [G loss: -15.749] \n",
      "1045 [D loss: (-12.955)(R -38.512, F 12.601)]  [G loss: -8.751] \n",
      "1045 [D loss: (-6.531)(R -38.142, F 25.079)]  [G loss: -21.051] \n",
      "1046 [D loss: (-10.522)(R -36.629, F 15.585)]  [G loss: -15.476] \n",
      "1046 [D loss: (-3.238)(R -34.156, F 27.680)]  [G loss: -20.611] \n",
      "1047 [D loss: (-9.490)(R -39.649, F 20.670)]  [G loss: -21.469] \n",
      "1047 [D loss: (-10.454)(R -33.600, F 12.693)]  [G loss: -21.737] \n",
      "1048 [D loss: (-8.859)(R -38.328, F 20.611)]  [G loss: -16.059] \n",
      "1048 [D loss: (-13.127)(R -39.321, F 13.067)]  [G loss: -12.070] \n",
      "1049 [D loss: (-2.285)(R -34.952, F 30.382)]  [G loss: -19.632] \n",
      "1049 [D loss: (-16.969)(R -40.532, F 6.595)]  [G loss: -20.657] \n",
      "1050 [D loss: (-4.612)(R -34.102, F 24.878)]  [G loss: -29.185] \n",
      "1050 [D loss: (-3.549)(R -35.010, F 27.912)]  [G loss: -20.722] \n",
      "1051 [D loss: (-9.880)(R -35.301, F 15.542)]  [G loss: -17.802] \n",
      "1051 [D loss: (-7.644)(R -32.147, F 16.858)]  [G loss: -16.732] \n",
      "1052 [D loss: (-3.469)(R -30.501, F 23.563)]  [G loss: -18.564] \n",
      "1052 [D loss: (-7.798)(R -36.107, F 20.512)]  [G loss: -20.493] \n",
      "1053 [D loss: (0.582)(R -29.515, F 30.678)]  [G loss: -16.771] \n",
      "1053 [D loss: (-5.880)(R -33.320, F 21.560)]  [G loss: -21.979] \n",
      "1054 [D loss: (-2.022)(R -29.322, F 25.278)]  [G loss: -17.429] \n",
      "1054 [D loss: (-6.651)(R -34.805, F 21.503)]  [G loss: -15.220] \n",
      "1055 [D loss: (2.435)(R -26.102, F 30.973)]  [G loss: -29.865] \n",
      "1055 [D loss: (1.106)(R -32.686, F 34.897)]  [G loss: -28.684] \n",
      "1056 [D loss: (-1.389)(R -35.780, F 33.002)]  [G loss: -30.159] \n",
      "1056 [D loss: (-5.302)(R -34.196, F 23.592)]  [G loss: -28.633] \n",
      "1057 [D loss: (-4.329)(R -36.762, F 28.105)]  [G loss: -20.544] \n",
      "1057 [D loss: (-4.634)(R -37.429, F 28.161)]  [G loss: -27.368] \n",
      "1058 [D loss: (-3.981)(R -35.192, F 27.230)]  [G loss: -21.624] \n",
      "1058 [D loss: (-1.051)(R -33.694, F 31.592)]  [G loss: -23.908] \n",
      "1059 [D loss: (-6.580)(R -40.402, F 27.243)]  [G loss: -32.236] \n",
      "1059 [D loss: (6.112)(R -29.528, F 41.752)]  [G loss: -26.550] \n",
      "1060 [D loss: (-7.135)(R -33.341, F 19.071)]  [G loss: -29.673] \n",
      "1060 [D loss: (-1.896)(R -32.849, F 29.057)]  [G loss: -27.307] \n",
      "1061 [D loss: (-3.141)(R -31.587, F 25.305)]  [G loss: -25.987] \n",
      "1061 [D loss: (-2.157)(R -31.161, F 26.848)]  [G loss: -20.754] \n",
      "1062 [D loss: (-3.405)(R -28.889, F 22.079)]  [G loss: -31.169] \n",
      "1062 [D loss: (-9.665)(R -35.707, F 16.377)]  [G loss: -34.378] \n",
      "1063 [D loss: (-5.230)(R -29.523, F 19.063)]  [G loss: -27.910] \n",
      "1063 [D loss: (-1.329)(R -30.241, F 27.582)]  [G loss: -18.608] \n",
      "1064 [D loss: (-3.594)(R -30.637, F 23.448)]  [G loss: -12.757] \n",
      "1064 [D loss: (-2.563)(R -27.067, F 21.940)]  [G loss: -13.800] \n",
      "1065 [D loss: (-1.346)(R -32.182, F 29.490)]  [G loss: -22.986] \n",
      "1065 [D loss: (-2.656)(R -24.916, F 19.605)]  [G loss: -21.593] \n",
      "1066 [D loss: (-5.536)(R -30.582, F 19.510)]  [G loss: -17.954] \n",
      "1066 [D loss: (3.387)(R -27.034, F 33.809)]  [G loss: -27.118] \n",
      "1067 [D loss: (-6.349)(R -28.787, F 16.089)]  [G loss: -25.742] \n",
      "1067 [D loss: (-4.051)(R -28.078, F 19.975)]  [G loss: -18.132] \n",
      "1068 [D loss: (-11.558)(R -29.862, F 6.746)]  [G loss: -15.468] \n",
      "1068 [D loss: (-5.081)(R -31.048, F 20.885)]  [G loss: -12.280] \n",
      "1069 [D loss: (-3.824)(R -27.278, F 19.631)]  [G loss: -18.306] \n",
      "1069 [D loss: (-3.724)(R -28.557, F 21.109)]  [G loss: -16.567] \n",
      "1070 [D loss: (-4.543)(R -32.888, F 23.801)]  [G loss: -17.964] \n",
      "1070 [D loss: (-7.508)(R -30.104, F 15.087)]  [G loss: -14.265] \n",
      "1071 [D loss: (-5.702)(R -26.343, F 14.939)]  [G loss: -18.651] \n",
      "1071 [D loss: (-6.689)(R -31.441, F 18.063)]  [G loss: -9.268] \n",
      "1072 [D loss: (-6.970)(R -33.870, F 19.929)]  [G loss: -15.250] \n",
      "1072 [D loss: (-11.463)(R -31.376, F 8.451)]  [G loss: -16.574] \n",
      "1073 [D loss: (-7.649)(R -28.295, F 12.996)]  [G loss: -13.661] \n",
      "1073 [D loss: (-6.601)(R -24.838, F 11.636)]  [G loss: -13.469] \n",
      "1074 [D loss: (-11.173)(R -28.103, F 5.756)]  [G loss: -10.149] \n",
      "1074 [D loss: (-7.112)(R -29.646, F 15.423)]  [G loss: -12.847] \n",
      "1075 [D loss: (-8.590)(R -27.087, F 9.906)]  [G loss: -8.984] \n",
      "1075 [D loss: (-9.905)(R -28.705, F 8.895)]  [G loss: -11.324] \n",
      "1076 [D loss: (-8.215)(R -28.488, F 12.059)]  [G loss: -12.188] \n",
      "1076 [D loss: (-11.626)(R -31.383, F 8.132)]  [G loss: -10.136] \n",
      "1077 [D loss: (-12.056)(R -32.812, F 8.700)]  [G loss: -9.388] \n",
      "1077 [D loss: (-10.502)(R -29.176, F 8.172)]  [G loss: -3.719] \n",
      "1078 [D loss: (-13.819)(R -31.290, F 3.651)]  [G loss: -2.586] \n",
      "1078 [D loss: (-13.617)(R -30.830, F 3.597)]  [G loss: -2.643] \n",
      "1079 [D loss: (-12.125)(R -30.911, F 6.661)]  [G loss: -8.203] \n",
      "1079 [D loss: (-16.035)(R -31.531, F -0.540)]  [G loss: 1.680] \n",
      "1080 [D loss: (-14.161)(R -30.669, F 2.347)]  [G loss: -0.697] \n",
      "1080 [D loss: (-14.512)(R -29.917, F 0.893)]  [G loss: -0.212] \n",
      "1081 [D loss: (-15.323)(R -30.694, F 0.049)]  [G loss: 0.613] \n",
      "1081 [D loss: (-14.818)(R -28.863, F -0.772)]  [G loss: 3.112] \n",
      "1082 [D loss: (-15.611)(R -29.005, F -2.217)]  [G loss: 1.611] \n",
      "1082 [D loss: (-16.246)(R -29.741, F -2.751)]  [G loss: 4.718] \n",
      "1083 [D loss: (-15.904)(R -31.608, F -0.201)]  [G loss: 4.257] \n",
      "1083 [D loss: (-16.958)(R -27.704, F -6.211)]  [G loss: 7.896] \n",
      "1084 [D loss: (-21.099)(R -34.689, F -7.510)]  [G loss: 8.692] \n",
      "1084 [D loss: (-21.378)(R -31.674, F -11.081)]  [G loss: 9.595] \n",
      "1085 [D loss: (-21.503)(R -31.068, F -11.938)]  [G loss: 5.172] \n",
      "1085 [D loss: (-17.062)(R -30.692, F -3.431)]  [G loss: 7.203] \n",
      "1086 [D loss: (-21.955)(R -34.579, F -9.331)]  [G loss: 10.273] \n",
      "1086 [D loss: (-21.039)(R -32.093, F -9.985)]  [G loss: 11.375] \n",
      "1087 [D loss: (-21.766)(R -37.074, F -6.457)]  [G loss: 9.964] \n",
      "1087 [D loss: (-24.317)(R -35.211, F -13.423)]  [G loss: 13.052] \n",
      "1088 [D loss: (-24.146)(R -35.511, F -12.782)]  [G loss: 11.148] \n",
      "1088 [D loss: (-24.749)(R -36.994, F -12.504)]  [G loss: 5.406] \n",
      "1089 [D loss: (-27.517)(R -39.815, F -15.218)]  [G loss: 11.663] \n",
      "1089 [D loss: (-25.751)(R -38.821, F -12.680)]  [G loss: 13.875] \n",
      "1090 [D loss: (-26.730)(R -39.677, F -13.783)]  [G loss: 11.963] \n",
      "1090 [D loss: (-23.411)(R -31.469, F -15.352)]  [G loss: 11.677] \n",
      "1091 [D loss: (-29.659)(R -39.328, F -19.989)]  [G loss: 14.366] \n",
      "1091 [D loss: (-26.986)(R -37.045, F -16.927)]  [G loss: 16.125] \n",
      "1092 [D loss: (-29.163)(R -39.830, F -18.496)]  [G loss: 12.098] \n",
      "1092 [D loss: (-26.361)(R -41.624, F -11.098)]  [G loss: 12.585] \n",
      "1093 [D loss: (-27.371)(R -39.816, F -14.927)]  [G loss: 10.190] \n",
      "1093 [D loss: (-27.200)(R -39.859, F -14.540)]  [G loss: 7.484] \n",
      "1094 [D loss: (-29.917)(R -43.817, F -16.017)]  [G loss: 10.915] \n",
      "1094 [D loss: (-25.958)(R -41.412, F -10.505)]  [G loss: 11.186] \n",
      "1095 [D loss: (-29.662)(R -46.618, F -12.706)]  [G loss: 18.595] \n",
      "1095 [D loss: (-26.295)(R -43.761, F -8.829)]  [G loss: 15.572] \n",
      "1096 [D loss: (-31.781)(R -45.525, F -18.038)]  [G loss: 14.548] \n",
      "1096 [D loss: (-28.240)(R -44.060, F -12.420)]  [G loss: 18.542] \n",
      "1097 [D loss: (-29.855)(R -50.375, F -9.334)]  [G loss: 18.561] \n",
      "1097 [D loss: (-27.523)(R -45.745, F -9.301)]  [G loss: 12.885] \n",
      "1098 [D loss: (-26.698)(R -43.195, F -10.201)]  [G loss: 14.975] \n",
      "1098 [D loss: (-26.793)(R -47.298, F -6.288)]  [G loss: 13.413] \n",
      "1099 [D loss: (-23.774)(R -42.435, F -5.114)]  [G loss: 8.488] \n",
      "1099 [D loss: (-24.678)(R -44.207, F -5.149)]  [G loss: 6.963] \n",
      "1100 [D loss: (-28.125)(R -46.725, F -9.526)]  [G loss: 10.092] \n",
      "1100 [D loss: (-30.709)(R -47.202, F -14.217)]  [G loss: 12.165] \n",
      "1101 [D loss: (-27.059)(R -45.035, F -9.083)]  [G loss: 16.531] \n",
      "1101 [D loss: (-29.283)(R -51.998, F -6.568)]  [G loss: 12.619] \n",
      "1102 [D loss: (-26.477)(R -45.264, F -7.691)]  [G loss: 7.060] \n",
      "1102 [D loss: (-25.582)(R -44.666, F -6.498)]  [G loss: 8.917] \n",
      "1103 [D loss: (-33.390)(R -50.945, F -15.834)]  [G loss: 10.901] \n",
      "1103 [D loss: (-28.170)(R -46.394, F -9.945)]  [G loss: 7.811] \n",
      "1104 [D loss: (-27.555)(R -46.008, F -9.102)]  [G loss: 5.983] \n",
      "1104 [D loss: (-26.712)(R -40.500, F -12.923)]  [G loss: 5.789] \n",
      "1105 [D loss: (-24.143)(R -45.441, F -2.844)]  [G loss: 10.828] \n",
      "1105 [D loss: (-27.572)(R -43.658, F -11.485)]  [G loss: 4.905] \n",
      "1106 [D loss: (-24.602)(R -51.009, F 1.805)]  [G loss: 3.509] \n",
      "1106 [D loss: (-25.934)(R -45.858, F -6.010)]  [G loss: 9.480] \n",
      "1107 [D loss: (-26.624)(R -44.965, F -8.283)]  [G loss: 6.590] \n",
      "1107 [D loss: (-25.469)(R -43.666, F -7.271)]  [G loss: 2.833] \n",
      "1108 [D loss: (-28.522)(R -45.445, F -11.598)]  [G loss: 12.335] \n",
      "1108 [D loss: (-22.150)(R -43.256, F -1.044)]  [G loss: 1.698] \n",
      "1109 [D loss: (-23.942)(R -49.512, F 1.628)]  [G loss: 2.751] \n",
      "1109 [D loss: (-23.929)(R -43.923, F -3.935)]  [G loss: 6.757] \n",
      "1110 [D loss: (-27.322)(R -49.656, F -4.989)]  [G loss: 2.854] \n",
      "1110 [D loss: (-20.394)(R -44.872, F 4.084)]  [G loss: -0.025] \n",
      "1111 [D loss: (-21.423)(R -50.266, F 7.419)]  [G loss: 2.365] \n",
      "1111 [D loss: (-23.367)(R -45.465, F -1.270)]  [G loss: -9.331] \n",
      "1112 [D loss: (-20.927)(R -43.518, F 1.665)]  [G loss: -2.772] \n",
      "1112 [D loss: (-20.548)(R -48.128, F 7.032)]  [G loss: -7.033] \n",
      "1113 [D loss: (-22.363)(R -50.847, F 6.121)]  [G loss: -3.498] \n",
      "1113 [D loss: (-15.754)(R -42.617, F 11.108)]  [G loss: -7.570] \n",
      "1114 [D loss: (-24.787)(R -45.049, F -4.525)]  [G loss: -2.361] \n",
      "1114 [D loss: (-18.375)(R -47.969, F 11.219)]  [G loss: -11.993] \n",
      "1115 [D loss: (-14.948)(R -42.406, F 12.511)]  [G loss: -6.337] \n",
      "1115 [D loss: (-18.598)(R -46.769, F 9.574)]  [G loss: -7.727] \n",
      "1116 [D loss: (-16.366)(R -49.256, F 16.525)]  [G loss: -8.651] \n",
      "1116 [D loss: (-17.842)(R -46.961, F 11.276)]  [G loss: -19.006] \n",
      "1117 [D loss: (-16.493)(R -48.146, F 15.160)]  [G loss: -16.771] \n",
      "1117 [D loss: (-13.845)(R -53.156, F 25.467)]  [G loss: -27.016] \n",
      "1118 [D loss: (-19.247)(R -54.056, F 15.562)]  [G loss: -22.191] \n",
      "1118 [D loss: (-23.360)(R -54.292, F 7.572)]  [G loss: -15.140] \n",
      "1119 [D loss: (-11.108)(R -55.859, F 33.644)]  [G loss: -22.302] \n",
      "1119 [D loss: (-14.926)(R -50.361, F 20.508)]  [G loss: -24.767] \n",
      "1120 [D loss: (-11.749)(R -48.133, F 24.634)]  [G loss: -26.353] \n",
      "1120 [D loss: (-14.546)(R -52.599, F 23.507)]  [G loss: -26.895] \n",
      "1121 [D loss: (-14.505)(R -56.375, F 27.365)]  [G loss: -24.101] \n",
      "1121 [D loss: (-14.965)(R -54.744, F 24.815)]  [G loss: -35.919] \n",
      "1122 [D loss: (-11.561)(R -51.720, F 28.598)]  [G loss: -29.997] \n",
      "1122 [D loss: (-8.929)(R -56.118, F 38.261)]  [G loss: -33.073] \n",
      "1123 [D loss: (-3.011)(R -49.752, F 43.729)]  [G loss: -41.814] \n",
      "1123 [D loss: (-6.019)(R -56.709, F 44.672)]  [G loss: -34.421] \n",
      "1124 [D loss: (-4.175)(R -49.081, F 40.731)]  [G loss: -40.204] \n",
      "1124 [D loss: (-9.755)(R -54.678, F 35.168)]  [G loss: -35.115] \n",
      "1125 [D loss: (-4.065)(R -51.496, F 43.367)]  [G loss: -44.169] \n",
      "1125 [D loss: (-6.370)(R -52.001, F 39.261)]  [G loss: -40.490] \n",
      "1126 [D loss: (-8.059)(R -51.101, F 34.982)]  [G loss: -48.744] \n",
      "1126 [D loss: (-9.880)(R -61.826, F 42.066)]  [G loss: -35.263] \n",
      "1127 [D loss: (-7.284)(R -56.592, F 42.024)]  [G loss: -39.824] \n",
      "1127 [D loss: (-8.053)(R -56.205, F 40.099)]  [G loss: -37.762] \n",
      "1128 [D loss: (-7.592)(R -60.087, F 44.902)]  [G loss: -50.042] \n",
      "1128 [D loss: (-8.126)(R -58.984, F 42.731)]  [G loss: -43.716] \n",
      "1129 [D loss: (-4.349)(R -56.380, F 47.682)]  [G loss: -51.328] \n",
      "1129 [D loss: (-8.017)(R -60.253, F 44.220)]  [G loss: -50.450] \n",
      "1130 [D loss: (1.597)(R -53.450, F 56.644)]  [G loss: -49.773] \n",
      "1130 [D loss: (-1.062)(R -52.361, F 50.237)]  [G loss: -49.390] \n",
      "1131 [D loss: (-1.599)(R -56.790, F 53.591)]  [G loss: -53.145] \n",
      "1131 [D loss: (-1.625)(R -60.338, F 57.089)]  [G loss: -53.977] \n",
      "1132 [D loss: (-0.334)(R -56.661, F 55.993)]  [G loss: -50.527] \n",
      "1132 [D loss: (-0.743)(R -59.992, F 58.505)]  [G loss: -59.665] \n",
      "1133 [D loss: (3.555)(R -53.745, F 60.855)]  [G loss: -56.323] \n",
      "1133 [D loss: (1.791)(R -61.248, F 64.830)]  [G loss: -54.184] \n",
      "1134 [D loss: (-0.118)(R -59.445, F 59.208)]  [G loss: -57.018] \n",
      "1134 [D loss: (1.962)(R -61.238, F 65.162)]  [G loss: -61.167] \n",
      "1135 [D loss: (1.289)(R -62.337, F 64.915)]  [G loss: -58.054] \n",
      "1135 [D loss: (2.332)(R -61.303, F 65.967)]  [G loss: -71.693] \n",
      "1136 [D loss: (1.559)(R -64.399, F 67.517)]  [G loss: -66.185] \n",
      "1136 [D loss: (2.640)(R -67.811, F 73.091)]  [G loss: -69.636] \n",
      "1137 [D loss: (1.310)(R -66.672, F 69.292)]  [G loss: -70.188] \n",
      "1137 [D loss: (4.098)(R -62.413, F 70.610)]  [G loss: -70.605] \n",
      "1138 [D loss: (0.672)(R -63.720, F 65.063)]  [G loss: -61.527] \n",
      "1138 [D loss: (2.068)(R -66.046, F 70.182)]  [G loss: -68.450] \n",
      "1139 [D loss: (2.782)(R -64.200, F 69.765)]  [G loss: -69.312] \n",
      "1139 [D loss: (3.896)(R -58.747, F 66.540)]  [G loss: -65.511] \n",
      "1140 [D loss: (2.366)(R -62.901, F 67.633)]  [G loss: -69.142] \n",
      "1140 [D loss: (1.568)(R -64.273, F 67.409)]  [G loss: -63.037] \n",
      "1141 [D loss: (1.523)(R -57.983, F 61.028)]  [G loss: -62.403] \n",
      "1141 [D loss: (3.543)(R -62.655, F 69.741)]  [G loss: -65.698] \n",
      "1142 [D loss: (1.189)(R -62.177, F 64.555)]  [G loss: -61.546] \n",
      "1142 [D loss: (3.631)(R -59.856, F 67.117)]  [G loss: -57.378] \n",
      "1143 [D loss: (3.550)(R -57.776, F 64.877)]  [G loss: -62.745] \n",
      "1143 [D loss: (1.855)(R -60.339, F 64.049)]  [G loss: -62.295] \n",
      "1144 [D loss: (-0.866)(R -59.600, F 57.869)]  [G loss: -61.723] \n",
      "1144 [D loss: (-1.602)(R -56.933, F 53.728)]  [G loss: -55.619] \n",
      "1145 [D loss: (1.657)(R -52.172, F 55.486)]  [G loss: -60.639] \n",
      "1145 [D loss: (3.390)(R -55.441, F 62.221)]  [G loss: -60.418] \n",
      "1146 [D loss: (2.452)(R -54.409, F 59.313)]  [G loss: -54.353] \n",
      "1146 [D loss: (3.195)(R -50.234, F 56.623)]  [G loss: -55.092] \n",
      "1147 [D loss: (2.886)(R -54.759, F 60.531)]  [G loss: -55.410] \n",
      "1147 [D loss: (1.827)(R -52.690, F 56.345)]  [G loss: -55.951] \n",
      "1148 [D loss: (4.528)(R -48.891, F 57.947)]  [G loss: -55.577] \n",
      "1148 [D loss: (2.543)(R -48.820, F 53.906)]  [G loss: -48.474] \n",
      "1149 [D loss: (1.408)(R -47.671, F 50.487)]  [G loss: -52.226] \n",
      "1149 [D loss: (1.076)(R -46.473, F 48.626)]  [G loss: -49.162] \n",
      "1150 [D loss: (3.402)(R -43.967, F 50.772)]  [G loss: -47.624] \n",
      "1150 [D loss: (-1.289)(R -45.563, F 42.985)]  [G loss: -43.417] \n",
      "1151 [D loss: (0.913)(R -43.713, F 45.539)]  [G loss: -43.706] \n",
      "1151 [D loss: (-0.200)(R -44.438, F 44.037)]  [G loss: -45.713] \n",
      "1152 [D loss: (2.175)(R -43.870, F 48.219)]  [G loss: -44.409] \n",
      "1152 [D loss: (-1.424)(R -40.952, F 38.104)]  [G loss: -42.194] \n",
      "1153 [D loss: (-1.573)(R -43.521, F 40.375)]  [G loss: -41.983] \n",
      "1153 [D loss: (0.388)(R -41.598, F 42.373)]  [G loss: -40.593] \n",
      "1154 [D loss: (-0.615)(R -43.297, F 42.068)]  [G loss: -38.851] \n",
      "1154 [D loss: (1.853)(R -39.739, F 43.445)]  [G loss: -39.696] \n",
      "1155 [D loss: (-1.218)(R -40.339, F 37.903)]  [G loss: -39.461] \n",
      "1155 [D loss: (0.623)(R -38.494, F 39.740)]  [G loss: -34.257] \n",
      "1156 [D loss: (-4.010)(R -39.412, F 31.393)]  [G loss: -35.796] \n",
      "1156 [D loss: (-1.155)(R -35.680, F 33.371)]  [G loss: -35.001] \n",
      "1157 [D loss: (0.545)(R -34.179, F 35.269)]  [G loss: -31.868] \n",
      "1157 [D loss: (-0.737)(R -34.831, F 33.357)]  [G loss: -30.238] \n",
      "1158 [D loss: (-1.523)(R -33.133, F 30.087)]  [G loss: -30.457] \n",
      "1158 [D loss: (-1.079)(R -31.201, F 29.043)]  [G loss: -28.099] \n",
      "1159 [D loss: (-1.571)(R -31.137, F 27.995)]  [G loss: -24.336] \n",
      "1159 [D loss: (-8.326)(R -34.416, F 17.764)]  [G loss: -23.987] \n",
      "1160 [D loss: (-4.570)(R -31.621, F 22.480)]  [G loss: -24.317] \n",
      "1160 [D loss: (-2.024)(R -33.202, F 29.154)]  [G loss: -25.506] \n",
      "1161 [D loss: (-3.429)(R -30.650, F 23.793)]  [G loss: -24.302] \n",
      "1161 [D loss: (-4.756)(R -30.752, F 21.241)]  [G loss: -20.851] \n",
      "1162 [D loss: (-4.080)(R -30.438, F 22.279)]  [G loss: -22.039] \n",
      "1162 [D loss: (-4.188)(R -28.827, F 20.452)]  [G loss: -18.599] \n",
      "1163 [D loss: (-2.692)(R -27.544, F 22.159)]  [G loss: -14.691] \n",
      "1163 [D loss: (-4.048)(R -26.719, F 18.624)]  [G loss: -17.220] \n",
      "1164 [D loss: (-6.369)(R -25.095, F 12.357)]  [G loss: -15.666] \n",
      "1164 [D loss: (-6.604)(R -24.497, F 11.290)]  [G loss: -13.395] \n",
      "1165 [D loss: (-7.097)(R -24.067, F 9.874)]  [G loss: -12.711] \n",
      "1165 [D loss: (-6.604)(R -21.195, F 7.987)]  [G loss: -7.861] \n",
      "1166 [D loss: (-4.920)(R -18.271, F 8.431)]  [G loss: -8.958] \n",
      "1166 [D loss: (-8.577)(R -20.408, F 3.254)]  [G loss: -5.828] \n",
      "1167 [D loss: (-7.870)(R -18.945, F 3.205)]  [G loss: -0.485] \n",
      "1167 [D loss: (-9.139)(R -16.154, F -2.125)]  [G loss: -2.538] \n",
      "1168 [D loss: (-9.787)(R -18.451, F -1.123)]  [G loss: 0.854] \n",
      "1168 [D loss: (-8.973)(R -13.814, F -4.131)]  [G loss: 3.901] \n",
      "1169 [D loss: (-10.651)(R -13.818, F -7.485)]  [G loss: 4.491] \n",
      "1169 [D loss: (-7.861)(R -10.422, F -5.300)]  [G loss: 4.437] \n",
      "1170 [D loss: (-7.462)(R -7.731, F -7.193)]  [G loss: 12.871] \n",
      "1170 [D loss: (-11.073)(R -8.714, F -13.431)]  [G loss: 11.811] \n",
      "1171 [D loss: (-9.042)(R -5.160, F -12.923)]  [G loss: 14.426] \n",
      "1171 [D loss: (-9.950)(R -5.247, F -14.653)]  [G loss: 12.575] \n",
      "1172 [D loss: (-11.032)(R -2.683, F -19.382)]  [G loss: 18.361] \n",
      "1172 [D loss: (-12.127)(R -0.882, F -23.372)]  [G loss: 21.775] \n",
      "1173 [D loss: (-14.539)(R 3.646, F -32.724)]  [G loss: 22.742] \n",
      "1173 [D loss: (-14.052)(R 4.380, F -32.484)]  [G loss: 29.327] \n",
      "1174 [D loss: (-14.145)(R 7.079, F -35.369)]  [G loss: 35.499] \n",
      "1174 [D loss: (-14.017)(R 11.369, F -39.404)]  [G loss: 37.884] \n",
      "1175 [D loss: (-15.248)(R 11.719, F -42.215)]  [G loss: 40.119] \n",
      "1175 [D loss: (-15.143)(R 17.631, F -47.916)]  [G loss: 48.224] \n",
      "1176 [D loss: (-14.433)(R 20.453, F -49.318)]  [G loss: 52.220] \n",
      "1176 [D loss: (-14.459)(R 23.665, F -52.584)]  [G loss: 59.393] \n",
      "1177 [D loss: (-18.228)(R 27.661, F -64.116)]  [G loss: 60.169] \n",
      "1177 [D loss: (-15.879)(R 29.153, F -60.911)]  [G loss: 69.130] \n",
      "1178 [D loss: (-21.092)(R 31.929, F -74.112)]  [G loss: 72.995] \n",
      "1178 [D loss: (-15.970)(R 42.360, F -74.301)]  [G loss: 79.409] \n",
      "1179 [D loss: (-21.778)(R 43.428, F -86.985)]  [G loss: 86.542] \n",
      "1179 [D loss: (-18.369)(R 49.552, F -86.290)]  [G loss: 92.699] \n",
      "1180 [D loss: (-19.034)(R 55.606, F -93.673)]  [G loss: 94.685] \n",
      "1180 [D loss: (-22.870)(R 59.599, F -105.339)]  [G loss: 103.695] \n",
      "1181 [D loss: (-21.581)(R 61.747, F -104.908)]  [G loss: 106.201] \n",
      "1181 [D loss: (-24.130)(R 65.754, F -114.014)]  [G loss: 117.662] \n",
      "1182 [D loss: (-23.410)(R 70.883, F -117.703)]  [G loss: 118.092] \n",
      "1182 [D loss: (-25.777)(R 72.555, F -124.109)]  [G loss: 127.889] \n",
      "1183 [D loss: (-28.515)(R 75.214, F -132.244)]  [G loss: 137.633] \n",
      "1183 [D loss: (-28.863)(R 79.493, F -137.219)]  [G loss: 135.239] \n",
      "1184 [D loss: (-28.159)(R 86.926, F -143.244)]  [G loss: 143.474] \n",
      "1184 [D loss: (-29.643)(R 86.542, F -145.829)]  [G loss: 146.868] \n",
      "1185 [D loss: (-26.664)(R 96.941, F -150.268)]  [G loss: 152.987] \n",
      "1185 [D loss: (-27.819)(R 98.072, F -153.711)]  [G loss: 157.743] \n",
      "1186 [D loss: (-29.383)(R 103.970, F -162.737)]  [G loss: 157.118] \n",
      "1186 [D loss: (-32.977)(R 99.937, F -165.891)]  [G loss: 168.840] \n",
      "1187 [D loss: (-33.248)(R 106.914, F -173.410)]  [G loss: 165.911] \n",
      "1187 [D loss: (-33.902)(R 109.803, F -177.606)]  [G loss: 173.349] \n",
      "1188 [D loss: (-33.018)(R 116.022, F -182.058)]  [G loss: 185.460] \n",
      "1188 [D loss: (-28.465)(R 122.115, F -179.044)]  [G loss: 185.949] \n",
      "1189 [D loss: (-29.297)(R 126.256, F -184.850)]  [G loss: 185.095] \n",
      "1189 [D loss: (-33.290)(R 127.473, F -194.053)]  [G loss: 195.576] \n",
      "1190 [D loss: (-32.923)(R 132.361, F -198.206)]  [G loss: 191.161] \n",
      "1190 [D loss: (-29.519)(R 133.045, F -192.082)]  [G loss: 199.284] \n",
      "1191 [D loss: (-35.001)(R 135.205, F -205.208)]  [G loss: 199.511] \n",
      "1191 [D loss: (-28.294)(R 140.618, F -197.205)]  [G loss: 203.051] \n",
      "1192 [D loss: (-32.179)(R 138.840, F -203.199)]  [G loss: 205.924] \n",
      "1192 [D loss: (-34.311)(R 142.274, F -210.896)]  [G loss: 200.119] \n",
      "1193 [D loss: (-30.625)(R 144.801, F -206.052)]  [G loss: 206.390] \n",
      "1193 [D loss: (-30.631)(R 149.784, F -211.046)]  [G loss: 209.865] \n",
      "1194 [D loss: (-28.423)(R 150.437, F -207.283)]  [G loss: 221.389] \n",
      "1194 [D loss: (-28.785)(R 153.579, F -211.150)]  [G loss: 215.060] \n",
      "1195 [D loss: (-28.914)(R 154.141, F -211.968)]  [G loss: 225.763] \n",
      "1195 [D loss: (-25.197)(R 159.624, F -210.018)]  [G loss: 216.295] \n",
      "1196 [D loss: (-29.665)(R 156.138, F -215.468)]  [G loss: 227.496] \n",
      "1196 [D loss: (-35.168)(R 158.585, F -228.920)]  [G loss: 218.663] \n",
      "1197 [D loss: (-24.010)(R 161.505, F -209.524)]  [G loss: 220.057] \n",
      "1197 [D loss: (-34.978)(R 153.712, F -223.668)]  [G loss: 216.035] \n",
      "1198 [D loss: (-25.651)(R 161.685, F -212.987)]  [G loss: 228.079] \n",
      "1198 [D loss: (-23.614)(R 167.978, F -215.207)]  [G loss: 213.636] \n",
      "1199 [D loss: (-27.387)(R 164.221, F -218.994)]  [G loss: 226.521] \n",
      "1199 [D loss: (-23.039)(R 171.783, F -217.860)]  [G loss: 216.954] \n",
      "1200 [D loss: (-19.189)(R 159.355, F -197.734)]  [G loss: 215.517] \n",
      "1200 [D loss: (-26.802)(R 161.730, F -215.335)]  [G loss: 220.637] \n",
      "1201 [D loss: (-20.138)(R 168.606, F -208.881)]  [G loss: 220.634] \n",
      "1201 [D loss: (-24.635)(R 167.239, F -216.508)]  [G loss: 214.815] \n",
      "1202 [D loss: (-18.468)(R 163.929, F -200.865)]  [G loss: 207.583] \n",
      "1202 [D loss: (-25.565)(R 157.524, F -208.654)]  [G loss: 201.819] \n",
      "1203 [D loss: (-18.908)(R 160.591, F -198.408)]  [G loss: 204.724] \n",
      "1203 [D loss: (-23.759)(R 161.723, F -209.241)]  [G loss: 203.587] \n",
      "1204 [D loss: (-11.188)(R 168.259, F -190.634)]  [G loss: 202.821] \n",
      "1204 [D loss: (-21.751)(R 163.802, F -207.304)]  [G loss: 207.991] \n",
      "1205 [D loss: (-14.330)(R 170.836, F -199.497)]  [G loss: 190.882] \n",
      "1205 [D loss: (-17.498)(R 165.960, F -200.955)]  [G loss: 200.125] \n",
      "1206 [D loss: (-16.449)(R 165.790, F -198.689)]  [G loss: 202.595] \n",
      "1206 [D loss: (-16.251)(R 166.346, F -198.848)]  [G loss: 193.510] \n",
      "1207 [D loss: (-18.858)(R 160.796, F -198.512)]  [G loss: 192.860] \n",
      "1207 [D loss: (-13.032)(R 162.832, F -188.896)]  [G loss: 193.922] \n",
      "1208 [D loss: (-18.133)(R 161.741, F -198.008)]  [G loss: 190.638] \n",
      "1208 [D loss: (-10.882)(R 166.982, F -188.746)]  [G loss: 179.523] \n",
      "1209 [D loss: (-4.342)(R 169.597, F -178.282)]  [G loss: 183.323] \n",
      "1209 [D loss: (-9.499)(R 154.086, F -173.084)]  [G loss: 186.511] \n",
      "1210 [D loss: (-15.373)(R 155.361, F -186.107)]  [G loss: 184.161] \n",
      "1210 [D loss: (-9.924)(R 156.756, F -176.603)]  [G loss: 181.534] \n",
      "1211 [D loss: (-7.527)(R 162.650, F -177.704)]  [G loss: 181.392] \n",
      "1211 [D loss: (-15.378)(R 152.984, F -183.741)]  [G loss: 173.382] \n",
      "1212 [D loss: (-9.159)(R 159.331, F -177.648)]  [G loss: 183.608] \n",
      "1212 [D loss: (-9.839)(R 151.376, F -171.055)]  [G loss: 172.303] \n",
      "1213 [D loss: (-6.327)(R 153.827, F -166.481)]  [G loss: 168.021] \n",
      "1213 [D loss: (-9.370)(R 156.949, F -175.688)]  [G loss: 163.572] \n",
      "1214 [D loss: (-10.769)(R 153.272, F -174.810)]  [G loss: 165.874] \n",
      "1214 [D loss: (-4.042)(R 155.581, F -163.665)]  [G loss: 165.447] \n",
      "1215 [D loss: (-9.098)(R 150.189, F -168.384)]  [G loss: 170.076] \n",
      "1215 [D loss: (0.998)(R 163.939, F -161.943)]  [G loss: 165.128] \n",
      "1216 [D loss: (-5.662)(R 149.259, F -160.584)]  [G loss: 168.427] \n",
      "1216 [D loss: (-8.878)(R 147.403, F -165.158)]  [G loss: 156.269] \n",
      "1217 [D loss: (-5.565)(R 149.272, F -160.403)]  [G loss: 157.899] \n",
      "1217 [D loss: (-2.761)(R 148.869, F -154.392)]  [G loss: 154.969] \n",
      "1218 [D loss: (-8.960)(R 148.553, F -166.473)]  [G loss: 155.424] \n",
      "1218 [D loss: (-1.336)(R 148.661, F -151.332)]  [G loss: 151.764] \n",
      "1219 [D loss: (2.630)(R 149.123, F -143.863)]  [G loss: 149.974] \n",
      "1219 [D loss: (-3.909)(R 138.253, F -146.071)]  [G loss: 152.919] \n",
      "1220 [D loss: (2.565)(R 140.136, F -135.005)]  [G loss: 149.140] \n",
      "1220 [D loss: (-3.311)(R 139.848, F -146.469)]  [G loss: 139.871] \n",
      "1221 [D loss: (5.888)(R 142.555, F -130.779)]  [G loss: 132.424] \n",
      "1221 [D loss: (-0.594)(R 143.876, F -145.064)]  [G loss: 131.806] \n",
      "1222 [D loss: (3.180)(R 135.752, F -129.391)]  [G loss: 140.189] \n",
      "1222 [D loss: (1.438)(R 131.345, F -128.469)]  [G loss: 134.687] \n",
      "1223 [D loss: (1.426)(R 132.562, F -129.711)]  [G loss: 131.309] \n",
      "1223 [D loss: (-2.136)(R 123.934, F -128.207)]  [G loss: 130.086] \n",
      "1224 [D loss: (0.930)(R 126.553, F -124.694)]  [G loss: 122.014] \n",
      "1224 [D loss: (3.611)(R 129.967, F -122.746)]  [G loss: 126.398] \n",
      "1225 [D loss: (-0.441)(R 124.383, F -125.265)]  [G loss: 120.800] \n",
      "1225 [D loss: (-1.660)(R 122.256, F -125.575)]  [G loss: 125.427] \n",
      "1226 [D loss: (-3.562)(R 117.705, F -124.829)]  [G loss: 122.230] \n",
      "1226 [D loss: (3.023)(R 116.728, F -110.683)]  [G loss: 111.259] \n",
      "1227 [D loss: (2.011)(R 116.964, F -112.943)]  [G loss: 121.896] \n",
      "1227 [D loss: (4.836)(R 117.400, F -107.727)]  [G loss: 113.093] \n",
      "1228 [D loss: (-1.472)(R 110.805, F -113.749)]  [G loss: 116.499] \n",
      "1228 [D loss: (2.271)(R 110.141, F -105.599)]  [G loss: 106.498] \n",
      "1229 [D loss: (-0.197)(R 112.586, F -112.980)]  [G loss: 106.312] \n",
      "1229 [D loss: (9.241)(R 113.224, F -94.742)]  [G loss: 91.879] \n",
      "1230 [D loss: (-1.516)(R 103.281, F -106.312)]  [G loss: 95.976] \n",
      "1230 [D loss: (2.101)(R 105.015, F -100.813)]  [G loss: 104.655] \n",
      "1231 [D loss: (2.898)(R 101.752, F -95.956)]  [G loss: 104.686] \n",
      "1231 [D loss: (3.878)(R 97.172, F -89.415)]  [G loss: 96.439] \n",
      "1232 [D loss: (2.497)(R 98.866, F -93.871)]  [G loss: 96.952] \n",
      "1232 [D loss: (-3.555)(R 91.557, F -98.668)]  [G loss: 92.307] \n",
      "1233 [D loss: (-0.122)(R 95.539, F -95.783)]  [G loss: 94.494] \n",
      "1233 [D loss: (-1.268)(R 91.488, F -94.025)]  [G loss: 89.004] \n",
      "1234 [D loss: (3.534)(R 92.698, F -85.631)]  [G loss: 89.573] \n",
      "1234 [D loss: (1.698)(R 89.616, F -86.221)]  [G loss: 85.231] \n",
      "1235 [D loss: (2.579)(R 87.773, F -82.616)]  [G loss: 86.664] \n",
      "1235 [D loss: (-4.206)(R 85.309, F -93.721)]  [G loss: 89.643] \n",
      "1236 [D loss: (2.917)(R 86.236, F -80.402)]  [G loss: 83.371] \n",
      "1236 [D loss: (-0.927)(R 82.891, F -84.745)]  [G loss: 92.971] \n",
      "1237 [D loss: (-1.797)(R 84.081, F -87.675)]  [G loss: 84.111] \n",
      "1237 [D loss: (-3.076)(R 83.225, F -89.378)]  [G loss: 85.449] \n",
      "1238 [D loss: (0.442)(R 82.113, F -81.228)]  [G loss: 82.586] \n",
      "1238 [D loss: (1.528)(R 85.302, F -82.246)]  [G loss: 88.108] \n",
      "1239 [D loss: (-1.359)(R 79.882, F -82.599)]  [G loss: 81.383] \n",
      "1239 [D loss: (-0.743)(R 80.973, F -82.459)]  [G loss: 79.822] \n",
      "1240 [D loss: (0.437)(R 79.788, F -78.913)]  [G loss: 83.778] \n",
      "1240 [D loss: (-1.631)(R 75.762, F -79.024)]  [G loss: 80.884] \n",
      "1241 [D loss: (-3.261)(R 75.311, F -81.833)]  [G loss: 78.407] \n",
      "1241 [D loss: (-2.926)(R 75.333, F -81.185)]  [G loss: 79.918] \n",
      "1242 [D loss: (1.028)(R 76.975, F -74.918)]  [G loss: 75.664] \n",
      "1242 [D loss: (-2.167)(R 72.610, F -76.943)]  [G loss: 75.886] \n",
      "1243 [D loss: (-1.185)(R 75.000, F -77.371)]  [G loss: 79.384] \n",
      "1243 [D loss: (0.100)(R 76.075, F -75.874)]  [G loss: 72.987] \n",
      "1244 [D loss: (-2.032)(R 73.000, F -77.064)]  [G loss: 73.606] \n",
      "1244 [D loss: (-1.823)(R 73.374, F -77.020)]  [G loss: 79.429] \n",
      "1245 [D loss: (-0.306)(R 77.487, F -78.098)]  [G loss: 79.727] \n",
      "1245 [D loss: (-1.313)(R 72.574, F -75.201)]  [G loss: 75.382] \n",
      "1246 [D loss: (-1.432)(R 76.829, F -79.694)]  [G loss: 77.152] \n",
      "1246 [D loss: (-0.811)(R 74.765, F -76.386)]  [G loss: 80.918] \n",
      "1247 [D loss: (-1.323)(R 72.716, F -75.362)]  [G loss: 78.667] \n",
      "1247 [D loss: (-2.655)(R 74.250, F -79.560)]  [G loss: 73.668] \n",
      "1248 [D loss: (0.365)(R 73.785, F -73.055)]  [G loss: 80.376] \n",
      "1248 [D loss: (-1.396)(R 73.971, F -76.764)]  [G loss: 79.080] \n",
      "1249 [D loss: (-0.396)(R 74.792, F -75.585)]  [G loss: 76.223] \n",
      "1249 [D loss: (-2.627)(R 70.554, F -75.808)]  [G loss: 73.006] \n",
      "1250 [D loss: (-1.460)(R 71.629, F -74.549)]  [G loss: 73.221] \n",
      "1250 [D loss: (-3.831)(R 68.060, F -75.722)]  [G loss: 73.469] \n",
      "1251 [D loss: (-3.665)(R 68.426, F -75.756)]  [G loss: 73.022] \n",
      "1251 [D loss: (-4.582)(R 65.931, F -75.094)]  [G loss: 75.219] \n",
      "1252 [D loss: (-5.174)(R 66.481, F -76.830)]  [G loss: 73.902] \n",
      "1252 [D loss: (-3.021)(R 65.064, F -71.106)]  [G loss: 77.070] \n",
      "1253 [D loss: (-1.638)(R 66.720, F -69.995)]  [G loss: 74.027] \n",
      "1253 [D loss: (-3.647)(R 64.730, F -72.023)]  [G loss: 72.934] \n",
      "1254 [D loss: (-3.508)(R 65.919, F -72.935)]  [G loss: 76.952] \n",
      "1254 [D loss: (-5.658)(R 64.759, F -76.076)]  [G loss: 74.569] \n",
      "1255 [D loss: (-3.854)(R 67.787, F -75.495)]  [G loss: 73.535] \n",
      "1255 [D loss: (-3.558)(R 65.566, F -72.683)]  [G loss: 75.015] \n",
      "1256 [D loss: (-6.045)(R 64.082, F -76.172)]  [G loss: 78.112] \n",
      "1256 [D loss: (-3.350)(R 69.213, F -75.912)]  [G loss: 79.420] \n",
      "1257 [D loss: (-5.518)(R 69.456, F -80.492)]  [G loss: 76.973] \n",
      "1257 [D loss: (-2.488)(R 72.391, F -77.367)]  [G loss: 77.473] \n",
      "1258 [D loss: (-5.145)(R 70.898, F -81.188)]  [G loss: 82.399] \n",
      "1258 [D loss: (-4.799)(R 73.450, F -83.047)]  [G loss: 79.780] \n",
      "1259 [D loss: (-5.383)(R 72.080, F -82.846)]  [G loss: 83.034] \n",
      "1259 [D loss: (-5.308)(R 72.584, F -83.201)]  [G loss: 82.935] \n",
      "1260 [D loss: (-5.287)(R 73.655, F -84.230)]  [G loss: 86.613] \n",
      "1260 [D loss: (-6.738)(R 72.488, F -85.965)]  [G loss: 85.970] \n",
      "1261 [D loss: (-7.111)(R 72.615, F -86.838)]  [G loss: 85.972] \n",
      "1261 [D loss: (-6.181)(R 75.523, F -87.884)]  [G loss: 87.572] \n",
      "1262 [D loss: (-9.205)(R 73.296, F -91.705)]  [G loss: 89.577] \n",
      "1262 [D loss: (-9.109)(R 76.206, F -94.423)]  [G loss: 91.831] \n",
      "1263 [D loss: (-8.689)(R 78.998, F -96.375)]  [G loss: 94.356] \n",
      "1263 [D loss: (-6.336)(R 80.569, F -93.241)]  [G loss: 97.219] \n",
      "1264 [D loss: (-9.415)(R 80.941, F -99.772)]  [G loss: 95.100] \n",
      "1264 [D loss: (-8.896)(R 82.336, F -100.129)]  [G loss: 99.768] \n",
      "1265 [D loss: (-8.654)(R 83.128, F -100.436)]  [G loss: 102.870] \n",
      "1265 [D loss: (-8.935)(R 85.178, F -103.047)]  [G loss: 102.670] \n",
      "1266 [D loss: (-11.198)(R 85.506, F -107.903)]  [G loss: 103.240] \n",
      "1266 [D loss: (-10.093)(R 86.911, F -107.097)]  [G loss: 111.175] \n",
      "1267 [D loss: (-12.368)(R 85.366, F -110.103)]  [G loss: 108.503] \n",
      "1267 [D loss: (-12.662)(R 90.073, F -115.398)]  [G loss: 114.270] \n",
      "1268 [D loss: (-12.220)(R 88.616, F -113.055)]  [G loss: 117.271] \n",
      "1268 [D loss: (-11.590)(R 92.641, F -115.821)]  [G loss: 118.200] \n",
      "1269 [D loss: (-14.026)(R 93.542, F -121.594)]  [G loss: 119.281] \n",
      "1269 [D loss: (-12.606)(R 96.022, F -121.235)]  [G loss: 121.828] \n",
      "1270 [D loss: (-14.724)(R 94.249, F -123.697)]  [G loss: 125.255] \n",
      "1270 [D loss: (-14.511)(R 97.774, F -126.796)]  [G loss: 127.200] \n",
      "1271 [D loss: (-15.395)(R 97.833, F -128.623)]  [G loss: 126.743] \n",
      "1271 [D loss: (-15.109)(R 98.896, F -129.115)]  [G loss: 132.079] \n",
      "1272 [D loss: (-14.303)(R 101.499, F -130.105)]  [G loss: 134.943] \n",
      "1272 [D loss: (-18.056)(R 102.490, F -138.603)]  [G loss: 137.480] \n",
      "1273 [D loss: (-17.657)(R 104.149, F -139.464)]  [G loss: 142.811] \n",
      "1273 [D loss: (-19.705)(R 106.355, F -145.765)]  [G loss: 144.333] \n",
      "1274 [D loss: (-17.484)(R 106.950, F -141.917)]  [G loss: 142.329] \n",
      "1274 [D loss: (-19.050)(R 108.552, F -146.651)]  [G loss: 152.480] \n",
      "1275 [D loss: (-17.601)(R 115.524, F -150.725)]  [G loss: 155.432] \n",
      "1275 [D loss: (-20.907)(R 115.610, F -157.424)]  [G loss: 159.290] \n",
      "1276 [D loss: (-19.195)(R 117.742, F -156.132)]  [G loss: 156.673] \n",
      "1276 [D loss: (-22.857)(R 116.682, F -162.397)]  [G loss: 156.154] \n",
      "1277 [D loss: (-21.983)(R 120.510, F -164.476)]  [G loss: 160.310] \n",
      "1277 [D loss: (-22.775)(R 119.420, F -164.971)]  [G loss: 165.645] \n",
      "1278 [D loss: (-20.309)(R 126.371, F -166.989)]  [G loss: 172.477] \n",
      "1278 [D loss: (-21.204)(R 123.026, F -165.433)]  [G loss: 173.289] \n",
      "1279 [D loss: (-22.663)(R 128.148, F -173.475)]  [G loss: 179.846] \n",
      "1279 [D loss: (-24.778)(R 125.793, F -175.349)]  [G loss: 177.403] \n",
      "1280 [D loss: (-22.093)(R 129.871, F -174.056)]  [G loss: 179.568] \n",
      "1280 [D loss: (-25.070)(R 129.288, F -179.429)]  [G loss: 177.409] \n",
      "1281 [D loss: (-23.306)(R 133.538, F -180.150)]  [G loss: 184.410] \n",
      "1281 [D loss: (-26.910)(R 131.749, F -185.569)]  [G loss: 184.488] \n",
      "1282 [D loss: (-23.729)(R 136.850, F -184.308)]  [G loss: 190.727] \n",
      "1282 [D loss: (-23.891)(R 137.561, F -185.343)]  [G loss: 186.950] \n",
      "1283 [D loss: (-26.918)(R 132.983, F -186.818)]  [G loss: 192.568] \n",
      "1283 [D loss: (-25.561)(R 137.510, F -188.632)]  [G loss: 193.946] \n",
      "1284 [D loss: (-28.109)(R 137.488, F -193.707)]  [G loss: 184.368] \n",
      "1284 [D loss: (-29.112)(R 139.085, F -197.310)]  [G loss: 187.984] \n",
      "1285 [D loss: (-28.106)(R 136.334, F -192.545)]  [G loss: 191.971] \n",
      "1285 [D loss: (-27.320)(R 143.149, F -197.790)]  [G loss: 200.011] \n",
      "1286 [D loss: (-27.363)(R 142.174, F -196.901)]  [G loss: 194.797] \n",
      "1286 [D loss: (-26.271)(R 145.260, F -197.802)]  [G loss: 202.190] \n",
      "1287 [D loss: (-26.476)(R 144.681, F -197.634)]  [G loss: 203.938] \n",
      "1287 [D loss: (-26.126)(R 145.114, F -197.366)]  [G loss: 203.972] \n",
      "1288 [D loss: (-31.591)(R 143.168, F -206.349)]  [G loss: 201.555] \n",
      "1288 [D loss: (-23.672)(R 154.241, F -201.584)]  [G loss: 199.989] \n",
      "1289 [D loss: (-24.731)(R 153.693, F -203.155)]  [G loss: 201.209] \n",
      "1289 [D loss: (-26.892)(R 151.061, F -204.846)]  [G loss: 205.907] \n",
      "1290 [D loss: (-25.916)(R 155.441, F -207.272)]  [G loss: 204.433] \n",
      "1290 [D loss: (-27.908)(R 146.090, F -201.907)]  [G loss: 207.259] \n",
      "1291 [D loss: (-26.439)(R 158.870, F -211.747)]  [G loss: 199.601] \n",
      "1291 [D loss: (-24.114)(R 155.765, F -203.993)]  [G loss: 203.378] \n",
      "1292 [D loss: (-26.744)(R 151.245, F -204.733)]  [G loss: 211.614] \n",
      "1292 [D loss: (-29.025)(R 150.424, F -208.475)]  [G loss: 198.471] \n",
      "1293 [D loss: (-24.456)(R 155.070, F -203.981)]  [G loss: 209.727] \n",
      "1293 [D loss: (-25.485)(R 151.873, F -202.842)]  [G loss: 204.045] \n",
      "1294 [D loss: (-26.258)(R 153.979, F -206.495)]  [G loss: 201.622] \n",
      "1294 [D loss: (-25.163)(R 155.025, F -205.350)]  [G loss: 206.090] \n",
      "1295 [D loss: (-21.700)(R 153.502, F -196.902)]  [G loss: 198.760] \n",
      "1295 [D loss: (-22.792)(R 159.612, F -205.196)]  [G loss: 203.194] \n",
      "1296 [D loss: (-24.017)(R 159.924, F -207.957)]  [G loss: 198.740] \n",
      "1296 [D loss: (-24.395)(R 151.091, F -199.882)]  [G loss: 192.340] \n",
      "1297 [D loss: (-23.186)(R 153.724, F -200.095)]  [G loss: 202.214] \n",
      "1297 [D loss: (-19.053)(R 149.553, F -187.658)]  [G loss: 195.035] \n",
      "1298 [D loss: (-26.952)(R 147.805, F -201.709)]  [G loss: 194.800] \n",
      "1298 [D loss: (-19.657)(R 151.413, F -190.726)]  [G loss: 203.028] \n",
      "1299 [D loss: (-19.421)(R 154.167, F -193.009)]  [G loss: 184.925] \n",
      "1299 [D loss: (-21.182)(R 152.341, F -194.704)]  [G loss: 197.057] \n",
      "1300 [D loss: (-20.962)(R 145.032, F -186.956)]  [G loss: 190.375] \n",
      "1300 [D loss: (-19.799)(R 155.299, F -194.898)]  [G loss: 192.586] \n",
      "1301 [D loss: (-16.600)(R 152.523, F -185.722)]  [G loss: 180.323] \n",
      "1301 [D loss: (-22.319)(R 148.337, F -192.976)]  [G loss: 179.632] \n",
      "1302 [D loss: (-20.695)(R 150.151, F -191.541)]  [G loss: 184.725] \n",
      "1302 [D loss: (-15.608)(R 144.381, F -175.596)]  [G loss: 183.262] \n",
      "1303 [D loss: (-15.116)(R 146.500, F -176.732)]  [G loss: 179.188] \n",
      "1303 [D loss: (-15.632)(R 141.895, F -173.158)]  [G loss: 176.333] \n",
      "1304 [D loss: (-9.824)(R 147.188, F -166.836)]  [G loss: 182.446] \n",
      "1304 [D loss: (-10.416)(R 149.136, F -169.968)]  [G loss: 173.898] \n",
      "1305 [D loss: (-10.182)(R 148.835, F -169.198)]  [G loss: 177.017] \n",
      "1305 [D loss: (-10.342)(R 147.395, F -168.079)]  [G loss: 166.032] \n",
      "1306 [D loss: (-6.491)(R 148.988, F -161.971)]  [G loss: 168.658] \n",
      "1306 [D loss: (-6.750)(R 139.304, F -152.804)]  [G loss: 166.784] \n",
      "1307 [D loss: (-11.913)(R 135.383, F -159.209)]  [G loss: 156.438] \n",
      "1307 [D loss: (-7.774)(R 135.043, F -150.590)]  [G loss: 160.947] \n",
      "1308 [D loss: (-0.830)(R 141.463, F -143.123)]  [G loss: 157.385] \n",
      "1308 [D loss: (-6.472)(R 134.426, F -147.369)]  [G loss: 146.188] \n",
      "1309 [D loss: (-5.066)(R 131.970, F -142.102)]  [G loss: 146.342] \n",
      "1309 [D loss: (-4.710)(R 131.103, F -140.523)]  [G loss: 138.313] \n",
      "1310 [D loss: (-0.427)(R 129.278, F -130.132)]  [G loss: 143.009] \n",
      "1310 [D loss: (-1.299)(R 127.932, F -130.531)]  [G loss: 142.913] \n",
      "1311 [D loss: (-1.578)(R 127.626, F -130.783)]  [G loss: 128.429] \n",
      "1311 [D loss: (-9.731)(R 114.866, F -134.328)]  [G loss: 131.478] \n",
      "1312 [D loss: (-0.547)(R 120.713, F -121.806)]  [G loss: 121.684] \n",
      "1312 [D loss: (-3.604)(R 114.687, F -121.896)]  [G loss: 126.493] \n",
      "1313 [D loss: (-1.475)(R 111.203, F -114.154)]  [G loss: 118.677] \n",
      "1313 [D loss: (-1.836)(R 109.860, F -113.532)]  [G loss: 114.721] \n",
      "1314 [D loss: (-1.846)(R 105.500, F -109.192)]  [G loss: 116.273] \n",
      "1314 [D loss: (6.995)(R 108.263, F -94.273)]  [G loss: 103.875] \n",
      "1315 [D loss: (0.055)(R 99.437, F -99.328)]  [G loss: 100.487] \n",
      "1315 [D loss: (-3.484)(R 95.161, F -102.129)]  [G loss: 88.493] \n",
      "1316 [D loss: (-5.835)(R 93.940, F -105.610)]  [G loss: 91.339] \n",
      "1316 [D loss: (-0.953)(R 85.895, F -87.802)]  [G loss: 88.517] \n",
      "1317 [D loss: (9.908)(R 96.035, F -76.218)]  [G loss: 88.336] \n",
      "1317 [D loss: (0.372)(R 78.630, F -77.887)]  [G loss: 64.407] \n",
      "1318 [D loss: (1.343)(R 78.788, F -76.101)]  [G loss: 81.204] \n",
      "1318 [D loss: (1.923)(R 78.688, F -74.843)]  [G loss: 70.471] \n",
      "1319 [D loss: (3.811)(R 75.845, F -68.222)]  [G loss: 63.383] \n",
      "1319 [D loss: (2.742)(R 73.525, F -68.041)]  [G loss: 59.565] \n",
      "1320 [D loss: (4.471)(R 66.903, F -57.961)]  [G loss: 60.761] \n",
      "1320 [D loss: (3.204)(R 60.144, F -53.736)]  [G loss: 50.560] \n",
      "1321 [D loss: (-0.340)(R 62.860, F -63.539)]  [G loss: 50.799] \n",
      "1321 [D loss: (3.389)(R 57.592, F -50.815)]  [G loss: 44.615] \n",
      "1322 [D loss: (7.599)(R 57.469, F -42.271)]  [G loss: 40.628] \n",
      "1322 [D loss: (3.220)(R 46.929, F -40.488)]  [G loss: 39.194] \n",
      "1323 [D loss: (2.389)(R 44.105, F -39.326)]  [G loss: 37.181] \n",
      "1323 [D loss: (9.513)(R 47.153, F -28.127)]  [G loss: 34.798] \n",
      "1324 [D loss: (6.373)(R 44.522, F -31.775)]  [G loss: 31.218] \n",
      "1324 [D loss: (8.454)(R 40.560, F -23.653)]  [G loss: 30.005] \n",
      "1325 [D loss: (3.057)(R 30.219, F -24.104)]  [G loss: 26.269] \n",
      "1325 [D loss: (-0.702)(R 28.362, F -29.765)]  [G loss: 24.395] \n",
      "1326 [D loss: (9.862)(R 28.077, F -8.354)]  [G loss: 25.882] \n",
      "1326 [D loss: (5.079)(R 27.170, F -17.013)]  [G loss: 19.907] \n",
      "1327 [D loss: (7.220)(R 24.672, F -10.233)]  [G loss: 17.159] \n",
      "1327 [D loss: (4.319)(R 24.013, F -15.375)]  [G loss: 17.714] \n",
      "1328 [D loss: (4.220)(R 18.494, F -10.054)]  [G loss: 3.916] \n",
      "1328 [D loss: (3.813)(R 18.314, F -10.688)]  [G loss: 0.918] \n",
      "1329 [D loss: (8.353)(R 16.856, F -0.150)]  [G loss: 6.558] \n",
      "1329 [D loss: (4.378)(R 11.059, F -2.302)]  [G loss: -1.117] \n",
      "1330 [D loss: (4.296)(R 8.562, F 0.031)]  [G loss: -1.131] \n",
      "1330 [D loss: (4.275)(R 4.971, F 3.580)]  [G loss: -2.950] \n",
      "1331 [D loss: (4.762)(R 4.721, F 4.803)]  [G loss: 1.523] \n",
      "1331 [D loss: (4.158)(R 0.004, F 8.313)]  [G loss: -7.760] \n",
      "1332 [D loss: (1.199)(R -0.873, F 3.271)]  [G loss: -3.309] \n",
      "1332 [D loss: (2.305)(R -1.348, F 5.959)]  [G loss: -10.753] \n",
      "1333 [D loss: (-2.685)(R -4.901, F -0.469)]  [G loss: -8.862] \n",
      "1333 [D loss: (2.973)(R -8.582, F 14.529)]  [G loss: -10.382] \n",
      "1334 [D loss: (1.289)(R -5.303, F 7.881)]  [G loss: -9.633] \n",
      "1334 [D loss: (3.816)(R -5.993, F 13.625)]  [G loss: -19.334] \n",
      "1335 [D loss: (6.063)(R -9.096, F 21.222)]  [G loss: -18.135] \n",
      "1335 [D loss: (7.530)(R -7.007, F 22.067)]  [G loss: -17.294] \n",
      "1336 [D loss: (3.791)(R -12.224, F 19.806)]  [G loss: -15.042] \n",
      "1336 [D loss: (1.929)(R -12.587, F 16.446)]  [G loss: -20.265] \n",
      "1337 [D loss: (-0.454)(R -16.971, F 16.063)]  [G loss: -15.197] \n",
      "1337 [D loss: (0.260)(R -16.906, F 17.427)]  [G loss: -22.209] \n",
      "1338 [D loss: (2.759)(R -19.057, F 24.575)]  [G loss: -28.381] \n",
      "1338 [D loss: (1.669)(R -18.482, F 21.820)]  [G loss: -20.099] \n",
      "1339 [D loss: (1.971)(R -22.140, F 26.082)]  [G loss: -27.032] \n",
      "1339 [D loss: (2.861)(R -22.496, F 28.217)]  [G loss: -22.971] \n",
      "1340 [D loss: (0.936)(R -21.876, F 23.749)]  [G loss: -23.377] \n",
      "1340 [D loss: (0.317)(R -28.120, F 28.755)]  [G loss: -27.995] \n",
      "1341 [D loss: (-1.046)(R -29.527, F 27.435)]  [G loss: -26.425] \n",
      "1341 [D loss: (4.274)(R -27.704, F 36.253)]  [G loss: -28.357] \n",
      "1342 [D loss: (-0.334)(R -29.886, F 29.218)]  [G loss: -29.021] \n",
      "1342 [D loss: (2.212)(R -28.670, F 33.095)]  [G loss: -28.502] \n",
      "1343 [D loss: (-0.748)(R -30.171, F 28.674)]  [G loss: -31.043] \n",
      "1343 [D loss: (0.218)(R -32.588, F 33.023)]  [G loss: -30.646] \n",
      "1344 [D loss: (0.364)(R -34.006, F 34.735)]  [G loss: -32.591] \n",
      "1344 [D loss: (-3.822)(R -35.847, F 28.204)]  [G loss: -28.268] \n",
      "1345 [D loss: (-1.256)(R -37.014, F 34.502)]  [G loss: -35.865] \n",
      "1345 [D loss: (-0.988)(R -38.322, F 36.346)]  [G loss: -37.968] \n",
      "1346 [D loss: (0.659)(R -39.331, F 40.650)]  [G loss: -37.983] \n",
      "1346 [D loss: (-2.805)(R -40.783, F 35.174)]  [G loss: -35.129] \n",
      "1347 [D loss: (-2.683)(R -43.283, F 37.917)]  [G loss: -32.933] \n",
      "1347 [D loss: (-2.722)(R -42.952, F 37.508)]  [G loss: -35.457] \n",
      "1348 [D loss: (-5.061)(R -45.166, F 35.044)]  [G loss: -37.351] \n",
      "1348 [D loss: (-0.743)(R -43.165, F 41.678)]  [G loss: -39.610] \n",
      "1349 [D loss: (-1.405)(R -43.169, F 40.358)]  [G loss: -39.737] \n",
      "1349 [D loss: (-5.237)(R -48.566, F 38.091)]  [G loss: -44.598] \n",
      "1350 [D loss: (-1.689)(R -48.835, F 45.457)]  [G loss: -42.382] \n",
      "1350 [D loss: (-3.519)(R -51.019, F 43.980)]  [G loss: -42.988] \n",
      "1351 [D loss: (-3.109)(R -52.573, F 46.355)]  [G loss: -46.748] \n",
      "1351 [D loss: (-6.097)(R -56.483, F 44.290)]  [G loss: -42.182] \n",
      "1352 [D loss: (-2.689)(R -52.994, F 47.617)]  [G loss: -48.192] \n",
      "1352 [D loss: (-7.037)(R -57.456, F 43.383)]  [G loss: -48.552] \n",
      "1353 [D loss: (-6.640)(R -55.008, F 41.727)]  [G loss: -43.702] \n",
      "1353 [D loss: (-5.129)(R -59.193, F 48.935)]  [G loss: -45.194] \n",
      "1354 [D loss: (-3.283)(R -56.772, F 50.207)]  [G loss: -47.028] \n",
      "1354 [D loss: (-6.534)(R -60.042, F 46.974)]  [G loss: -52.549] \n",
      "1355 [D loss: (-2.795)(R -56.309, F 50.719)]  [G loss: -52.114] \n",
      "1355 [D loss: (-3.490)(R -58.472, F 51.491)]  [G loss: -51.145] \n",
      "1356 [D loss: (-4.087)(R -65.762, F 57.588)]  [G loss: -55.729] \n",
      "1356 [D loss: (-5.804)(R -67.078, F 55.470)]  [G loss: -56.399] \n",
      "1357 [D loss: (-5.869)(R -65.770, F 54.033)]  [G loss: -51.328] \n",
      "1357 [D loss: (-4.449)(R -68.885, F 59.987)]  [G loss: -56.496] \n",
      "1358 [D loss: (-5.184)(R -68.706, F 58.337)]  [G loss: -56.540] \n",
      "1358 [D loss: (-5.842)(R -72.136, F 60.452)]  [G loss: -61.660] \n",
      "1359 [D loss: (-5.499)(R -70.192, F 59.193)]  [G loss: -56.196] \n",
      "1359 [D loss: (-3.613)(R -71.573, F 64.346)]  [G loss: -58.693] \n",
      "1360 [D loss: (-6.528)(R -75.808, F 62.751)]  [G loss: -57.990] \n",
      "1360 [D loss: (-4.147)(R -75.676, F 67.383)]  [G loss: -61.289] \n",
      "1361 [D loss: (-7.076)(R -79.381, F 65.230)]  [G loss: -63.175] \n",
      "1361 [D loss: (-6.895)(R -75.457, F 61.667)]  [G loss: -65.637] \n",
      "1362 [D loss: (-5.043)(R -74.881, F 64.795)]  [G loss: -65.940] \n",
      "1362 [D loss: (-9.613)(R -85.366, F 66.141)]  [G loss: -63.527] \n",
      "1363 [D loss: (-5.070)(R -80.042, F 69.902)]  [G loss: -63.623] \n",
      "1363 [D loss: (-5.987)(R -80.130, F 68.155)]  [G loss: -65.678] \n",
      "1364 [D loss: (-10.365)(R -84.807, F 64.077)]  [G loss: -69.764] \n",
      "1364 [D loss: (-6.693)(R -86.745, F 73.358)]  [G loss: -70.519] \n",
      "1365 [D loss: (-7.682)(R -85.543, F 70.180)]  [G loss: -72.814] \n",
      "1365 [D loss: (-7.187)(R -90.243, F 75.869)]  [G loss: -72.122] \n",
      "1366 [D loss: (-6.169)(R -91.910, F 79.572)]  [G loss: -67.622] \n",
      "1366 [D loss: (-10.668)(R -95.788, F 74.453)]  [G loss: -79.287] \n",
      "1367 [D loss: (-10.985)(R -93.768, F 71.798)]  [G loss: -75.396] \n",
      "1367 [D loss: (-8.993)(R -94.967, F 76.981)]  [G loss: -78.638] \n",
      "1368 [D loss: (-5.982)(R -91.786, F 79.823)]  [G loss: -79.007] \n",
      "1368 [D loss: (-8.349)(R -92.963, F 76.265)]  [G loss: -77.008] \n",
      "1369 [D loss: (-11.048)(R -97.183, F 75.087)]  [G loss: -78.682] \n",
      "1369 [D loss: (-10.684)(R -100.685, F 79.318)]  [G loss: -83.764] \n",
      "1370 [D loss: (-9.468)(R -99.799, F 80.864)]  [G loss: -80.093] \n",
      "1370 [D loss: (-9.866)(R -106.092, F 86.359)]  [G loss: -87.820] \n",
      "1371 [D loss: (-8.830)(R -101.559, F 83.899)]  [G loss: -84.540] \n",
      "1371 [D loss: (-8.944)(R -99.926, F 82.037)]  [G loss: -89.021] \n",
      "1372 [D loss: (-6.678)(R -103.700, F 90.345)]  [G loss: -84.842] \n",
      "1372 [D loss: (-8.276)(R -105.805, F 89.253)]  [G loss: -83.359] \n",
      "1373 [D loss: (-8.123)(R -104.452, F 88.205)]  [G loss: -91.664] \n",
      "1373 [D loss: (-7.908)(R -107.364, F 91.547)]  [G loss: -87.613] \n",
      "1374 [D loss: (-9.085)(R -105.906, F 87.736)]  [G loss: -85.785] \n",
      "1374 [D loss: (-7.453)(R -104.250, F 89.343)]  [G loss: -86.187] \n",
      "1375 [D loss: (-6.763)(R -105.535, F 92.009)]  [G loss: -97.035] \n",
      "1375 [D loss: (-9.272)(R -113.334, F 94.791)]  [G loss: -85.832] \n",
      "1376 [D loss: (-9.616)(R -107.420, F 88.189)]  [G loss: -86.441] \n",
      "1376 [D loss: (-7.351)(R -108.164, F 93.463)]  [G loss: -89.408] \n",
      "1377 [D loss: (-9.562)(R -110.633, F 91.510)]  [G loss: -85.479] \n",
      "1377 [D loss: (-8.762)(R -112.633, F 95.109)]  [G loss: -88.325] \n",
      "1378 [D loss: (-9.938)(R -113.535, F 93.658)]  [G loss: -95.489] \n",
      "1378 [D loss: (-4.921)(R -111.292, F 101.450)]  [G loss: -95.582] \n",
      "1379 [D loss: (-8.482)(R -113.931, F 96.968)]  [G loss: -89.690] \n",
      "1379 [D loss: (-10.895)(R -115.271, F 93.481)]  [G loss: -95.401] \n",
      "1380 [D loss: (-9.065)(R -111.529, F 93.398)]  [G loss: -91.044] \n",
      "1380 [D loss: (-10.551)(R -114.539, F 93.438)]  [G loss: -96.997] \n",
      "1381 [D loss: (-11.576)(R -111.865, F 88.713)]  [G loss: -93.938] \n",
      "1381 [D loss: (-9.196)(R -114.410, F 96.019)]  [G loss: -88.201] \n",
      "1382 [D loss: (-10.746)(R -113.153, F 91.661)]  [G loss: -98.589] \n",
      "1382 [D loss: (-14.659)(R -119.349, F 90.030)]  [G loss: -85.674] \n",
      "1383 [D loss: (-13.231)(R -118.142, F 91.680)]  [G loss: -95.574] \n",
      "1383 [D loss: (-12.767)(R -120.380, F 94.846)]  [G loss: -94.046] \n",
      "1384 [D loss: (-11.238)(R -119.962, F 97.487)]  [G loss: -95.188] \n",
      "1384 [D loss: (-14.340)(R -118.979, F 90.298)]  [G loss: -97.122] \n",
      "1385 [D loss: (-14.156)(R -123.025, F 94.713)]  [G loss: -94.933] \n",
      "1385 [D loss: (-11.862)(R -114.731, F 91.008)]  [G loss: -93.536] \n",
      "1386 [D loss: (-12.093)(R -113.818, F 89.631)]  [G loss: -91.668] \n",
      "1386 [D loss: (-13.526)(R -118.592, F 91.540)]  [G loss: -95.011] \n",
      "1387 [D loss: (-13.360)(R -116.640, F 89.919)]  [G loss: -91.891] \n",
      "1387 [D loss: (-9.240)(R -118.936, F 100.455)]  [G loss: -92.690] \n",
      "1388 [D loss: (-14.540)(R -120.315, F 91.235)]  [G loss: -99.204] \n",
      "1388 [D loss: (-17.124)(R -118.728, F 84.480)]  [G loss: -92.916] \n",
      "1389 [D loss: (-13.766)(R -118.943, F 91.411)]  [G loss: -95.582] \n",
      "1389 [D loss: (-11.920)(R -117.778, F 93.939)]  [G loss: -92.839] \n",
      "1390 [D loss: (-14.252)(R -115.335, F 86.832)]  [G loss: -93.642] \n",
      "1390 [D loss: (-16.749)(R -118.016, F 84.517)]  [G loss: -91.700] \n",
      "1391 [D loss: (-13.675)(R -116.619, F 89.270)]  [G loss: -90.574] \n",
      "1391 [D loss: (-13.458)(R -115.878, F 88.963)]  [G loss: -93.650] \n",
      "1392 [D loss: (-19.009)(R -119.006, F 80.988)]  [G loss: -88.631] \n",
      "1392 [D loss: (-15.535)(R -120.797, F 89.727)]  [G loss: -89.025] \n",
      "1393 [D loss: (-17.860)(R -119.847, F 84.126)]  [G loss: -81.882] \n",
      "1393 [D loss: (-17.599)(R -118.000, F 82.803)]  [G loss: -85.741] \n",
      "1394 [D loss: (-14.796)(R -115.365, F 85.772)]  [G loss: -81.387] \n",
      "1394 [D loss: (-16.288)(R -118.985, F 86.409)]  [G loss: -82.858] \n",
      "1395 [D loss: (-17.142)(R -116.613, F 82.329)]  [G loss: -86.293] \n",
      "1395 [D loss: (-17.375)(R -118.462, F 83.711)]  [G loss: -82.289] \n",
      "1396 [D loss: (-24.866)(R -114.961, F 65.228)]  [G loss: -77.711] \n",
      "1396 [D loss: (-15.611)(R -113.196, F 81.974)]  [G loss: -77.599] \n",
      "1397 [D loss: (-15.586)(R -113.962, F 82.789)]  [G loss: -81.873] \n",
      "1397 [D loss: (-17.923)(R -115.446, F 79.601)]  [G loss: -74.176] \n",
      "1398 [D loss: (-13.041)(R -110.568, F 84.486)]  [G loss: -69.788] \n",
      "1398 [D loss: (-18.285)(R -110.690, F 74.121)]  [G loss: -72.839] \n",
      "1399 [D loss: (-20.719)(R -107.719, F 66.280)]  [G loss: -76.356] \n",
      "1399 [D loss: (-16.329)(R -110.602, F 77.944)]  [G loss: -76.721] \n",
      "1400 [D loss: (-18.522)(R -109.407, F 72.363)]  [G loss: -76.281] \n",
      "1400 [D loss: (-22.831)(R -107.405, F 61.743)]  [G loss: -70.206] \n",
      "1401 [D loss: (-18.109)(R -109.733, F 73.516)]  [G loss: -67.963] \n",
      "1401 [D loss: (-16.697)(R -105.207, F 71.813)]  [G loss: -67.306] \n",
      "1402 [D loss: (-20.205)(R -108.123, F 67.713)]  [G loss: -69.426] \n",
      "1402 [D loss: (-16.636)(R -105.313, F 72.041)]  [G loss: -66.285] \n",
      "1403 [D loss: (-20.719)(R -101.989, F 60.551)]  [G loss: -58.280] \n",
      "1403 [D loss: (-15.592)(R -101.644, F 70.460)]  [G loss: -64.489] \n",
      "1404 [D loss: (-17.969)(R -99.370, F 63.433)]  [G loss: -55.120] \n",
      "1404 [D loss: (-23.163)(R -101.004, F 54.679)]  [G loss: -55.057] \n",
      "1405 [D loss: (-19.790)(R -96.680, F 57.099)]  [G loss: -57.341] \n",
      "1405 [D loss: (-21.142)(R -96.109, F 53.824)]  [G loss: -54.098] \n",
      "1406 [D loss: (-21.312)(R -96.834, F 54.210)]  [G loss: -50.912] \n",
      "1406 [D loss: (-27.502)(R -99.842, F 44.837)]  [G loss: -43.675] \n",
      "1407 [D loss: (-21.470)(R -93.331, F 50.390)]  [G loss: -52.347] \n",
      "1407 [D loss: (-18.785)(R -87.692, F 50.122)]  [G loss: -46.338] \n",
      "1408 [D loss: (-19.778)(R -88.352, F 48.796)]  [G loss: -41.389] \n",
      "1408 [D loss: (-19.081)(R -85.404, F 47.243)]  [G loss: -38.523] \n",
      "1409 [D loss: (-21.158)(R -86.435, F 44.120)]  [G loss: -36.040] \n",
      "1409 [D loss: (-18.349)(R -82.802, F 46.104)]  [G loss: -42.341] \n",
      "1410 [D loss: (-23.536)(R -81.666, F 34.594)]  [G loss: -31.095] \n",
      "1410 [D loss: (-21.927)(R -78.530, F 34.677)]  [G loss: -39.134] \n",
      "1411 [D loss: (-22.143)(R -77.278, F 32.992)]  [G loss: -41.022] \n",
      "1411 [D loss: (-22.117)(R -73.396, F 29.162)]  [G loss: -41.097] \n",
      "1412 [D loss: (-25.799)(R -73.171, F 21.573)]  [G loss: -33.312] \n",
      "1412 [D loss: (-14.989)(R -69.714, F 39.735)]  [G loss: -29.141] \n",
      "1413 [D loss: (-18.240)(R -67.345, F 30.864)]  [G loss: -31.545] \n",
      "1413 [D loss: (-17.262)(R -62.905, F 28.380)]  [G loss: -29.592] \n",
      "1414 [D loss: (-19.852)(R -69.045, F 29.340)]  [G loss: -27.558] \n",
      "1414 [D loss: (-19.282)(R -63.156, F 24.592)]  [G loss: -34.295] \n",
      "1415 [D loss: (-16.019)(R -62.271, F 30.233)]  [G loss: -20.509] \n",
      "1415 [D loss: (-21.489)(R -65.040, F 22.063)]  [G loss: -32.826] \n",
      "1416 [D loss: (-22.511)(R -61.443, F 16.420)]  [G loss: -29.757] \n",
      "1416 [D loss: (-16.179)(R -53.682, F 21.325)]  [G loss: -23.467] \n",
      "1417 [D loss: (-15.194)(R -57.684, F 27.295)]  [G loss: -24.925] \n",
      "1417 [D loss: (-23.133)(R -56.491, F 10.226)]  [G loss: -13.854] \n",
      "1418 [D loss: (-15.911)(R -47.462, F 15.640)]  [G loss: -18.342] \n",
      "1418 [D loss: (-16.507)(R -48.687, F 15.674)]  [G loss: -23.579] \n",
      "1419 [D loss: (-12.954)(R -49.109, F 23.202)]  [G loss: -32.941] \n",
      "1419 [D loss: (-9.977)(R -42.189, F 22.236)]  [G loss: -20.342] \n",
      "1420 [D loss: (-14.724)(R -43.333, F 13.885)]  [G loss: -13.051] \n",
      "1420 [D loss: (-18.510)(R -43.610, F 6.590)]  [G loss: -18.556] \n",
      "1421 [D loss: (-9.719)(R -36.209, F 16.770)]  [G loss: -19.406] \n",
      "1421 [D loss: (-12.583)(R -47.478, F 22.313)]  [G loss: -25.824] \n",
      "1422 [D loss: (-11.759)(R -38.969, F 15.451)]  [G loss: -12.581] \n",
      "1422 [D loss: (-7.038)(R -35.851, F 21.776)]  [G loss: -19.431] \n",
      "1423 [D loss: (-11.195)(R -35.515, F 13.125)]  [G loss: -18.372] \n",
      "1423 [D loss: (-8.540)(R -35.294, F 18.214)]  [G loss: -21.080] \n",
      "1424 [D loss: (-8.842)(R -40.584, F 22.899)]  [G loss: -13.902] \n",
      "1424 [D loss: (-12.721)(R -38.257, F 12.814)]  [G loss: -19.396] \n",
      "1425 [D loss: (-12.141)(R -32.506, F 8.224)]  [G loss: -19.518] \n",
      "1425 [D loss: (-9.923)(R -37.039, F 17.193)]  [G loss: -13.310] \n",
      "1426 [D loss: (-4.901)(R -31.215, F 21.413)]  [G loss: -21.783] \n",
      "1426 [D loss: (-6.312)(R -29.411, F 16.787)]  [G loss: -18.507] \n",
      "1427 [D loss: (-7.353)(R -29.533, F 14.827)]  [G loss: -23.786] \n",
      "1427 [D loss: (-6.551)(R -26.796, F 13.695)]  [G loss: -18.860] \n",
      "1428 [D loss: (-4.557)(R -31.888, F 22.775)]  [G loss: -27.674] \n",
      "1428 [D loss: (-9.422)(R -29.866, F 11.021)]  [G loss: -20.926] \n",
      "1429 [D loss: (-5.958)(R -30.080, F 18.163)]  [G loss: -12.835] \n",
      "1429 [D loss: (4.106)(R -27.805, F 36.018)]  [G loss: -24.798] \n",
      "1430 [D loss: (-3.070)(R -27.267, F 21.126)]  [G loss: -31.768] \n",
      "1430 [D loss: (-6.258)(R -29.622, F 17.106)]  [G loss: -18.812] \n",
      "1431 [D loss: (-4.467)(R -25.072, F 16.137)]  [G loss: -23.677] \n",
      "1431 [D loss: (5.537)(R -17.969, F 29.042)]  [G loss: -25.777] \n",
      "1432 [D loss: (-0.998)(R -23.460, F 21.465)]  [G loss: -26.319] \n",
      "1432 [D loss: (0.777)(R -23.691, F 25.246)]  [G loss: -24.306] \n",
      "1433 [D loss: (-0.986)(R -23.076, F 21.104)]  [G loss: -29.525] \n",
      "1433 [D loss: (6.737)(R -21.579, F 35.054)]  [G loss: -16.169] \n",
      "1434 [D loss: (4.951)(R -16.750, F 26.652)]  [G loss: -32.658] \n",
      "1434 [D loss: (0.915)(R -21.395, F 23.225)]  [G loss: -30.106] \n",
      "1435 [D loss: (3.552)(R -19.307, F 26.411)]  [G loss: -22.956] \n",
      "1435 [D loss: (2.225)(R -21.945, F 26.395)]  [G loss: -20.690] \n",
      "1436 [D loss: (10.878)(R -9.697, F 31.452)]  [G loss: -24.062] \n",
      "1436 [D loss: (5.838)(R -15.464, F 27.140)]  [G loss: -23.761] \n",
      "1437 [D loss: (4.125)(R -14.824, F 23.074)]  [G loss: -29.663] \n",
      "1437 [D loss: (8.316)(R -12.733, F 29.364)]  [G loss: -22.578] \n",
      "1438 [D loss: (0.934)(R -19.705, F 21.574)]  [G loss: -33.141] \n",
      "1438 [D loss: (4.969)(R -17.331, F 27.269)]  [G loss: -30.825] \n",
      "1439 [D loss: (1.538)(R -17.080, F 20.157)]  [G loss: -33.325] \n",
      "1439 [D loss: (8.945)(R -10.225, F 28.114)]  [G loss: -22.445] \n",
      "1440 [D loss: (6.521)(R -13.930, F 26.973)]  [G loss: -27.406] \n",
      "1440 [D loss: (10.377)(R -11.371, F 32.125)]  [G loss: -33.354] \n",
      "1441 [D loss: (9.901)(R -17.081, F 36.883)]  [G loss: -19.967] \n",
      "1441 [D loss: (8.685)(R -13.600, F 30.971)]  [G loss: -27.151] \n",
      "1442 [D loss: (2.997)(R -17.356, F 23.351)]  [G loss: -28.017] \n",
      "1442 [D loss: (10.236)(R -10.893, F 31.365)]  [G loss: -26.811] \n",
      "1443 [D loss: (7.483)(R -9.941, F 24.907)]  [G loss: -25.474] \n",
      "1443 [D loss: (5.236)(R -16.381, F 26.854)]  [G loss: -25.558] \n",
      "1444 [D loss: (7.081)(R -7.443, F 21.605)]  [G loss: -24.521] \n",
      "1444 [D loss: (10.350)(R -8.169, F 28.869)]  [G loss: -25.500] \n",
      "1445 [D loss: (9.474)(R -9.335, F 28.282)]  [G loss: -25.312] \n",
      "1445 [D loss: (8.517)(R -12.230, F 29.263)]  [G loss: -25.334] \n",
      "1446 [D loss: (9.743)(R -9.776, F 29.263)]  [G loss: -26.805] \n",
      "1446 [D loss: (5.192)(R -10.140, F 20.524)]  [G loss: -22.384] \n",
      "1447 [D loss: (2.833)(R -12.523, F 18.190)]  [G loss: -25.507] \n",
      "1447 [D loss: (5.805)(R -11.328, F 22.939)]  [G loss: -19.610] \n",
      "1448 [D loss: (2.627)(R -11.853, F 17.107)]  [G loss: -19.846] \n",
      "1448 [D loss: (4.184)(R -12.465, F 20.834)]  [G loss: -27.187] \n",
      "1449 [D loss: (6.101)(R -9.483, F 21.685)]  [G loss: -21.841] \n",
      "1449 [D loss: (6.306)(R -11.087, F 23.700)]  [G loss: -19.238] \n",
      "1450 [D loss: (6.451)(R -7.723, F 20.624)]  [G loss: -22.922] \n",
      "1450 [D loss: (4.023)(R -10.439, F 18.484)]  [G loss: -22.839] \n",
      "1451 [D loss: (5.784)(R -9.046, F 20.613)]  [G loss: -19.804] \n",
      "1451 [D loss: (3.070)(R -10.692, F 16.831)]  [G loss: -21.789] \n",
      "1452 [D loss: (3.914)(R -10.386, F 18.213)]  [G loss: -17.692] \n",
      "1452 [D loss: (5.584)(R -9.764, F 20.931)]  [G loss: -17.421] \n",
      "1453 [D loss: (4.131)(R -11.412, F 19.674)]  [G loss: -16.799] \n",
      "1453 [D loss: (5.103)(R -8.526, F 18.731)]  [G loss: -19.331] \n",
      "1454 [D loss: (4.257)(R -8.730, F 17.245)]  [G loss: -19.619] \n",
      "1454 [D loss: (3.353)(R -12.952, F 19.657)]  [G loss: -19.455] \n",
      "1455 [D loss: (5.146)(R -9.414, F 19.705)]  [G loss: -16.049] \n",
      "1455 [D loss: (3.421)(R -9.285, F 16.127)]  [G loss: -14.617] \n",
      "1456 [D loss: (3.152)(R -10.322, F 16.627)]  [G loss: -13.955] \n",
      "1456 [D loss: (2.473)(R -11.483, F 16.429)]  [G loss: -12.195] \n",
      "1457 [D loss: (2.778)(R -9.772, F 15.328)]  [G loss: -14.470] \n",
      "1457 [D loss: (1.596)(R -11.068, F 14.260)]  [G loss: -14.003] \n",
      "1458 [D loss: (5.857)(R -8.998, F 20.712)]  [G loss: -12.648] \n",
      "1458 [D loss: (2.753)(R -10.934, F 16.440)]  [G loss: -13.256] \n",
      "1459 [D loss: (3.438)(R -7.875, F 14.752)]  [G loss: -12.544] \n",
      "1459 [D loss: (1.925)(R -11.361, F 15.211)]  [G loss: -12.401] \n",
      "1460 [D loss: (1.433)(R -9.105, F 11.971)]  [G loss: -14.551] \n",
      "1460 [D loss: (-0.127)(R -10.934, F 10.681)]  [G loss: -13.849] \n",
      "1461 [D loss: (1.957)(R -11.264, F 15.179)]  [G loss: -11.127] \n",
      "1461 [D loss: (0.574)(R -10.726, F 11.873)]  [G loss: -10.730] \n",
      "1462 [D loss: (0.867)(R -10.678, F 12.412)]  [G loss: -14.189] \n",
      "1462 [D loss: (1.161)(R -9.725, F 12.046)]  [G loss: -9.909] \n",
      "1463 [D loss: (2.052)(R -9.399, F 13.503)]  [G loss: -12.246] \n",
      "1463 [D loss: (2.345)(R -9.003, F 13.692)]  [G loss: -9.298] \n",
      "1464 [D loss: (1.478)(R -9.332, F 12.289)]  [G loss: -10.792] \n",
      "1464 [D loss: (1.912)(R -8.863, F 12.688)]  [G loss: -11.855] \n",
      "1465 [D loss: (1.485)(R -8.235, F 11.205)]  [G loss: -11.005] \n",
      "1465 [D loss: (1.268)(R -10.023, F 12.559)]  [G loss: -9.697] \n",
      "1466 [D loss: (0.839)(R -10.962, F 12.640)]  [G loss: -9.706] \n",
      "1466 [D loss: (0.262)(R -11.430, F 11.954)]  [G loss: -11.271] \n",
      "1467 [D loss: (-0.050)(R -9.989, F 9.889)]  [G loss: -10.726] \n",
      "1467 [D loss: (1.281)(R -9.559, F 12.121)]  [G loss: -12.388] \n",
      "1468 [D loss: (1.526)(R -10.621, F 13.673)]  [G loss: -12.182] \n",
      "1468 [D loss: (-0.701)(R -8.625, F 7.223)]  [G loss: -9.926] \n",
      "1469 [D loss: (0.943)(R -9.431, F 11.317)]  [G loss: -9.753] \n",
      "1469 [D loss: (2.024)(R -9.277, F 13.324)]  [G loss: -11.462] \n",
      "1470 [D loss: (0.013)(R -9.338, F 9.363)]  [G loss: -10.477] \n",
      "1470 [D loss: (1.920)(R -9.421, F 13.260)]  [G loss: -10.824] \n",
      "1471 [D loss: (1.043)(R -10.529, F 12.616)]  [G loss: -10.522] \n",
      "1471 [D loss: (0.413)(R -9.756, F 10.583)]  [G loss: -10.832] \n",
      "1472 [D loss: (2.165)(R -8.829, F 13.159)]  [G loss: -8.976] \n",
      "1472 [D loss: (1.236)(R -9.098, F 11.570)]  [G loss: -10.098] \n",
      "1473 [D loss: (0.698)(R -10.825, F 12.221)]  [G loss: -11.394] \n",
      "1473 [D loss: (1.709)(R -8.405, F 11.823)]  [G loss: -10.166] \n",
      "1474 [D loss: (0.398)(R -9.570, F 10.367)]  [G loss: -11.333] \n",
      "1474 [D loss: (-0.835)(R -12.406, F 10.737)]  [G loss: -11.202] \n",
      "1475 [D loss: (0.548)(R -10.098, F 11.194)]  [G loss: -9.970] \n",
      "1475 [D loss: (0.479)(R -10.894, F 11.851)]  [G loss: -10.167] \n",
      "1476 [D loss: (0.949)(R -9.890, F 11.788)]  [G loss: -9.402] \n",
      "1476 [D loss: (-0.694)(R -10.382, F 8.994)]  [G loss: -10.568] \n",
      "1477 [D loss: (0.424)(R -10.485, F 11.333)]  [G loss: -12.294] \n",
      "1477 [D loss: (-0.695)(R -10.134, F 8.745)]  [G loss: -12.191] \n",
      "1478 [D loss: (0.985)(R -10.337, F 12.308)]  [G loss: -10.571] \n",
      "1478 [D loss: (1.262)(R -9.061, F 11.584)]  [G loss: -12.022] \n",
      "1479 [D loss: (0.785)(R -9.324, F 10.895)]  [G loss: -10.760] \n",
      "1479 [D loss: (-0.259)(R -10.314, F 9.796)]  [G loss: -11.282] \n",
      "1480 [D loss: (1.504)(R -10.413, F 13.421)]  [G loss: -12.070] \n",
      "1480 [D loss: (-1.227)(R -12.112, F 9.659)]  [G loss: -10.497] \n",
      "1481 [D loss: (0.397)(R -11.074, F 11.868)]  [G loss: -11.047] \n",
      "1481 [D loss: (0.007)(R -11.029, F 11.042)]  [G loss: -11.479] \n",
      "1482 [D loss: (0.704)(R -10.768, F 12.177)]  [G loss: -11.893] \n",
      "1482 [D loss: (0.173)(R -11.075, F 11.422)]  [G loss: -11.344] \n",
      "1483 [D loss: (0.137)(R -10.517, F 10.792)]  [G loss: -11.929] \n",
      "1483 [D loss: (-0.265)(R -11.546, F 11.016)]  [G loss: -10.599] \n",
      "1484 [D loss: (-0.949)(R -10.359, F 8.462)]  [G loss: -11.720] \n",
      "1484 [D loss: (0.234)(R -10.670, F 11.138)]  [G loss: -10.582] \n",
      "1485 [D loss: (0.325)(R -9.324, F 9.975)]  [G loss: -12.552] \n",
      "1485 [D loss: (-1.420)(R -12.206, F 9.366)]  [G loss: -10.842] \n",
      "1486 [D loss: (0.051)(R -10.903, F 11.005)]  [G loss: -10.946] \n",
      "1486 [D loss: (-0.427)(R -10.496, F 9.642)]  [G loss: -10.431] \n",
      "1487 [D loss: (-0.218)(R -11.494, F 11.058)]  [G loss: -12.068] \n",
      "1487 [D loss: (-0.562)(R -11.525, F 10.401)]  [G loss: -10.676] \n",
      "1488 [D loss: (-0.116)(R -9.909, F 9.676)]  [G loss: -11.522] \n",
      "1488 [D loss: (-0.263)(R -11.300, F 10.774)]  [G loss: -10.537] \n",
      "1489 [D loss: (-0.556)(R -10.888, F 9.776)]  [G loss: -10.915] \n",
      "1489 [D loss: (0.090)(R -10.546, F 10.726)]  [G loss: -11.611] \n",
      "1490 [D loss: (-0.427)(R -10.362, F 9.507)]  [G loss: -11.043] \n",
      "1490 [D loss: (0.634)(R -9.721, F 10.989)]  [G loss: -11.155] \n",
      "1491 [D loss: (1.543)(R -10.286, F 13.371)]  [G loss: -10.875] \n",
      "1491 [D loss: (1.918)(R -9.144, F 12.979)]  [G loss: -10.822] \n",
      "1492 [D loss: (0.709)(R -10.931, F 12.348)]  [G loss: -12.018] \n",
      "1492 [D loss: (0.333)(R -10.572, F 11.238)]  [G loss: -10.683] \n",
      "1493 [D loss: (1.222)(R -9.634, F 12.078)]  [G loss: -10.718] \n",
      "1493 [D loss: (0.809)(R -8.617, F 10.234)]  [G loss: -9.909] \n",
      "1494 [D loss: (0.148)(R -10.441, F 10.736)]  [G loss: -9.617] \n",
      "1494 [D loss: (0.016)(R -9.921, F 9.953)]  [G loss: -10.735] \n",
      "1495 [D loss: (0.019)(R -10.707, F 10.745)]  [G loss: -9.822] \n",
      "1495 [D loss: (0.221)(R -9.306, F 9.747)]  [G loss: -9.495] \n",
      "1496 [D loss: (1.241)(R -9.151, F 11.633)]  [G loss: -10.077] \n",
      "1496 [D loss: (0.532)(R -9.165, F 10.228)]  [G loss: -10.637] \n",
      "1497 [D loss: (1.554)(R -8.373, F 11.481)]  [G loss: -11.854] \n",
      "1497 [D loss: (0.767)(R -9.646, F 11.180)]  [G loss: -9.609] \n",
      "1498 [D loss: (-0.159)(R -11.105, F 10.787)]  [G loss: -8.641] \n",
      "1498 [D loss: (-1.249)(R -11.197, F 8.698)]  [G loss: -8.256] \n",
      "1499 [D loss: (-0.477)(R -9.873, F 8.920)]  [G loss: -9.533] \n",
      "1499 [D loss: (0.371)(R -8.658, F 9.401)]  [G loss: -9.790] \n",
      "1500 [D loss: (0.182)(R -9.346, F 9.709)]  [G loss: -9.180] \n",
      "1500 [D loss: (0.804)(R -7.272, F 8.879)]  [G loss: -8.976] \n",
      "1501 [D loss: (-0.376)(R -9.521, F 8.769)]  [G loss: -8.671] \n",
      "1501 [D loss: (-0.020)(R -8.874, F 8.835)]  [G loss: -10.271] \n",
      "1502 [D loss: (0.208)(R -8.686, F 9.102)]  [G loss: -9.840] \n",
      "1502 [D loss: (0.009)(R -8.677, F 8.695)]  [G loss: -8.969] \n",
      "1503 [D loss: (-0.013)(R -8.626, F 8.601)]  [G loss: -8.939] \n",
      "1503 [D loss: (1.249)(R -6.991, F 9.489)]  [G loss: -9.575] \n",
      "1504 [D loss: (-0.047)(R -8.707, F 8.612)]  [G loss: -9.376] \n",
      "1504 [D loss: (0.560)(R -7.236, F 8.357)]  [G loss: -7.769] \n",
      "1505 [D loss: (0.260)(R -8.464, F 8.983)]  [G loss: -7.302] \n",
      "1505 [D loss: (0.692)(R -6.943, F 8.327)]  [G loss: -8.775] \n",
      "1506 [D loss: (0.256)(R -7.817, F 8.329)]  [G loss: -8.116] \n",
      "1506 [D loss: (0.575)(R -7.236, F 8.386)]  [G loss: -9.172] \n",
      "1507 [D loss: (-0.039)(R -7.480, F 7.403)]  [G loss: -8.028] \n",
      "1507 [D loss: (1.348)(R -6.221, F 8.917)]  [G loss: -7.388] \n",
      "1508 [D loss: (0.203)(R -8.574, F 8.980)]  [G loss: -7.842] \n",
      "1508 [D loss: (0.526)(R -7.305, F 8.357)]  [G loss: -7.502] \n",
      "1509 [D loss: (-0.262)(R -7.498, F 6.973)]  [G loss: -8.290] \n",
      "1509 [D loss: (0.131)(R -7.700, F 7.962)]  [G loss: -7.511] \n",
      "1510 [D loss: (-0.351)(R -7.652, F 6.950)]  [G loss: -6.889] \n",
      "1510 [D loss: (0.200)(R -6.713, F 7.113)]  [G loss: -7.771] \n",
      "1511 [D loss: (1.029)(R -6.045, F 8.104)]  [G loss: -6.631] \n",
      "1511 [D loss: (0.871)(R -6.266, F 8.007)]  [G loss: -7.768] \n",
      "1512 [D loss: (-0.114)(R -6.534, F 6.306)]  [G loss: -7.030] \n",
      "1512 [D loss: (-0.059)(R -6.994, F 6.876)]  [G loss: -7.634] \n",
      "1513 [D loss: (0.795)(R -5.602, F 7.192)]  [G loss: -6.675] \n",
      "1513 [D loss: (-0.012)(R -6.718, F 6.693)]  [G loss: -7.091] \n",
      "1514 [D loss: (0.582)(R -5.793, F 6.957)]  [G loss: -6.565] \n",
      "1514 [D loss: (0.218)(R -7.291, F 7.726)]  [G loss: -5.939] \n",
      "1515 [D loss: (0.442)(R -5.530, F 6.415)]  [G loss: -7.490] \n",
      "1515 [D loss: (0.272)(R -6.145, F 6.689)]  [G loss: -7.148] \n",
      "1516 [D loss: (0.196)(R -5.385, F 5.776)]  [G loss: -5.649] \n",
      "1516 [D loss: (0.221)(R -5.397, F 5.839)]  [G loss: -5.533] \n",
      "1517 [D loss: (0.897)(R -4.344, F 6.139)]  [G loss: -5.518] \n",
      "1517 [D loss: (-0.000)(R -5.518, F 5.518)]  [G loss: -5.950] \n",
      "1518 [D loss: (0.222)(R -5.156, F 5.600)]  [G loss: -6.194] \n",
      "1518 [D loss: (0.523)(R -4.940, F 5.986)]  [G loss: -5.692] \n",
      "1519 [D loss: (0.192)(R -4.977, F 5.360)]  [G loss: -4.897] \n",
      "1519 [D loss: (0.534)(R -5.059, F 6.128)]  [G loss: -5.455] \n",
      "1520 [D loss: (0.228)(R -4.617, F 5.073)]  [G loss: -4.781] \n",
      "1520 [D loss: (0.172)(R -4.175, F 4.518)]  [G loss: -4.474] \n",
      "1521 [D loss: (-0.464)(R -4.994, F 4.067)]  [G loss: -5.095] \n",
      "1521 [D loss: (-0.311)(R -5.035, F 4.414)]  [G loss: -4.977] \n",
      "1522 [D loss: (0.659)(R -3.887, F 5.206)]  [G loss: -5.579] \n",
      "1522 [D loss: (-0.056)(R -4.033, F 3.920)]  [G loss: -4.452] \n",
      "1523 [D loss: (-0.442)(R -5.021, F 4.138)]  [G loss: -4.525] \n",
      "1523 [D loss: (0.247)(R -3.493, F 3.986)]  [G loss: -3.747] \n",
      "1524 [D loss: (0.438)(R -3.294, F 4.171)]  [G loss: -3.811] \n",
      "1524 [D loss: (0.129)(R -3.666, F 3.923)]  [G loss: -4.874] \n",
      "1525 [D loss: (0.967)(R -2.989, F 4.922)]  [G loss: -4.234] \n",
      "1525 [D loss: (0.335)(R -2.469, F 3.138)]  [G loss: -3.683] \n",
      "1526 [D loss: (-0.884)(R -3.817, F 2.048)]  [G loss: -2.995] \n",
      "1526 [D loss: (-0.793)(R -3.949, F 2.363)]  [G loss: -3.738] \n",
      "1527 [D loss: (1.188)(R -1.942, F 4.319)]  [G loss: -3.962] \n",
      "1527 [D loss: (0.877)(R -2.087, F 3.841)]  [G loss: -3.339] \n",
      "1528 [D loss: (-0.323)(R -3.143, F 2.498)]  [G loss: -4.073] \n",
      "1528 [D loss: (0.226)(R -2.591, F 3.043)]  [G loss: -2.812] \n",
      "1529 [D loss: (0.683)(R -2.324, F 3.691)]  [G loss: -4.295] \n",
      "1529 [D loss: (0.055)(R -1.623, F 1.732)]  [G loss: -3.001] \n",
      "1530 [D loss: (0.611)(R -1.260, F 2.482)]  [G loss: -2.050] \n",
      "1530 [D loss: (0.812)(R -1.606, F 3.230)]  [G loss: -1.521] \n",
      "1531 [D loss: (0.648)(R -0.802, F 2.098)]  [G loss: -3.999] \n",
      "1531 [D loss: (0.769)(R -1.163, F 2.702)]  [G loss: -2.142] \n",
      "1532 [D loss: (0.026)(R -2.093, F 2.144)]  [G loss: -2.335] \n",
      "1532 [D loss: (-0.188)(R -2.056, F 1.680)]  [G loss: -2.327] \n",
      "1533 [D loss: (0.367)(R -2.063, F 2.797)]  [G loss: -2.635] \n",
      "1533 [D loss: (0.140)(R -1.957, F 2.236)]  [G loss: -1.534] \n",
      "1534 [D loss: (-0.155)(R -2.358, F 2.049)]  [G loss: -3.099] \n",
      "1534 [D loss: (1.031)(R -0.881, F 2.943)]  [G loss: -1.701] \n",
      "1535 [D loss: (0.042)(R -1.402, F 1.487)]  [G loss: -1.806] \n",
      "1535 [D loss: (0.094)(R -1.647, F 1.834)]  [G loss: -1.979] \n",
      "1536 [D loss: (0.971)(R 0.015, F 1.927)]  [G loss: -1.354] \n",
      "1536 [D loss: (0.944)(R -0.537, F 2.424)]  [G loss: 0.079] \n",
      "1537 [D loss: (0.522)(R -1.088, F 2.132)]  [G loss: -0.825] \n",
      "1537 [D loss: (0.544)(R -0.216, F 1.303)]  [G loss: -0.068] \n",
      "1538 [D loss: (0.460)(R -0.255, F 1.175)]  [G loss: -1.195] \n",
      "1538 [D loss: (0.558)(R -0.163, F 1.279)]  [G loss: -0.220] \n",
      "1539 [D loss: (0.581)(R 0.392, F 0.769)]  [G loss: -0.092] \n",
      "1539 [D loss: (0.763)(R 0.473, F 1.052)]  [G loss: -0.328] \n",
      "1540 [D loss: (-0.038)(R -0.231, F 0.154)]  [G loss: -0.696] \n",
      "1540 [D loss: (0.077)(R 0.276, F -0.122)]  [G loss: 0.357] \n",
      "1541 [D loss: (-0.385)(R -0.007, F -0.763)]  [G loss: -0.418] \n",
      "1541 [D loss: (0.084)(R 1.059, F -0.890)]  [G loss: 0.114] \n",
      "1542 [D loss: (0.146)(R 0.873, F -0.581)]  [G loss: 0.307] \n",
      "1542 [D loss: (0.498)(R 0.931, F 0.065)]  [G loss: 0.174] \n",
      "1543 [D loss: (-0.251)(R -0.281, F -0.221)]  [G loss: 0.723] \n",
      "1543 [D loss: (0.450)(R 1.574, F -0.675)]  [G loss: 0.479] \n",
      "1544 [D loss: (0.907)(R 1.746, F 0.069)]  [G loss: 0.940] \n",
      "1544 [D loss: (-0.047)(R 1.574, F -1.668)]  [G loss: 1.375] \n",
      "1545 [D loss: (0.319)(R 1.671, F -1.033)]  [G loss: 1.093] \n",
      "1545 [D loss: (-0.086)(R 1.571, F -1.744)]  [G loss: 0.866] \n",
      "1546 [D loss: (-0.141)(R 1.052, F -1.334)]  [G loss: 0.702] \n",
      "1546 [D loss: (0.247)(R 1.684, F -1.190)]  [G loss: 2.038] \n",
      "1547 [D loss: (0.231)(R 1.803, F -1.340)]  [G loss: 2.066] \n",
      "1547 [D loss: (0.206)(R 2.095, F -1.683)]  [G loss: 1.569] \n",
      "1548 [D loss: (-0.017)(R 1.829, F -1.863)]  [G loss: 2.397] \n",
      "1548 [D loss: (0.102)(R 2.393, F -2.190)]  [G loss: 2.683] \n",
      "1549 [D loss: (0.241)(R 2.428, F -1.947)]  [G loss: 2.466] \n",
      "1549 [D loss: (0.241)(R 3.009, F -2.527)]  [G loss: 2.497] \n",
      "1550 [D loss: (-0.331)(R 2.376, F -3.038)]  [G loss: 2.837] \n",
      "1550 [D loss: (0.015)(R 3.245, F -3.215)]  [G loss: 3.153] \n",
      "1551 [D loss: (0.448)(R 3.083, F -2.188)]  [G loss: 2.318] \n",
      "1551 [D loss: (0.139)(R 2.625, F -2.348)]  [G loss: 3.300] \n",
      "1552 [D loss: (0.186)(R 3.471, F -3.099)]  [G loss: 2.653] \n",
      "1552 [D loss: (-0.134)(R 3.373, F -3.641)]  [G loss: 2.754] \n",
      "1553 [D loss: (0.051)(R 2.938, F -2.836)]  [G loss: 3.084] \n",
      "1553 [D loss: (0.209)(R 3.858, F -3.440)]  [G loss: 3.409] \n",
      "1554 [D loss: (0.368)(R 4.202, F -3.465)]  [G loss: 4.072] \n",
      "1554 [D loss: (-0.417)(R 2.821, F -3.654)]  [G loss: 4.095] \n",
      "1555 [D loss: (-0.208)(R 3.873, F -4.289)]  [G loss: 4.549] \n",
      "1555 [D loss: (-0.281)(R 3.782, F -4.345)]  [G loss: 4.618] \n",
      "1556 [D loss: (0.345)(R 4.351, F -3.660)]  [G loss: 4.461] \n",
      "1556 [D loss: (-0.462)(R 4.040, F -4.965)]  [G loss: 4.486] \n",
      "1557 [D loss: (-0.034)(R 4.168, F -4.236)]  [G loss: 4.771] \n",
      "1557 [D loss: (-0.352)(R 3.871, F -4.576)]  [G loss: 4.795] \n",
      "1558 [D loss: (-0.111)(R 3.685, F -3.907)]  [G loss: 4.447] \n",
      "1558 [D loss: (0.183)(R 4.664, F -4.298)]  [G loss: 4.992] \n",
      "1559 [D loss: (-0.171)(R 4.417, F -4.758)]  [G loss: 5.503] \n",
      "1559 [D loss: (-0.582)(R 4.434, F -5.598)]  [G loss: 4.485] \n",
      "1560 [D loss: (-0.549)(R 4.331, F -5.429)]  [G loss: 5.155] \n",
      "1560 [D loss: (0.154)(R 5.705, F -5.397)]  [G loss: 4.863] \n",
      "1561 [D loss: (-0.128)(R 5.106, F -5.361)]  [G loss: 5.325] \n",
      "1561 [D loss: (0.166)(R 5.365, F -5.034)]  [G loss: 6.173] \n",
      "1562 [D loss: (-0.291)(R 5.162, F -5.744)]  [G loss: 5.923] \n",
      "1562 [D loss: (-0.335)(R 6.122, F -6.792)]  [G loss: 5.568] \n",
      "1563 [D loss: (-0.796)(R 4.950, F -6.543)]  [G loss: 6.677] \n",
      "1563 [D loss: (-0.530)(R 5.759, F -6.819)]  [G loss: 6.379] \n",
      "1564 [D loss: (-0.778)(R 5.909, F -7.466)]  [G loss: 7.567] \n",
      "1564 [D loss: (-0.451)(R 6.044, F -6.945)]  [G loss: 6.693] \n",
      "1565 [D loss: (-0.767)(R 5.707, F -7.241)]  [G loss: 7.016] \n",
      "1565 [D loss: (-0.774)(R 6.018, F -7.565)]  [G loss: 7.170] \n",
      "1566 [D loss: (-0.893)(R 5.709, F -7.496)]  [G loss: 7.135] \n",
      "1566 [D loss: (-0.796)(R 6.495, F -8.086)]  [G loss: 7.543] \n",
      "1567 [D loss: (-0.249)(R 6.922, F -7.419)]  [G loss: 7.316] \n",
      "1567 [D loss: (-0.276)(R 6.740, F -7.293)]  [G loss: 7.415] \n",
      "1568 [D loss: (-0.843)(R 6.698, F -8.384)]  [G loss: 8.142] \n",
      "1568 [D loss: (-0.691)(R 7.385, F -8.767)]  [G loss: 8.454] \n",
      "1569 [D loss: (-0.600)(R 7.327, F -8.526)]  [G loss: 7.840] \n",
      "1569 [D loss: (-0.955)(R 6.699, F -8.609)]  [G loss: 8.550] \n",
      "1570 [D loss: (-0.700)(R 7.818, F -9.217)]  [G loss: 8.806] \n",
      "1570 [D loss: (-0.247)(R 7.809, F -8.304)]  [G loss: 9.536] \n",
      "1571 [D loss: (-1.301)(R 7.729, F -10.332)]  [G loss: 9.631] \n",
      "1571 [D loss: (-1.177)(R 7.719, F -10.074)]  [G loss: 9.418] \n",
      "1572 [D loss: (-1.311)(R 7.371, F -9.994)]  [G loss: 10.054] \n",
      "1572 [D loss: (-1.230)(R 7.548, F -10.008)]  [G loss: 9.758] \n",
      "1573 [D loss: (-0.974)(R 8.223, F -10.171)]  [G loss: 10.701] \n",
      "1573 [D loss: (-1.128)(R 7.845, F -10.101)]  [G loss: 10.698] \n",
      "1574 [D loss: (-0.704)(R 8.823, F -10.231)]  [G loss: 10.615] \n",
      "1574 [D loss: (-1.068)(R 8.853, F -10.988)]  [G loss: 11.054] \n",
      "1575 [D loss: (-1.140)(R 8.893, F -11.172)]  [G loss: 10.900] \n",
      "1575 [D loss: (-0.870)(R 9.610, F -11.350)]  [G loss: 11.912] \n",
      "1576 [D loss: (-1.264)(R 9.175, F -11.703)]  [G loss: 11.811] \n",
      "1576 [D loss: (-0.977)(R 10.171, F -12.125)]  [G loss: 12.161] \n",
      "1577 [D loss: (-1.172)(R 9.976, F -12.319)]  [G loss: 12.112] \n",
      "1577 [D loss: (-1.393)(R 9.703, F -12.488)]  [G loss: 11.700] \n",
      "1578 [D loss: (-1.205)(R 10.575, F -12.985)]  [G loss: 13.021] \n",
      "1578 [D loss: (-1.146)(R 10.790, F -13.081)]  [G loss: 13.451] \n",
      "1579 [D loss: (-1.143)(R 10.742, F -13.027)]  [G loss: 13.778] \n",
      "1579 [D loss: (-0.766)(R 11.788, F -13.320)]  [G loss: 14.178] \n",
      "1580 [D loss: (-1.364)(R 11.408, F -14.136)]  [G loss: 14.649] \n",
      "1580 [D loss: (-1.894)(R 10.881, F -14.669)]  [G loss: 14.775] \n",
      "1581 [D loss: (-1.728)(R 11.654, F -15.111)]  [G loss: 15.261] \n",
      "1581 [D loss: (-1.077)(R 12.024, F -14.177)]  [G loss: 15.420] \n",
      "1582 [D loss: (-1.578)(R 12.389, F -15.545)]  [G loss: 16.155] \n",
      "1582 [D loss: (-1.772)(R 12.529, F -16.073)]  [G loss: 16.473] \n",
      "1583 [D loss: (-2.167)(R 12.512, F -16.846)]  [G loss: 16.969] \n",
      "1583 [D loss: (-1.818)(R 13.591, F -17.227)]  [G loss: 17.458] \n",
      "1584 [D loss: (-2.056)(R 14.273, F -18.384)]  [G loss: 17.741] \n",
      "1584 [D loss: (-1.887)(R 14.317, F -18.092)]  [G loss: 17.916] \n",
      "1585 [D loss: (-2.513)(R 14.082, F -19.107)]  [G loss: 18.324] \n",
      "1585 [D loss: (-2.058)(R 15.291, F -19.407)]  [G loss: 18.834] \n",
      "1586 [D loss: (-2.557)(R 14.860, F -19.975)]  [G loss: 20.586] \n",
      "1586 [D loss: (-2.190)(R 15.678, F -20.058)]  [G loss: 20.512] \n",
      "1587 [D loss: (-2.489)(R 16.055, F -21.033)]  [G loss: 20.974] \n",
      "1587 [D loss: (-2.395)(R 16.388, F -21.177)]  [G loss: 20.820] \n",
      "1588 [D loss: (-2.816)(R 16.704, F -22.336)]  [G loss: 21.890] \n",
      "1588 [D loss: (-2.526)(R 17.414, F -22.466)]  [G loss: 23.490] \n",
      "1589 [D loss: (-2.808)(R 17.871, F -23.486)]  [G loss: 23.954] \n",
      "1589 [D loss: (-2.813)(R 17.748, F -23.374)]  [G loss: 24.473] \n",
      "1590 [D loss: (-3.467)(R 17.817, F -24.751)]  [G loss: 24.928] \n",
      "1590 [D loss: (-3.358)(R 19.188, F -25.904)]  [G loss: 25.603] \n",
      "1591 [D loss: (-2.853)(R 19.909, F -25.614)]  [G loss: 26.942] \n",
      "1591 [D loss: (-4.181)(R 20.430, F -28.792)]  [G loss: 27.739] \n",
      "1592 [D loss: (-3.457)(R 21.302, F -28.215)]  [G loss: 28.870] \n",
      "1592 [D loss: (-3.700)(R 21.770, F -29.170)]  [G loss: 28.912] \n",
      "1593 [D loss: (-3.388)(R 23.174, F -29.950)]  [G loss: 30.207] \n",
      "1593 [D loss: (-3.472)(R 23.425, F -30.370)]  [G loss: 31.520] \n",
      "1594 [D loss: (-3.384)(R 23.981, F -30.749)]  [G loss: 31.912] \n",
      "1594 [D loss: (-4.221)(R 24.526, F -32.968)]  [G loss: 33.102] \n",
      "1595 [D loss: (-3.842)(R 25.783, F -33.467)]  [G loss: 34.578] \n",
      "1595 [D loss: (-4.196)(R 26.127, F -34.519)]  [G loss: 34.428] \n",
      "1596 [D loss: (-5.062)(R 26.435, F -36.560)]  [G loss: 37.262] \n",
      "1596 [D loss: (-4.440)(R 28.299, F -37.180)]  [G loss: 37.755] \n",
      "1597 [D loss: (-4.886)(R 29.492, F -39.264)]  [G loss: 39.834] \n",
      "1597 [D loss: (-5.114)(R 30.951, F -41.178)]  [G loss: 40.229] \n",
      "1598 [D loss: (-5.544)(R 31.604, F -42.693)]  [G loss: 41.580] \n",
      "1598 [D loss: (-5.842)(R 32.168, F -43.852)]  [G loss: 43.743] \n",
      "1599 [D loss: (-6.254)(R 33.327, F -45.836)]  [G loss: 45.582] \n",
      "1599 [D loss: (-5.882)(R 35.823, F -47.587)]  [G loss: 46.522] \n",
      "1600 [D loss: (-5.498)(R 37.374, F -48.371)]  [G loss: 48.999] \n",
      "1600 [D loss: (-6.528)(R 37.542, F -50.598)]  [G loss: 50.331] \n",
      "1601 [D loss: (-6.934)(R 38.936, F -52.804)]  [G loss: 53.324] \n",
      "1601 [D loss: (-7.486)(R 40.286, F -55.257)]  [G loss: 54.063] \n",
      "1602 [D loss: (-6.867)(R 42.716, F -56.450)]  [G loss: 56.517] \n",
      "1602 [D loss: (-7.727)(R 43.088, F -58.542)]  [G loss: 57.865] \n",
      "1603 [D loss: (-7.589)(R 44.816, F -59.994)]  [G loss: 60.929] \n",
      "1603 [D loss: (-8.918)(R 45.128, F -62.963)]  [G loss: 62.571] \n",
      "1604 [D loss: (-8.059)(R 47.527, F -63.646)]  [G loss: 63.813] \n",
      "1604 [D loss: (-9.438)(R 47.950, F -66.827)]  [G loss: 66.973] \n",
      "1605 [D loss: (-7.674)(R 52.061, F -67.410)]  [G loss: 69.894] \n",
      "1605 [D loss: (-9.207)(R 53.816, F -72.230)]  [G loss: 72.043] \n",
      "1606 [D loss: (-9.303)(R 54.577, F -73.183)]  [G loss: 73.990] \n",
      "1606 [D loss: (-9.678)(R 55.569, F -74.925)]  [G loss: 76.632] \n",
      "1607 [D loss: (-10.743)(R 56.272, F -77.758)]  [G loss: 79.042] \n",
      "1607 [D loss: (-10.875)(R 59.184, F -80.934)]  [G loss: 80.720] \n",
      "1608 [D loss: (-9.558)(R 63.188, F -82.304)]  [G loss: 84.600] \n",
      "1608 [D loss: (-12.054)(R 63.954, F -88.062)]  [G loss: 86.543] \n",
      "1609 [D loss: (-9.534)(R 67.881, F -86.948)]  [G loss: 88.937] \n",
      "1609 [D loss: (-10.311)(R 71.251, F -91.873)]  [G loss: 91.073] \n",
      "1610 [D loss: (-10.214)(R 73.413, F -93.841)]  [G loss: 93.558] \n",
      "1610 [D loss: (-11.016)(R 74.429, F -96.461)]  [G loss: 98.961] \n",
      "1611 [D loss: (-11.520)(R 75.753, F -98.793)]  [G loss: 99.626] \n",
      "1611 [D loss: (-10.849)(R 80.273, F -101.971)]  [G loss: 104.738] \n",
      "1612 [D loss: (-11.147)(R 84.553, F -106.846)]  [G loss: 105.623] \n",
      "1612 [D loss: (-14.022)(R 84.947, F -112.991)]  [G loss: 114.463] \n",
      "1613 [D loss: (-10.869)(R 90.343, F -112.081)]  [G loss: 116.280] \n",
      "1613 [D loss: (-12.398)(R 92.131, F -116.926)]  [G loss: 115.943] \n",
      "1614 [D loss: (-12.754)(R 95.214, F -120.722)]  [G loss: 122.643] \n",
      "1614 [D loss: (-11.486)(R 95.763, F -118.736)]  [G loss: 123.108] \n",
      "1615 [D loss: (-13.900)(R 97.200, F -125.000)]  [G loss: 123.818] \n",
      "1615 [D loss: (-12.319)(R 103.167, F -127.805)]  [G loss: 129.710] \n",
      "1616 [D loss: (-14.706)(R 101.587, F -130.998)]  [G loss: 128.745] \n",
      "1616 [D loss: (-15.482)(R 101.260, F -132.224)]  [G loss: 134.411] \n",
      "1617 [D loss: (-14.287)(R 106.568, F -135.142)]  [G loss: 136.402] \n",
      "1617 [D loss: (-12.963)(R 108.694, F -134.621)]  [G loss: 140.383] \n",
      "1618 [D loss: (-13.338)(R 112.283, F -138.959)]  [G loss: 143.942] \n",
      "1618 [D loss: (-17.128)(R 109.617, F -143.873)]  [G loss: 142.504] \n",
      "1619 [D loss: (-12.469)(R 117.640, F -142.579)]  [G loss: 146.705] \n",
      "1619 [D loss: (-14.796)(R 114.510, F -144.102)]  [G loss: 146.344] \n",
      "1620 [D loss: (-16.756)(R 116.406, F -149.918)]  [G loss: 152.423] \n",
      "1620 [D loss: (-12.924)(R 124.377, F -150.226)]  [G loss: 152.304] \n",
      "1621 [D loss: (-15.948)(R 121.864, F -153.760)]  [G loss: 153.776] \n",
      "1621 [D loss: (-17.379)(R 121.309, F -156.066)]  [G loss: 151.665] \n",
      "1622 [D loss: (-16.037)(R 126.308, F -158.382)]  [G loss: 159.332] \n",
      "1622 [D loss: (-15.102)(R 128.478, F -158.683)]  [G loss: 160.476] \n",
      "1623 [D loss: (-14.898)(R 129.865, F -159.660)]  [G loss: 161.319] \n",
      "1623 [D loss: (-16.216)(R 132.904, F -165.335)]  [G loss: 158.066] \n",
      "1624 [D loss: (-13.956)(R 134.432, F -162.344)]  [G loss: 166.579] \n",
      "1624 [D loss: (-15.577)(R 136.444, F -167.598)]  [G loss: 170.418] \n",
      "1625 [D loss: (-14.353)(R 141.176, F -169.882)]  [G loss: 169.794] \n",
      "1625 [D loss: (-14.316)(R 140.956, F -169.588)]  [G loss: 170.073] \n",
      "1626 [D loss: (-12.827)(R 143.167, F -168.821)]  [G loss: 172.838] \n",
      "1626 [D loss: (-15.881)(R 142.067, F -173.828)]  [G loss: 171.611] \n",
      "1627 [D loss: (-14.764)(R 144.194, F -173.722)]  [G loss: 177.652] \n",
      "1627 [D loss: (-12.802)(R 150.103, F -175.706)]  [G loss: 169.365] \n",
      "1628 [D loss: (-12.114)(R 153.432, F -177.660)]  [G loss: 172.357] \n",
      "1628 [D loss: (-14.541)(R 146.555, F -175.636)]  [G loss: 171.358] \n",
      "1629 [D loss: (-11.796)(R 150.700, F -174.292)]  [G loss: 180.836] \n",
      "1629 [D loss: (-11.251)(R 154.768, F -177.270)]  [G loss: 174.817] \n",
      "1630 [D loss: (-11.947)(R 148.566, F -172.459)]  [G loss: 174.536] \n",
      "1630 [D loss: (-14.660)(R 149.695, F -179.016)]  [G loss: 178.832] \n",
      "1631 [D loss: (-11.970)(R 150.488, F -174.429)]  [G loss: 174.185] \n",
      "1631 [D loss: (-9.800)(R 151.536, F -171.136)]  [G loss: 176.594] \n",
      "1632 [D loss: (-11.637)(R 149.332, F -172.607)]  [G loss: 173.277] \n",
      "1632 [D loss: (-12.238)(R 152.637, F -177.113)]  [G loss: 175.618] \n",
      "1633 [D loss: (-10.160)(R 152.081, F -172.401)]  [G loss: 171.983] \n",
      "1633 [D loss: (-13.555)(R 152.678, F -179.788)]  [G loss: 171.734] \n",
      "1634 [D loss: (-11.471)(R 149.511, F -172.453)]  [G loss: 175.160] \n",
      "1634 [D loss: (-11.974)(R 151.888, F -175.837)]  [G loss: 170.649] \n",
      "1635 [D loss: (-11.759)(R 148.981, F -172.499)]  [G loss: 173.153] \n",
      "1635 [D loss: (-11.306)(R 154.222, F -176.834)]  [G loss: 169.233] \n",
      "1636 [D loss: (-13.761)(R 147.922, F -175.445)]  [G loss: 170.559] \n",
      "1636 [D loss: (-9.958)(R 148.101, F -168.018)]  [G loss: 171.424] \n",
      "1637 [D loss: (-9.617)(R 149.443, F -168.677)]  [G loss: 171.405] \n",
      "1637 [D loss: (-9.104)(R 154.468, F -172.676)]  [G loss: 167.590] \n",
      "1638 [D loss: (-8.648)(R 149.022, F -166.317)]  [G loss: 170.906] \n",
      "1638 [D loss: (-3.889)(R 154.685, F -162.463)]  [G loss: 166.716] \n",
      "1639 [D loss: (-1.578)(R 155.994, F -159.150)]  [G loss: 166.191] \n",
      "1639 [D loss: (-2.808)(R 154.646, F -160.262)]  [G loss: 164.069] \n",
      "1640 [D loss: (-8.673)(R 149.483, F -166.828)]  [G loss: 169.155] \n",
      "1640 [D loss: (-10.425)(R 150.140, F -170.990)]  [G loss: 169.307] \n",
      "1641 [D loss: (-11.836)(R 144.431, F -168.102)]  [G loss: 161.523] \n",
      "1641 [D loss: (-5.648)(R 153.147, F -164.443)]  [G loss: 159.984] \n",
      "1642 [D loss: (-0.290)(R 153.228, F -153.808)]  [G loss: 162.219] \n",
      "1642 [D loss: (-8.015)(R 148.373, F -164.402)]  [G loss: 166.898] \n",
      "1643 [D loss: (-4.041)(R 150.337, F -158.420)]  [G loss: 163.087] \n",
      "1643 [D loss: (-4.853)(R 148.700, F -158.405)]  [G loss: 158.941] \n",
      "1644 [D loss: (-3.809)(R 145.587, F -153.205)]  [G loss: 159.241] \n",
      "1644 [D loss: (-3.334)(R 145.533, F -152.201)]  [G loss: 151.246] \n",
      "1645 [D loss: (-2.312)(R 146.597, F -151.222)]  [G loss: 162.325] \n",
      "1645 [D loss: (-6.185)(R 146.798, F -159.168)]  [G loss: 152.925] \n",
      "1646 [D loss: (0.798)(R 150.248, F -148.652)]  [G loss: 152.101] \n",
      "1646 [D loss: (-8.549)(R 138.244, F -155.343)]  [G loss: 154.480] \n",
      "1647 [D loss: (-4.896)(R 140.300, F -150.092)]  [G loss: 153.446] \n",
      "1647 [D loss: (-2.178)(R 145.421, F -149.777)]  [G loss: 149.579] \n",
      "1648 [D loss: (-2.907)(R 144.044, F -149.857)]  [G loss: 150.181] \n",
      "1648 [D loss: (-1.619)(R 143.044, F -146.282)]  [G loss: 150.054] \n",
      "1649 [D loss: (2.752)(R 147.471, F -141.968)]  [G loss: 151.425] \n",
      "1649 [D loss: (-5.278)(R 138.141, F -148.696)]  [G loss: 150.917] \n",
      "1650 [D loss: (-4.583)(R 139.977, F -149.144)]  [G loss: 150.192] \n",
      "1650 [D loss: (-4.153)(R 142.824, F -151.130)]  [G loss: 150.974] \n",
      "1651 [D loss: (-4.034)(R 136.037, F -144.104)]  [G loss: 146.732] \n",
      "1651 [D loss: (-0.183)(R 138.003, F -138.368)]  [G loss: 143.270] \n",
      "1652 [D loss: (-1.451)(R 140.899, F -143.801)]  [G loss: 147.079] \n",
      "1652 [D loss: (0.639)(R 136.353, F -135.075)]  [G loss: 134.392] \n",
      "1653 [D loss: (-3.780)(R 134.853, F -142.413)]  [G loss: 135.100] \n",
      "1653 [D loss: (-2.271)(R 136.600, F -141.142)]  [G loss: 141.379] \n",
      "1654 [D loss: (-2.716)(R 138.501, F -143.932)]  [G loss: 138.584] \n",
      "1654 [D loss: (-1.283)(R 136.458, F -139.024)]  [G loss: 137.286] \n",
      "1655 [D loss: (-4.860)(R 131.451, F -141.171)]  [G loss: 140.170] \n",
      "1655 [D loss: (-0.748)(R 133.493, F -134.989)]  [G loss: 132.399] \n",
      "1656 [D loss: (-4.654)(R 131.328, F -140.636)]  [G loss: 139.956] \n",
      "1656 [D loss: (-1.534)(R 138.243, F -141.311)]  [G loss: 138.298] \n",
      "1657 [D loss: (-0.034)(R 135.835, F -135.903)]  [G loss: 133.552] \n",
      "1657 [D loss: (-3.639)(R 132.725, F -140.003)]  [G loss: 145.468] \n",
      "1658 [D loss: (-0.699)(R 132.087, F -133.485)]  [G loss: 141.995] \n",
      "1658 [D loss: (-3.327)(R 132.175, F -138.830)]  [G loss: 142.663] \n",
      "1659 [D loss: (1.141)(R 134.838, F -132.556)]  [G loss: 133.060] \n",
      "1659 [D loss: (-2.954)(R 136.168, F -142.077)]  [G loss: 137.774] \n",
      "1660 [D loss: (-4.055)(R 135.218, F -143.328)]  [G loss: 134.503] \n",
      "1660 [D loss: (-4.428)(R 141.808, F -150.664)]  [G loss: 133.400] \n",
      "1661 [D loss: (-4.585)(R 133.642, F -142.813)]  [G loss: 139.864] \n",
      "1661 [D loss: (-4.960)(R 136.142, F -146.061)]  [G loss: 138.500] \n",
      "1662 [D loss: (-1.947)(R 134.677, F -138.571)]  [G loss: 142.808] \n",
      "1662 [D loss: (-2.380)(R 130.854, F -135.614)]  [G loss: 146.132] \n",
      "1663 [D loss: (-2.475)(R 133.748, F -138.697)]  [G loss: 140.468] \n",
      "1663 [D loss: (-1.263)(R 133.693, F -136.220)]  [G loss: 141.560] \n",
      "1664 [D loss: (-1.184)(R 133.134, F -135.502)]  [G loss: 141.043] \n",
      "1664 [D loss: (-6.621)(R 129.856, F -143.098)]  [G loss: 140.894] \n",
      "1665 [D loss: (-7.211)(R 131.221, F -145.643)]  [G loss: 136.709] \n",
      "1665 [D loss: (-6.354)(R 129.727, F -142.435)]  [G loss: 137.240] \n",
      "1666 [D loss: (-3.526)(R 128.689, F -135.741)]  [G loss: 137.192] \n",
      "1666 [D loss: (-6.667)(R 125.885, F -139.219)]  [G loss: 142.003] \n",
      "1667 [D loss: (-3.510)(R 124.440, F -131.460)]  [G loss: 132.392] \n",
      "1667 [D loss: (-0.479)(R 127.844, F -128.801)]  [G loss: 136.958] \n",
      "1668 [D loss: (-2.390)(R 125.147, F -129.927)]  [G loss: 136.305] \n",
      "1668 [D loss: (-5.135)(R 123.265, F -133.535)]  [G loss: 132.081] \n",
      "1669 [D loss: (0.483)(R 123.812, F -122.847)]  [G loss: 138.865] \n",
      "1669 [D loss: (-7.451)(R 120.406, F -135.308)]  [G loss: 129.161] \n",
      "1670 [D loss: (-0.232)(R 116.328, F -116.792)]  [G loss: 125.254] \n",
      "1670 [D loss: (-2.545)(R 117.202, F -122.291)]  [G loss: 127.327] \n",
      "1671 [D loss: (-6.366)(R 116.817, F -129.550)]  [G loss: 125.634] \n",
      "1671 [D loss: (-2.802)(R 116.456, F -122.059)]  [G loss: 128.283] \n",
      "1672 [D loss: (-4.357)(R 116.076, F -124.790)]  [G loss: 128.608] \n",
      "1672 [D loss: (-3.364)(R 116.895, F -123.623)]  [G loss: 123.029] \n",
      "1673 [D loss: (-1.956)(R 111.054, F -114.966)]  [G loss: 121.961] \n",
      "1673 [D loss: (-3.759)(R 111.774, F -119.292)]  [G loss: 120.973] \n",
      "1674 [D loss: (-3.784)(R 112.107, F -119.674)]  [G loss: 115.115] \n",
      "1674 [D loss: (-3.729)(R 110.401, F -117.860)]  [G loss: 119.527] \n",
      "1675 [D loss: (-3.144)(R 107.444, F -113.731)]  [G loss: 115.973] \n",
      "1675 [D loss: (-6.532)(R 105.505, F -118.569)]  [G loss: 116.036] \n",
      "1676 [D loss: (-9.401)(R 102.187, F -120.989)]  [G loss: 117.395] \n",
      "1676 [D loss: (-7.904)(R 97.351, F -113.159)]  [G loss: 114.726] \n",
      "1677 [D loss: (-0.310)(R 106.800, F -107.421)]  [G loss: 110.101] \n",
      "1677 [D loss: (-9.183)(R 103.003, F -121.370)]  [G loss: 109.105] \n",
      "1678 [D loss: (-3.649)(R 101.943, F -109.240)]  [G loss: 104.908] \n",
      "1678 [D loss: (-4.142)(R 97.652, F -105.935)]  [G loss: 102.049] \n",
      "1679 [D loss: (-3.172)(R 99.076, F -105.420)]  [G loss: 106.293] \n",
      "1679 [D loss: (-6.872)(R 96.557, F -110.301)]  [G loss: 104.520] \n",
      "1680 [D loss: (-6.058)(R 96.918, F -109.033)]  [G loss: 103.586] \n",
      "1680 [D loss: (-2.716)(R 100.063, F -105.494)]  [G loss: 103.561] \n",
      "1681 [D loss: (-7.198)(R 87.364, F -101.761)]  [G loss: 99.140] \n",
      "1681 [D loss: (0.846)(R 93.048, F -91.357)]  [G loss: 99.889] \n",
      "1682 [D loss: (-10.371)(R 91.081, F -111.823)]  [G loss: 98.392] \n",
      "1682 [D loss: (-6.458)(R 85.214, F -98.129)]  [G loss: 97.646] \n",
      "1683 [D loss: (-2.221)(R 92.300, F -96.741)]  [G loss: 98.311] \n",
      "1683 [D loss: (-5.914)(R 82.845, F -94.673)]  [G loss: 99.193] \n",
      "1684 [D loss: (-8.221)(R 82.075, F -98.517)]  [G loss: 97.388] \n",
      "1684 [D loss: (-7.597)(R 82.687, F -97.881)]  [G loss: 91.777] \n",
      "1685 [D loss: (-3.253)(R 80.404, F -86.910)]  [G loss: 94.682] \n",
      "1685 [D loss: (-8.247)(R 81.303, F -97.797)]  [G loss: 92.092] \n",
      "1686 [D loss: (-6.592)(R 78.118, F -91.302)]  [G loss: 92.477] \n",
      "1686 [D loss: (-5.553)(R 81.177, F -92.283)]  [G loss: 89.324] \n",
      "1687 [D loss: (-7.665)(R 77.777, F -93.107)]  [G loss: 90.914] \n",
      "1687 [D loss: (-7.090)(R 79.102, F -93.281)]  [G loss: 95.341] \n",
      "1688 [D loss: (-6.122)(R 80.900, F -93.144)]  [G loss: 95.846] \n",
      "1688 [D loss: (-8.157)(R 73.142, F -89.457)]  [G loss: 88.922] \n",
      "1689 [D loss: (-4.983)(R 73.778, F -83.744)]  [G loss: 85.366] \n",
      "1689 [D loss: (-9.083)(R 71.113, F -89.279)]  [G loss: 92.905] \n",
      "1690 [D loss: (-3.948)(R 77.346, F -85.242)]  [G loss: 88.878] \n",
      "1690 [D loss: (-6.722)(R 72.867, F -86.311)]  [G loss: 86.473] \n",
      "1691 [D loss: (-6.233)(R 72.810, F -85.277)]  [G loss: 87.249] \n",
      "1691 [D loss: (-8.130)(R 68.525, F -84.785)]  [G loss: 84.795] \n",
      "1692 [D loss: (-10.204)(R 67.441, F -87.848)]  [G loss: 87.763] \n",
      "1692 [D loss: (-8.022)(R 73.115, F -89.158)]  [G loss: 76.851] \n",
      "1693 [D loss: (-8.458)(R 68.167, F -85.083)]  [G loss: 80.238] \n",
      "1693 [D loss: (-8.334)(R 65.449, F -82.118)]  [G loss: 81.146] \n",
      "1694 [D loss: (-5.583)(R 73.139, F -84.305)]  [G loss: 86.725] \n",
      "1694 [D loss: (-10.578)(R 67.407, F -88.562)]  [G loss: 80.774] \n",
      "1695 [D loss: (-4.149)(R 70.989, F -79.287)]  [G loss: 80.386] \n",
      "1695 [D loss: (-3.147)(R 70.910, F -77.204)]  [G loss: 79.810] \n",
      "1696 [D loss: (-6.511)(R 67.328, F -80.351)]  [G loss: 81.040] \n",
      "1696 [D loss: (-7.766)(R 62.521, F -78.054)]  [G loss: 76.162] \n",
      "1697 [D loss: (-10.780)(R 64.361, F -85.921)]  [G loss: 79.693] \n",
      "1697 [D loss: (-7.775)(R 65.351, F -80.902)]  [G loss: 83.938] \n",
      "1698 [D loss: (-8.636)(R 64.385, F -81.657)]  [G loss: 82.352] \n",
      "1698 [D loss: (-8.176)(R 63.570, F -79.923)]  [G loss: 84.523] \n",
      "1699 [D loss: (-11.857)(R 62.313, F -86.026)]  [G loss: 80.851] \n",
      "1699 [D loss: (-12.697)(R 59.784, F -85.179)]  [G loss: 81.388] \n",
      "1700 [D loss: (-6.820)(R 66.318, F -79.957)]  [G loss: 83.795] \n",
      "1700 [D loss: (-9.448)(R 62.495, F -81.391)]  [G loss: 83.404] \n",
      "1701 [D loss: (-8.731)(R 62.766, F -80.229)]  [G loss: 85.752] \n",
      "1701 [D loss: (-16.236)(R 57.488, F -89.959)]  [G loss: 85.636] \n",
      "1702 [D loss: (-10.044)(R 64.730, F -84.817)]  [G loss: 81.051] \n",
      "1702 [D loss: (-10.794)(R 60.495, F -82.083)]  [G loss: 84.889] \n",
      "1703 [D loss: (-11.755)(R 60.045, F -83.554)]  [G loss: 80.343] \n",
      "1703 [D loss: (-8.927)(R 59.207, F -77.061)]  [G loss: 83.254] \n",
      "1704 [D loss: (-11.376)(R 57.163, F -79.914)]  [G loss: 83.150] \n",
      "1704 [D loss: (-13.709)(R 58.222, F -85.639)]  [G loss: 87.963] \n",
      "1705 [D loss: (-17.642)(R 57.613, F -92.897)]  [G loss: 82.941] \n",
      "1705 [D loss: (-12.389)(R 59.975, F -84.752)]  [G loss: 85.904] \n",
      "1706 [D loss: (-16.656)(R 56.037, F -89.350)]  [G loss: 85.520] \n",
      "1706 [D loss: (-14.874)(R 60.299, F -90.047)]  [G loss: 85.553] \n",
      "1707 [D loss: (-12.260)(R 62.620, F -87.140)]  [G loss: 85.415] \n",
      "1707 [D loss: (-9.046)(R 61.193, F -79.285)]  [G loss: 88.718] \n",
      "1708 [D loss: (-14.696)(R 60.393, F -89.785)]  [G loss: 85.971] \n",
      "1708 [D loss: (-15.857)(R 57.885, F -89.599)]  [G loss: 88.401] \n",
      "1709 [D loss: (-15.896)(R 60.606, F -92.399)]  [G loss: 86.737] \n",
      "1709 [D loss: (-19.098)(R 53.729, F -91.926)]  [G loss: 88.520] \n",
      "1710 [D loss: (-14.377)(R 58.380, F -87.135)]  [G loss: 85.582] \n",
      "1710 [D loss: (-16.014)(R 59.022, F -91.050)]  [G loss: 91.278] \n",
      "1711 [D loss: (-14.281)(R 56.022, F -84.583)]  [G loss: 90.377] \n",
      "1711 [D loss: (-9.603)(R 63.541, F -82.748)]  [G loss: 87.147] \n",
      "1712 [D loss: (-17.472)(R 58.597, F -93.541)]  [G loss: 86.966] \n",
      "1712 [D loss: (-15.775)(R 60.466, F -92.016)]  [G loss: 84.580] \n",
      "1713 [D loss: (-16.826)(R 56.119, F -89.771)]  [G loss: 87.503] \n",
      "1713 [D loss: (-17.491)(R 59.186, F -94.168)]  [G loss: 88.201] \n",
      "1714 [D loss: (-17.952)(R 55.800, F -91.705)]  [G loss: 87.732] \n",
      "1714 [D loss: (-12.445)(R 58.950, F -83.839)]  [G loss: 89.106] \n",
      "1715 [D loss: (-15.450)(R 59.106, F -90.006)]  [G loss: 92.919] \n",
      "1715 [D loss: (-14.011)(R 57.544, F -85.567)]  [G loss: 89.168] \n",
      "1716 [D loss: (-16.113)(R 56.242, F -88.467)]  [G loss: 88.213] \n",
      "1716 [D loss: (-14.514)(R 57.132, F -86.161)]  [G loss: 87.774] \n",
      "1717 [D loss: (-16.055)(R 57.209, F -89.318)]  [G loss: 86.543] \n",
      "1717 [D loss: (-15.082)(R 56.580, F -86.744)]  [G loss: 89.812] \n",
      "1718 [D loss: (-11.464)(R 58.828, F -81.756)]  [G loss: 86.410] \n",
      "1718 [D loss: (-18.856)(R 52.004, F -89.716)]  [G loss: 84.088] \n",
      "1719 [D loss: (-18.627)(R 52.597, F -89.850)]  [G loss: 83.388] \n",
      "1719 [D loss: (-15.978)(R 55.170, F -87.127)]  [G loss: 87.018] \n",
      "1720 [D loss: (-13.787)(R 58.083, F -85.657)]  [G loss: 83.968] \n",
      "1720 [D loss: (-10.388)(R 57.488, F -78.263)]  [G loss: 84.042] \n",
      "1721 [D loss: (-14.159)(R 54.604, F -82.922)]  [G loss: 85.649] \n",
      "1721 [D loss: (-15.248)(R 52.054, F -82.550)]  [G loss: 83.371] \n",
      "1722 [D loss: (-10.674)(R 56.621, F -77.968)]  [G loss: 79.798] \n",
      "1722 [D loss: (-17.287)(R 55.919, F -90.494)]  [G loss: 83.458] \n",
      "1723 [D loss: (-14.589)(R 54.376, F -83.554)]  [G loss: 83.531] \n",
      "1723 [D loss: (-10.488)(R 57.232, F -78.209)]  [G loss: 80.948] \n",
      "1724 [D loss: (-12.795)(R 54.528, F -80.117)]  [G loss: 76.191] \n",
      "1724 [D loss: (-15.034)(R 49.150, F -79.218)]  [G loss: 77.627] \n",
      "1725 [D loss: (-7.648)(R 56.916, F -72.212)]  [G loss: 74.485] \n",
      "1725 [D loss: (-8.007)(R 51.571, F -67.585)]  [G loss: 75.467] \n",
      "1726 [D loss: (-10.147)(R 52.672, F -72.966)]  [G loss: 75.780] \n",
      "1726 [D loss: (-8.558)(R 57.068, F -74.184)]  [G loss: 74.324] \n",
      "1727 [D loss: (-10.081)(R 52.872, F -73.034)]  [G loss: 71.993] \n",
      "1727 [D loss: (-6.182)(R 53.231, F -65.594)]  [G loss: 72.200] \n",
      "1728 [D loss: (-9.310)(R 54.035, F -72.655)]  [G loss: 74.234] \n",
      "1728 [D loss: (-10.515)(R 50.658, F -71.687)]  [G loss: 72.640] \n",
      "1729 [D loss: (-8.437)(R 50.194, F -67.068)]  [G loss: 69.269] \n",
      "1729 [D loss: (-7.153)(R 50.768, F -65.075)]  [G loss: 69.530] \n",
      "1730 [D loss: (-8.391)(R 48.888, F -65.669)]  [G loss: 63.906] \n",
      "1730 [D loss: (-6.477)(R 52.170, F -65.123)]  [G loss: 61.321] \n",
      "1731 [D loss: (-5.035)(R 49.620, F -59.690)]  [G loss: 61.321] \n",
      "1731 [D loss: (-6.463)(R 49.316, F -62.242)]  [G loss: 60.380] \n",
      "1732 [D loss: (-4.330)(R 44.321, F -52.980)]  [G loss: 63.204] \n",
      "1732 [D loss: (-2.838)(R 44.629, F -50.306)]  [G loss: 58.832] \n",
      "1733 [D loss: (-9.624)(R 43.943, F -63.192)]  [G loss: 55.571] \n",
      "1733 [D loss: (-5.657)(R 42.069, F -53.384)]  [G loss: 54.917] \n",
      "1734 [D loss: (-0.386)(R 47.655, F -48.428)]  [G loss: 58.816] \n",
      "1734 [D loss: (-4.237)(R 45.458, F -53.932)]  [G loss: 49.879] \n",
      "1735 [D loss: (-5.314)(R 41.324, F -51.952)]  [G loss: 49.915] \n",
      "1735 [D loss: (-6.218)(R 41.550, F -53.986)]  [G loss: 50.136] \n",
      "1736 [D loss: (-7.054)(R 41.431, F -55.539)]  [G loss: 50.272] \n",
      "1736 [D loss: (-8.156)(R 38.842, F -55.153)]  [G loss: 49.898] \n",
      "1737 [D loss: (-5.850)(R 39.098, F -50.797)]  [G loss: 47.138] \n",
      "1737 [D loss: (-8.721)(R 35.001, F -52.443)]  [G loss: 43.330] \n",
      "1738 [D loss: (-4.138)(R 38.252, F -46.528)]  [G loss: 44.997] \n",
      "1738 [D loss: (-4.248)(R 33.947, F -42.443)]  [G loss: 43.141] \n",
      "1739 [D loss: (-3.918)(R 35.602, F -43.439)]  [G loss: 45.005] \n",
      "1739 [D loss: (-3.170)(R 34.845, F -41.185)]  [G loss: 44.068] \n",
      "1740 [D loss: (-2.384)(R 32.636, F -37.404)]  [G loss: 39.972] \n",
      "1740 [D loss: (-1.239)(R 30.928, F -33.405)]  [G loss: 37.868] \n",
      "1741 [D loss: (-3.180)(R 30.527, F -36.887)]  [G loss: 39.947] \n",
      "1741 [D loss: (-2.321)(R 30.793, F -35.436)]  [G loss: 38.625] \n",
      "1742 [D loss: (-3.702)(R 28.987, F -36.391)]  [G loss: 37.535] \n",
      "1742 [D loss: (-3.816)(R 26.404, F -34.036)]  [G loss: 35.115] \n",
      "1743 [D loss: (-6.242)(R 25.969, F -38.453)]  [G loss: 31.449] \n",
      "1743 [D loss: (-6.265)(R 23.402, F -35.932)]  [G loss: 32.757] \n",
      "1744 [D loss: (-7.473)(R 20.331, F -35.277)]  [G loss: 31.936] \n",
      "1744 [D loss: (-6.780)(R 19.043, F -32.602)]  [G loss: 30.521] \n",
      "1745 [D loss: (-5.845)(R 17.686, F -29.376)]  [G loss: 31.897] \n",
      "1745 [D loss: (-6.824)(R 16.838, F -30.487)]  [G loss: 30.687] \n",
      "1746 [D loss: (-9.534)(R 15.058, F -34.126)]  [G loss: 32.712] \n",
      "1746 [D loss: (-9.032)(R 13.860, F -31.925)]  [G loss: 29.115] \n",
      "1747 [D loss: (-9.280)(R 14.254, F -32.815)]  [G loss: 32.997] \n",
      "1747 [D loss: (-9.489)(R 14.965, F -33.942)]  [G loss: 32.045] \n",
      "1748 [D loss: (-11.654)(R 12.457, F -35.765)]  [G loss: 31.282] \n",
      "1748 [D loss: (-12.007)(R 11.254, F -35.268)]  [G loss: 34.753] \n",
      "1749 [D loss: (-14.577)(R 9.359, F -38.513)]  [G loss: 35.059] \n",
      "1749 [D loss: (-13.176)(R 9.224, F -35.576)]  [G loss: 36.668] \n",
      "1750 [D loss: (-13.314)(R 9.746, F -36.375)]  [G loss: 38.583] \n",
      "1750 [D loss: (-15.018)(R 8.934, F -38.970)]  [G loss: 38.413] \n",
      "1751 [D loss: (-15.964)(R 7.289, F -39.217)]  [G loss: 41.758] \n",
      "1751 [D loss: (-17.168)(R 7.499, F -41.835)]  [G loss: 42.728] \n",
      "1752 [D loss: (-15.633)(R 7.835, F -39.102)]  [G loss: 43.060] \n",
      "1752 [D loss: (-18.434)(R 6.280, F -43.149)]  [G loss: 46.106] \n",
      "1753 [D loss: (-21.428)(R 6.559, F -49.415)]  [G loss: 49.281] \n",
      "1753 [D loss: (-22.538)(R 5.954, F -51.031)]  [G loss: 51.834] \n",
      "1754 [D loss: (-20.988)(R 5.831, F -47.808)]  [G loss: 50.572] \n",
      "1754 [D loss: (-23.558)(R 5.994, F -53.109)]  [G loss: 51.635] \n",
      "1755 [D loss: (-23.833)(R 5.588, F -53.254)]  [G loss: 55.719] \n",
      "1755 [D loss: (-25.232)(R 4.986, F -55.451)]  [G loss: 58.917] \n",
      "1756 [D loss: (-28.718)(R 5.926, F -63.361)]  [G loss: 58.535] \n",
      "1756 [D loss: (-29.730)(R 6.596, F -66.057)]  [G loss: 68.918] \n",
      "1757 [D loss: (-31.975)(R 6.124, F -70.073)]  [G loss: 73.398] \n",
      "1757 [D loss: (-35.677)(R 6.042, F -77.396)]  [G loss: 68.882] \n",
      "1758 [D loss: (-36.154)(R 7.960, F -80.268)]  [G loss: 75.566] \n",
      "1758 [D loss: (-32.339)(R 5.751, F -70.429)]  [G loss: 77.659] \n",
      "1759 [D loss: (-37.650)(R 5.871, F -81.171)]  [G loss: 78.715] \n",
      "1759 [D loss: (-37.877)(R 7.830, F -83.584)]  [G loss: 83.721] \n",
      "1760 [D loss: (-44.156)(R 7.534, F -95.846)]  [G loss: 88.060] \n",
      "1760 [D loss: (-43.115)(R 6.536, F -92.766)]  [G loss: 88.643] \n",
      "1761 [D loss: (-50.847)(R 8.946, F -110.639)]  [G loss: 94.105] \n",
      "1761 [D loss: (-49.604)(R 7.238, F -106.445)]  [G loss: 97.728] \n",
      "1762 [D loss: (-49.248)(R 7.139, F -105.635)]  [G loss: 105.276] \n",
      "1762 [D loss: (-47.085)(R 7.211, F -101.382)]  [G loss: 107.454] \n",
      "1763 [D loss: (-54.813)(R 9.686, F -119.311)]  [G loss: 110.195] \n",
      "1763 [D loss: (-55.031)(R 9.647, F -119.708)]  [G loss: 111.598] \n",
      "1764 [D loss: (-54.891)(R 11.451, F -121.233)]  [G loss: 114.607] \n",
      "1764 [D loss: (-63.289)(R 12.450, F -139.027)]  [G loss: 123.493] \n",
      "1765 [D loss: (-56.744)(R 14.236, F -127.724)]  [G loss: 120.769] \n",
      "1765 [D loss: (-59.006)(R 10.903, F -128.916)]  [G loss: 134.027] \n",
      "1766 [D loss: (-58.858)(R 11.235, F -128.950)]  [G loss: 130.320] \n",
      "1766 [D loss: (-64.749)(R 10.892, F -140.390)]  [G loss: 143.675] \n",
      "1767 [D loss: (-72.033)(R 14.232, F -158.297)]  [G loss: 144.959] \n",
      "1767 [D loss: (-65.818)(R 12.280, F -143.916)]  [G loss: 142.449] \n",
      "1768 [D loss: (-62.554)(R 13.484, F -138.591)]  [G loss: 147.190] \n",
      "1768 [D loss: (-64.197)(R 11.778, F -140.171)]  [G loss: 148.406] \n",
      "1769 [D loss: (-83.911)(R 17.080, F -184.902)]  [G loss: 152.474] \n",
      "1769 [D loss: (-71.217)(R 11.491, F -153.925)]  [G loss: 163.847] \n",
      "1770 [D loss: (-76.885)(R 11.462, F -165.231)]  [G loss: 157.199] \n",
      "1770 [D loss: (-80.222)(R 16.651, F -177.096)]  [G loss: 155.277] \n",
      "1771 [D loss: (-64.721)(R 15.758, F -145.199)]  [G loss: 172.330] \n",
      "1771 [D loss: (-78.535)(R 20.131, F -177.201)]  [G loss: 170.864] \n",
      "1772 [D loss: (-77.623)(R 13.988, F -169.234)]  [G loss: 179.361] \n",
      "1772 [D loss: (-80.246)(R 17.740, F -178.232)]  [G loss: 168.286] \n",
      "1773 [D loss: (-85.342)(R 16.011, F -186.696)]  [G loss: 180.697] \n",
      "1773 [D loss: (-74.983)(R 19.766, F -169.732)]  [G loss: 181.040] \n",
      "1774 [D loss: (-82.998)(R 18.276, F -184.273)]  [G loss: 176.741] \n",
      "1774 [D loss: (-80.823)(R 19.207, F -180.853)]  [G loss: 189.592] \n",
      "1775 [D loss: (-87.868)(R 24.772, F -200.508)]  [G loss: 199.057] \n",
      "1775 [D loss: (-84.714)(R 24.088, F -193.516)]  [G loss: 195.971] \n",
      "1776 [D loss: (-76.189)(R 20.439, F -172.816)]  [G loss: 198.733] \n",
      "1776 [D loss: (-91.101)(R 24.902, F -207.104)]  [G loss: 213.895] \n",
      "1777 [D loss: (-96.588)(R 24.962, F -218.139)]  [G loss: 193.477] \n",
      "1777 [D loss: (-83.091)(R 21.436, F -187.618)]  [G loss: 198.182] \n",
      "1778 [D loss: (-84.226)(R 20.669, F -189.121)]  [G loss: 208.791] \n",
      "1778 [D loss: (-92.721)(R 23.658, F -209.100)]  [G loss: 208.776] \n",
      "1779 [D loss: (-91.059)(R 23.457, F -205.575)]  [G loss: 208.527] \n",
      "1779 [D loss: (-84.993)(R 34.081, F -204.066)]  [G loss: 213.367] \n",
      "1780 [D loss: (-98.706)(R 18.040, F -215.452)]  [G loss: 221.691] \n",
      "1780 [D loss: (-84.023)(R 29.351, F -197.397)]  [G loss: 213.619] \n",
      "1781 [D loss: (-93.994)(R 19.968, F -207.956)]  [G loss: 199.501] \n",
      "1781 [D loss: (-87.409)(R 21.266, F -196.083)]  [G loss: 210.267] \n",
      "1782 [D loss: (-95.159)(R 28.857, F -219.176)]  [G loss: 202.969] \n",
      "1782 [D loss: (-98.538)(R 21.034, F -218.109)]  [G loss: 211.889] \n",
      "1783 [D loss: (-88.979)(R 32.313, F -210.271)]  [G loss: 203.069] \n",
      "1783 [D loss: (-83.097)(R 28.533, F -194.727)]  [G loss: 186.664] \n",
      "1784 [D loss: (-73.442)(R 28.617, F -175.502)]  [G loss: 193.246] \n",
      "1784 [D loss: (-61.958)(R 40.969, F -164.884)]  [G loss: 200.522] \n",
      "1785 [D loss: (-65.304)(R 28.637, F -159.244)]  [G loss: 186.763] \n",
      "1785 [D loss: (-77.815)(R 31.748, F -187.378)]  [G loss: 191.012] \n",
      "1786 [D loss: (-86.898)(R 27.716, F -201.511)]  [G loss: 189.456] \n",
      "1786 [D loss: (-69.685)(R 29.576, F -168.946)]  [G loss: 171.477] \n",
      "1787 [D loss: (-99.173)(R 26.952, F -225.297)]  [G loss: 185.255] \n",
      "1787 [D loss: (-88.811)(R 32.343, F -209.964)]  [G loss: 178.037] \n",
      "1788 [D loss: (-62.767)(R 28.304, F -153.839)]  [G loss: 167.856] \n",
      "1788 [D loss: (-65.205)(R 26.790, F -157.200)]  [G loss: 159.740] \n",
      "1789 [D loss: (-68.404)(R 29.053, F -165.861)]  [G loss: 158.751] \n",
      "1789 [D loss: (-63.535)(R 31.964, F -159.035)]  [G loss: 155.313] \n",
      "1790 [D loss: (-57.924)(R 31.166, F -147.013)]  [G loss: 145.966] \n",
      "1790 [D loss: (-55.707)(R 38.037, F -149.451)]  [G loss: 138.756] \n",
      "1791 [D loss: (-52.281)(R 34.989, F -139.550)]  [G loss: 140.430] \n",
      "1791 [D loss: (-54.700)(R 37.614, F -147.013)]  [G loss: 135.759] \n",
      "1792 [D loss: (-40.946)(R 37.064, F -118.955)]  [G loss: 124.205] \n",
      "1792 [D loss: (-30.044)(R 34.620, F -94.708)]  [G loss: 118.593] \n",
      "1793 [D loss: (-34.743)(R 23.283, F -92.768)]  [G loss: 108.138] \n",
      "1793 [D loss: (-40.503)(R 34.210, F -115.216)]  [G loss: 103.065] \n",
      "1794 [D loss: (-32.576)(R 29.052, F -94.205)]  [G loss: 101.107] \n",
      "1794 [D loss: (-23.480)(R 29.508, F -76.469)]  [G loss: 90.012] \n",
      "1795 [D loss: (-17.177)(R 32.006, F -66.360)]  [G loss: 77.848] \n",
      "1795 [D loss: (-11.065)(R 34.797, F -56.927)]  [G loss: 71.072] \n",
      "1796 [D loss: (-10.349)(R 26.585, F -47.282)]  [G loss: 68.671] \n",
      "1796 [D loss: (-7.293)(R 35.211, F -49.798)]  [G loss: 58.887] \n",
      "1797 [D loss: (-14.276)(R 31.042, F -59.594)]  [G loss: 54.288] \n",
      "1797 [D loss: (-1.851)(R 35.919, F -39.622)]  [G loss: 44.998] \n",
      "1798 [D loss: (4.417)(R 38.233, F -29.399)]  [G loss: 38.901] \n",
      "1798 [D loss: (4.902)(R 38.892, F -29.088)]  [G loss: 36.668] \n",
      "1799 [D loss: (7.538)(R 39.682, F -24.607)]  [G loss: 33.461] \n",
      "1799 [D loss: (4.806)(R 29.830, F -20.219)]  [G loss: 21.125] \n",
      "1800 [D loss: (10.374)(R 35.925, F -15.177)]  [G loss: 16.629] \n",
      "1800 [D loss: (14.478)(R 38.111, F -9.155)]  [G loss: 11.044] \n",
      "1801 [D loss: (10.392)(R 26.903, F -6.119)]  [G loss: 9.197] \n",
      "1801 [D loss: (11.416)(R 27.060, F -4.228)]  [G loss: 5.476] \n",
      "1802 [D loss: (8.217)(R 21.664, F -5.231)]  [G loss: 5.501] \n",
      "1802 [D loss: (12.871)(R 28.317, F -2.575)]  [G loss: 3.218] \n",
      "1803 [D loss: (13.654)(R 27.391, F -0.083)]  [G loss: 0.543] \n",
      "1803 [D loss: (13.953)(R 28.480, F -0.575)]  [G loss: 0.861] \n",
      "1804 [D loss: (14.098)(R 29.982, F -1.786)]  [G loss: 0.455] \n",
      "1804 [D loss: (15.426)(R 30.308, F 0.543)]  [G loss: 0.294] \n",
      "1805 [D loss: (12.823)(R 25.178, F 0.467)]  [G loss: -0.307] \n",
      "1805 [D loss: (12.068)(R 23.622, F 0.513)]  [G loss: -0.288] \n",
      "1806 [D loss: (10.025)(R 19.325, F 0.725)]  [G loss: -0.378] \n",
      "1806 [D loss: (12.537)(R 24.664, F 0.411)]  [G loss: -0.707] \n",
      "1807 [D loss: (10.565)(R 20.602, F 0.529)]  [G loss: -0.683] \n",
      "1807 [D loss: (12.310)(R 23.939, F 0.681)]  [G loss: -0.764] \n",
      "1808 [D loss: (9.421)(R 18.100, F 0.741)]  [G loss: -0.546] \n",
      "1808 [D loss: (13.146)(R 25.446, F 0.846)]  [G loss: -0.680] \n",
      "1809 [D loss: (12.633)(R 24.611, F 0.656)]  [G loss: -0.562] \n",
      "1809 [D loss: (12.456)(R 24.404, F 0.508)]  [G loss: -0.786] \n",
      "1810 [D loss: (10.184)(R 19.752, F 0.616)]  [G loss: -0.617] \n",
      "1810 [D loss: (10.439)(R 20.276, F 0.601)]  [G loss: -0.719] \n",
      "1811 [D loss: (11.085)(R 21.440, F 0.730)]  [G loss: -0.500] \n",
      "1811 [D loss: (9.612)(R 18.739, F 0.485)]  [G loss: -0.574] \n",
      "1812 [D loss: (7.559)(R 14.498, F 0.620)]  [G loss: -0.699] \n",
      "1812 [D loss: (7.792)(R 14.925, F 0.659)]  [G loss: -0.685] \n",
      "1813 [D loss: (9.827)(R 19.029, F 0.625)]  [G loss: -0.742] \n",
      "1813 [D loss: (8.646)(R 16.834, F 0.457)]  [G loss: -0.554] \n",
      "1814 [D loss: (8.849)(R 17.286, F 0.413)]  [G loss: -0.596] \n",
      "1814 [D loss: (8.472)(R 16.470, F 0.474)]  [G loss: -0.607] \n",
      "1815 [D loss: (9.789)(R 18.978, F 0.601)]  [G loss: -0.499] \n",
      "1815 [D loss: (8.228)(R 15.831, F 0.626)]  [G loss: -0.599] \n",
      "1816 [D loss: (7.048)(R 13.565, F 0.532)]  [G loss: -0.608] \n",
      "1816 [D loss: (9.282)(R 17.986, F 0.578)]  [G loss: -0.518] \n",
      "1817 [D loss: (8.293)(R 16.256, F 0.330)]  [G loss: -0.559] \n",
      "1817 [D loss: (6.188)(R 11.964, F 0.412)]  [G loss: -0.502] \n",
      "1818 [D loss: (8.266)(R 16.043, F 0.490)]  [G loss: -0.514] \n",
      "1818 [D loss: (5.691)(R 10.781, F 0.600)]  [G loss: -0.554] \n",
      "1819 [D loss: (7.644)(R 14.793, F 0.495)]  [G loss: -0.495] \n",
      "1819 [D loss: (7.161)(R 13.827, F 0.495)]  [G loss: -0.564] \n",
      "1820 [D loss: (6.983)(R 13.437, F 0.529)]  [G loss: -0.447] \n",
      "1820 [D loss: (8.360)(R 16.221, F 0.500)]  [G loss: -0.498] \n",
      "1821 [D loss: (7.737)(R 15.047, F 0.428)]  [G loss: -0.556] \n",
      "1821 [D loss: (6.624)(R 12.750, F 0.499)]  [G loss: -0.522] \n",
      "1822 [D loss: (6.721)(R 13.050, F 0.392)]  [G loss: -0.458] \n",
      "1822 [D loss: (5.500)(R 10.673, F 0.327)]  [G loss: -0.418] \n",
      "1823 [D loss: (6.044)(R 11.584, F 0.505)]  [G loss: -0.504] \n",
      "1823 [D loss: (5.413)(R 10.424, F 0.403)]  [G loss: -0.224] \n",
      "1824 [D loss: (5.332)(R 10.252, F 0.411)]  [G loss: -0.487] \n",
      "1824 [D loss: (6.855)(R 13.192, F 0.518)]  [G loss: -0.421] \n",
      "1825 [D loss: (6.021)(R 11.572, F 0.469)]  [G loss: -0.487] \n",
      "1825 [D loss: (4.951)(R 9.424, F 0.478)]  [G loss: -0.483] \n",
      "1826 [D loss: (6.709)(R 12.919, F 0.500)]  [G loss: -0.355] \n",
      "1826 [D loss: (5.229)(R 9.995, F 0.464)]  [G loss: -0.466] \n",
      "1827 [D loss: (5.356)(R 10.310, F 0.403)]  [G loss: -0.464] \n",
      "1827 [D loss: (5.864)(R 11.306, F 0.422)]  [G loss: -0.454] \n",
      "1828 [D loss: (6.174)(R 11.978, F 0.371)]  [G loss: -0.233] \n",
      "1828 [D loss: (3.849)(R 7.281, F 0.416)]  [G loss: -0.385] \n",
      "1829 [D loss: (6.355)(R 12.284, F 0.426)]  [G loss: -0.408] \n",
      "1829 [D loss: (3.906)(R 7.427, F 0.386)]  [G loss: -0.449] \n",
      "1830 [D loss: (5.529)(R 10.765, F 0.294)]  [G loss: -0.405] \n",
      "1830 [D loss: (4.005)(R 7.568, F 0.442)]  [G loss: -0.384] \n",
      "1831 [D loss: (3.797)(R 7.214, F 0.379)]  [G loss: -0.333] \n",
      "1831 [D loss: (5.236)(R 10.096, F 0.377)]  [G loss: -0.363] \n",
      "1832 [D loss: (4.706)(R 9.060, F 0.353)]  [G loss: -0.334] \n",
      "1832 [D loss: (4.113)(R 7.881, F 0.344)]  [G loss: -0.332] \n",
      "1833 [D loss: (4.832)(R 9.291, F 0.373)]  [G loss: -0.319] \n",
      "1833 [D loss: (4.144)(R 7.959, F 0.328)]  [G loss: -0.340] \n",
      "1834 [D loss: (4.480)(R 8.681, F 0.279)]  [G loss: -0.376] \n",
      "1834 [D loss: (4.055)(R 7.767, F 0.343)]  [G loss: -0.387] \n",
      "1835 [D loss: (3.775)(R 7.227, F 0.324)]  [G loss: -0.324] \n",
      "1835 [D loss: (3.771)(R 7.171, F 0.370)]  [G loss: -0.349] \n",
      "1836 [D loss: (4.125)(R 7.885, F 0.365)]  [G loss: -0.285] \n",
      "1836 [D loss: (3.662)(R 6.964, F 0.360)]  [G loss: -0.279] \n",
      "1837 [D loss: (4.588)(R 8.844, F 0.332)]  [G loss: -0.274] \n",
      "1837 [D loss: (3.805)(R 7.330, F 0.280)]  [G loss: -0.327] \n",
      "1838 [D loss: (3.616)(R 6.889, F 0.343)]  [G loss: -0.302] \n",
      "1838 [D loss: (3.687)(R 7.034, F 0.339)]  [G loss: -0.302] \n",
      "1839 [D loss: (3.683)(R 7.027, F 0.340)]  [G loss: -0.305] \n",
      "1839 [D loss: (4.351)(R 8.402, F 0.300)]  [G loss: -0.244] \n",
      "1840 [D loss: (3.649)(R 6.989, F 0.309)]  [G loss: -0.267] \n",
      "1840 [D loss: (4.341)(R 8.440, F 0.243)]  [G loss: -0.294] \n",
      "1841 [D loss: (3.816)(R 7.361, F 0.272)]  [G loss: -0.320] \n",
      "1841 [D loss: (3.479)(R 6.664, F 0.294)]  [G loss: -0.319] \n",
      "1842 [D loss: (3.323)(R 6.373, F 0.273)]  [G loss: -0.311] \n",
      "1842 [D loss: (3.151)(R 6.013, F 0.289)]  [G loss: -0.298] \n",
      "1843 [D loss: (2.917)(R 5.527, F 0.308)]  [G loss: -0.254] \n",
      "1843 [D loss: (3.362)(R 6.438, F 0.286)]  [G loss: -0.302] \n",
      "1844 [D loss: (3.509)(R 6.709, F 0.309)]  [G loss: -0.291] \n",
      "1844 [D loss: (3.586)(R 6.897, F 0.275)]  [G loss: -0.302] \n",
      "1845 [D loss: (2.756)(R 5.222, F 0.290)]  [G loss: -0.304] \n",
      "1845 [D loss: (3.565)(R 6.896, F 0.234)]  [G loss: -0.296] \n",
      "1846 [D loss: (3.256)(R 6.228, F 0.283)]  [G loss: -0.238] \n",
      "1846 [D loss: (2.885)(R 5.498, F 0.271)]  [G loss: -0.225] \n",
      "1847 [D loss: (2.657)(R 5.031, F 0.283)]  [G loss: -0.271] \n",
      "1847 [D loss: (2.678)(R 5.089, F 0.266)]  [G loss: -0.272] \n",
      "1848 [D loss: (3.507)(R 6.769, F 0.245)]  [G loss: -0.265] \n",
      "1848 [D loss: (2.522)(R 4.792, F 0.251)]  [G loss: -0.247] \n",
      "1849 [D loss: (2.585)(R 4.928, F 0.242)]  [G loss: -0.253] \n",
      "1849 [D loss: (2.996)(R 5.733, F 0.259)]  [G loss: -0.226] \n",
      "1850 [D loss: (2.383)(R 4.537, F 0.229)]  [G loss: -0.251] \n",
      "1850 [D loss: (2.686)(R 5.130, F 0.242)]  [G loss: -0.240] \n",
      "1851 [D loss: (2.255)(R 4.250, F 0.260)]  [G loss: -0.247] \n",
      "1851 [D loss: (2.502)(R 4.749, F 0.255)]  [G loss: -0.231] \n",
      "1852 [D loss: (2.538)(R 4.823, F 0.253)]  [G loss: -0.222] \n",
      "1852 [D loss: (2.568)(R 4.895, F 0.241)]  [G loss: -0.253] \n",
      "1853 [D loss: (2.861)(R 5.493, F 0.229)]  [G loss: -0.246] \n",
      "1853 [D loss: (2.084)(R 3.929, F 0.239)]  [G loss: -0.234] \n",
      "1854 [D loss: (2.545)(R 4.854, F 0.237)]  [G loss: -0.219] \n",
      "1854 [D loss: (3.009)(R 5.799, F 0.220)]  [G loss: -0.221] \n",
      "1855 [D loss: (2.698)(R 5.161, F 0.235)]  [G loss: -0.227] \n",
      "1855 [D loss: (2.570)(R 4.942, F 0.197)]  [G loss: -0.227] \n",
      "1856 [D loss: (2.672)(R 5.136, F 0.209)]  [G loss: -0.234] \n",
      "1856 [D loss: (2.456)(R 4.704, F 0.207)]  [G loss: -0.214] \n",
      "1857 [D loss: (1.930)(R 3.648, F 0.213)]  [G loss: -0.214] \n",
      "1857 [D loss: (1.968)(R 3.728, F 0.208)]  [G loss: -0.213] \n",
      "1858 [D loss: (2.107)(R 3.998, F 0.215)]  [G loss: -0.223] \n",
      "1858 [D loss: (1.733)(R 3.250, F 0.217)]  [G loss: -0.200] \n",
      "1859 [D loss: (2.041)(R 3.868, F 0.215)]  [G loss: -0.206] \n",
      "1859 [D loss: (1.900)(R 3.592, F 0.207)]  [G loss: -0.214] \n",
      "1860 [D loss: (2.413)(R 4.629, F 0.197)]  [G loss: -0.185] \n",
      "1860 [D loss: (2.240)(R 4.284, F 0.197)]  [G loss: -0.191] \n",
      "1861 [D loss: (1.623)(R 3.045, F 0.201)]  [G loss: -0.198] \n",
      "1861 [D loss: (2.426)(R 4.674, F 0.177)]  [G loss: -0.205] \n",
      "1862 [D loss: (1.839)(R 3.479, F 0.199)]  [G loss: -0.213] \n",
      "1862 [D loss: (2.013)(R 3.818, F 0.207)]  [G loss: -0.201] \n",
      "1863 [D loss: (1.615)(R 3.032, F 0.197)]  [G loss: -0.198] \n",
      "1863 [D loss: (2.128)(R 4.035, F 0.221)]  [G loss: -0.198] \n",
      "1864 [D loss: (1.844)(R 3.507, F 0.181)]  [G loss: -0.208] \n",
      "1864 [D loss: (1.632)(R 3.073, F 0.192)]  [G loss: -0.185] \n",
      "1865 [D loss: (2.117)(R 4.084, F 0.150)]  [G loss: -0.174] \n",
      "1865 [D loss: (1.704)(R 3.206, F 0.202)]  [G loss: -0.199] \n",
      "1866 [D loss: (1.832)(R 3.488, F 0.175)]  [G loss: -0.163] \n",
      "1866 [D loss: (1.532)(R 2.875, F 0.189)]  [G loss: -0.158] \n",
      "1867 [D loss: (1.254)(R 2.331, F 0.178)]  [G loss: -0.181] \n",
      "1867 [D loss: (1.569)(R 2.978, F 0.160)]  [G loss: -0.187] \n",
      "1868 [D loss: (1.674)(R 3.169, F 0.179)]  [G loss: -0.182] \n",
      "1868 [D loss: (1.679)(R 3.220, F 0.138)]  [G loss: -0.178] \n",
      "1869 [D loss: (1.754)(R 3.321, F 0.186)]  [G loss: -0.189] \n",
      "1869 [D loss: (1.722)(R 3.262, F 0.181)]  [G loss: -0.181] \n",
      "1870 [D loss: (1.903)(R 3.625, F 0.181)]  [G loss: -0.166] \n",
      "1870 [D loss: (1.279)(R 2.394, F 0.165)]  [G loss: -0.182] \n",
      "1871 [D loss: (1.833)(R 3.480, F 0.187)]  [G loss: -0.166] \n",
      "1871 [D loss: (1.959)(R 3.742, F 0.175)]  [G loss: -0.178] \n",
      "1872 [D loss: (1.758)(R 3.345, F 0.171)]  [G loss: -0.178] \n",
      "1872 [D loss: (1.660)(R 3.140, F 0.179)]  [G loss: -0.169] \n",
      "1873 [D loss: (1.702)(R 3.246, F 0.158)]  [G loss: -0.170] \n",
      "1873 [D loss: (1.543)(R 2.920, F 0.167)]  [G loss: -0.172] \n",
      "1874 [D loss: (1.602)(R 3.039, F 0.166)]  [G loss: -0.156] \n",
      "1874 [D loss: (1.326)(R 2.492, F 0.160)]  [G loss: -0.168] \n",
      "1875 [D loss: (1.432)(R 2.704, F 0.160)]  [G loss: -0.162] \n",
      "1875 [D loss: (1.797)(R 3.476, F 0.119)]  [G loss: -0.171] \n",
      "1876 [D loss: (1.210)(R 2.258, F 0.162)]  [G loss: -0.164] \n",
      "1876 [D loss: (1.300)(R 2.426, F 0.174)]  [G loss: -0.170] \n",
      "1877 [D loss: (1.201)(R 2.248, F 0.155)]  [G loss: -0.160] \n",
      "1877 [D loss: (1.423)(R 2.678, F 0.168)]  [G loss: -0.164] \n",
      "1878 [D loss: (1.015)(R 1.883, F 0.148)]  [G loss: -0.160] \n",
      "1878 [D loss: (1.044)(R 1.925, F 0.164)]  [G loss: -0.136] \n",
      "1879 [D loss: (1.268)(R 2.382, F 0.155)]  [G loss: -0.159] \n",
      "1879 [D loss: (1.000)(R 1.856, F 0.144)]  [G loss: -0.151] \n",
      "1880 [D loss: (1.124)(R 2.082, F 0.165)]  [G loss: -0.155] \n",
      "1880 [D loss: (1.212)(R 2.282, F 0.143)]  [G loss: -0.151] \n",
      "1881 [D loss: (1.113)(R 2.070, F 0.156)]  [G loss: -0.156] \n",
      "1881 [D loss: (1.071)(R 2.004, F 0.139)]  [G loss: -0.145] \n",
      "1882 [D loss: (1.146)(R 2.149, F 0.142)]  [G loss: -0.151] \n",
      "1882 [D loss: (1.100)(R 2.057, F 0.143)]  [G loss: -0.138] \n",
      "1883 [D loss: (1.096)(R 2.057, F 0.135)]  [G loss: -0.151] \n",
      "1883 [D loss: (1.189)(R 2.230, F 0.148)]  [G loss: -0.138] \n",
      "1884 [D loss: (1.158)(R 2.175, F 0.142)]  [G loss: -0.130] \n",
      "1884 [D loss: (1.062)(R 1.984, F 0.141)]  [G loss: -0.140] \n",
      "1885 [D loss: (0.802)(R 1.487, F 0.117)]  [G loss: -0.140] \n",
      "1885 [D loss: (1.087)(R 2.033, F 0.142)]  [G loss: -0.129] \n",
      "1886 [D loss: (0.793)(R 1.452, F 0.134)]  [G loss: -0.117] \n",
      "1886 [D loss: (0.799)(R 1.462, F 0.136)]  [G loss: -0.133] \n",
      "1887 [D loss: (1.135)(R 2.142, F 0.129)]  [G loss: -0.138] \n",
      "1887 [D loss: (1.066)(R 1.999, F 0.133)]  [G loss: -0.141] \n",
      "1888 [D loss: (0.999)(R 1.870, F 0.127)]  [G loss: -0.138] \n",
      "1888 [D loss: (0.975)(R 1.812, F 0.137)]  [G loss: -0.140] \n",
      "1889 [D loss: (1.011)(R 1.897, F 0.125)]  [G loss: -0.140] \n",
      "1889 [D loss: (0.861)(R 1.582, F 0.140)]  [G loss: -0.130] \n",
      "1890 [D loss: (0.885)(R 1.637, F 0.133)]  [G loss: -0.136] \n",
      "1890 [D loss: (0.916)(R 1.694, F 0.138)]  [G loss: -0.126] \n",
      "1891 [D loss: (0.860)(R 1.592, F 0.128)]  [G loss: -0.137] \n",
      "1891 [D loss: (0.707)(R 1.280, F 0.134)]  [G loss: -0.126] \n",
      "1892 [D loss: (0.631)(R 1.131, F 0.131)]  [G loss: -0.133] \n",
      "1892 [D loss: (0.944)(R 1.758, F 0.130)]  [G loss: -0.123] \n",
      "1893 [D loss: (0.972)(R 1.815, F 0.130)]  [G loss: -0.131] \n",
      "1893 [D loss: (0.970)(R 1.809, F 0.131)]  [G loss: -0.123] \n",
      "1894 [D loss: (0.791)(R 1.451, F 0.132)]  [G loss: -0.127] \n",
      "1894 [D loss: (0.901)(R 1.672, F 0.130)]  [G loss: -0.125] \n",
      "1895 [D loss: (0.763)(R 1.398, F 0.128)]  [G loss: -0.122] \n",
      "1895 [D loss: (0.613)(R 1.106, F 0.119)]  [G loss: -0.122] \n",
      "1896 [D loss: (0.758)(R 1.391, F 0.125)]  [G loss: -0.114] \n",
      "1896 [D loss: (0.759)(R 1.397, F 0.121)]  [G loss: -0.113] \n",
      "1897 [D loss: (1.011)(R 1.899, F 0.124)]  [G loss: -0.117] \n",
      "1897 [D loss: (0.609)(R 1.095, F 0.123)]  [G loss: -0.121] \n",
      "1898 [D loss: (0.609)(R 1.102, F 0.115)]  [G loss: -0.123] \n",
      "1898 [D loss: (0.747)(R 1.372, F 0.122)]  [G loss: -0.114] \n",
      "1899 [D loss: (0.489)(R 0.865, F 0.113)]  [G loss: -0.123] \n",
      "1899 [D loss: (0.506)(R 0.891, F 0.122)]  [G loss: -0.122] \n",
      "1900 [D loss: (0.576)(R 1.032, F 0.121)]  [G loss: -0.119] \n",
      "1900 [D loss: (0.542)(R 0.966, F 0.118)]  [G loss: -0.119] \n",
      "1901 [D loss: (0.711)(R 1.301, F 0.121)]  [G loss: -0.110] \n",
      "1901 [D loss: (0.756)(R 1.407, F 0.105)]  [G loss: -0.115] \n",
      "1902 [D loss: (0.763)(R 1.414, F 0.112)]  [G loss: -0.110] \n",
      "1902 [D loss: (0.733)(R 1.347, F 0.118)]  [G loss: -0.105] \n",
      "1903 [D loss: (0.770)(R 1.423, F 0.118)]  [G loss: -0.110] \n",
      "1903 [D loss: (0.567)(R 1.023, F 0.111)]  [G loss: -0.105] \n",
      "1904 [D loss: (0.586)(R 1.061, F 0.111)]  [G loss: -0.111] \n",
      "1904 [D loss: (0.640)(R 1.171, F 0.108)]  [G loss: -0.115] \n",
      "1905 [D loss: (0.658)(R 1.205, F 0.112)]  [G loss: -0.111] \n",
      "1905 [D loss: (0.697)(R 1.286, F 0.108)]  [G loss: -0.109] \n",
      "1906 [D loss: (0.640)(R 1.173, F 0.107)]  [G loss: -0.107] \n",
      "1906 [D loss: (0.550)(R 0.991, F 0.109)]  [G loss: -0.111] \n",
      "1907 [D loss: (0.502)(R 0.892, F 0.112)]  [G loss: -0.109] \n",
      "1907 [D loss: (0.731)(R 1.352, F 0.111)]  [G loss: -0.109] \n",
      "1908 [D loss: (0.568)(R 1.030, F 0.105)]  [G loss: -0.106] \n",
      "1908 [D loss: (0.456)(R 0.813, F 0.098)]  [G loss: -0.102] \n",
      "1909 [D loss: (0.497)(R 0.892, F 0.102)]  [G loss: -0.106] \n",
      "1909 [D loss: (0.475)(R 0.844, F 0.107)]  [G loss: -0.106] \n",
      "1910 [D loss: (0.552)(R 1.001, F 0.103)]  [G loss: -0.105] \n",
      "1910 [D loss: (0.495)(R 0.886, F 0.103)]  [G loss: -0.104] \n",
      "1911 [D loss: (0.679)(R 1.253, F 0.105)]  [G loss: -0.100] \n",
      "1911 [D loss: (0.542)(R 0.984, F 0.100)]  [G loss: -0.100] \n",
      "1912 [D loss: (0.590)(R 1.078, F 0.103)]  [G loss: -0.099] \n",
      "1912 [D loss: (0.462)(R 0.825, F 0.098)]  [G loss: -0.103] \n",
      "1913 [D loss: (0.530)(R 0.960, F 0.101)]  [G loss: -0.099] \n",
      "1913 [D loss: (0.442)(R 0.784, F 0.100)]  [G loss: -0.102] \n",
      "1914 [D loss: (0.477)(R 0.859, F 0.095)]  [G loss: -0.101] \n",
      "1914 [D loss: (0.531)(R 0.959, F 0.103)]  [G loss: -0.098] \n",
      "1915 [D loss: (0.501)(R 0.903, F 0.100)]  [G loss: -0.095] \n",
      "1915 [D loss: (0.456)(R 0.814, F 0.099)]  [G loss: -0.096] \n",
      "1916 [D loss: (0.431)(R 0.767, F 0.094)]  [G loss: -0.097] \n",
      "1916 [D loss: (0.368)(R 0.641, F 0.096)]  [G loss: -0.089] \n",
      "1917 [D loss: (0.501)(R 0.910, F 0.091)]  [G loss: -0.097] \n",
      "1917 [D loss: (0.428)(R 0.759, F 0.096)]  [G loss: -0.095] \n",
      "1918 [D loss: (0.570)(R 1.052, F 0.088)]  [G loss: -0.094] \n",
      "1918 [D loss: (0.524)(R 0.955, F 0.093)]  [G loss: -0.096] \n",
      "1919 [D loss: (0.359)(R 0.628, F 0.089)]  [G loss: -0.092] \n",
      "1919 [D loss: (0.342)(R 0.591, F 0.092)]  [G loss: -0.094] \n",
      "1920 [D loss: (0.398)(R 0.707, F 0.090)]  [G loss: -0.093] \n",
      "1920 [D loss: (0.545)(R 1.002, F 0.088)]  [G loss: -0.088] \n",
      "1921 [D loss: (0.421)(R 0.752, F 0.089)]  [G loss: -0.088] \n",
      "1921 [D loss: (0.414)(R 0.735, F 0.093)]  [G loss: -0.090] \n",
      "1922 [D loss: (0.425)(R 0.755, F 0.094)]  [G loss: -0.089] \n",
      "1922 [D loss: (0.356)(R 0.622, F 0.089)]  [G loss: -0.091] \n",
      "1923 [D loss: (0.382)(R 0.673, F 0.091)]  [G loss: -0.087] \n",
      "1923 [D loss: (0.415)(R 0.740, F 0.090)]  [G loss: -0.088] \n",
      "1924 [D loss: (0.502)(R 0.920, F 0.083)]  [G loss: -0.088] \n",
      "1924 [D loss: (0.403)(R 0.718, F 0.089)]  [G loss: -0.087] \n",
      "1925 [D loss: (0.488)(R 0.891, F 0.086)]  [G loss: -0.087] \n",
      "1925 [D loss: (0.371)(R 0.657, F 0.086)]  [G loss: -0.083] \n",
      "1926 [D loss: (0.377)(R 0.669, F 0.085)]  [G loss: -0.087] \n",
      "1926 [D loss: (0.421)(R 0.758, F 0.085)]  [G loss: -0.086] \n",
      "1927 [D loss: (0.352)(R 0.619, F 0.085)]  [G loss: -0.086] \n",
      "1927 [D loss: (0.269)(R 0.456, F 0.083)]  [G loss: -0.084] \n",
      "1928 [D loss: (0.365)(R 0.647, F 0.083)]  [G loss: -0.079] \n",
      "1928 [D loss: (0.372)(R 0.665, F 0.079)]  [G loss: -0.084] \n",
      "1929 [D loss: (0.442)(R 0.803, F 0.081)]  [G loss: -0.082] \n",
      "1929 [D loss: (0.347)(R 0.610, F 0.083)]  [G loss: -0.076] \n",
      "1930 [D loss: (0.478)(R 0.874, F 0.082)]  [G loss: -0.083] \n",
      "1930 [D loss: (0.277)(R 0.474, F 0.080)]  [G loss: -0.082] \n",
      "1931 [D loss: (0.265)(R 0.452, F 0.078)]  [G loss: -0.082] \n",
      "1931 [D loss: (0.360)(R 0.639, F 0.081)]  [G loss: -0.081] \n",
      "1932 [D loss: (0.272)(R 0.466, F 0.078)]  [G loss: -0.079] \n",
      "1932 [D loss: (0.328)(R 0.578, F 0.077)]  [G loss: -0.076] \n",
      "1933 [D loss: (0.333)(R 0.589, F 0.077)]  [G loss: -0.079] \n",
      "1933 [D loss: (0.399)(R 0.718, F 0.080)]  [G loss: -0.076] \n",
      "1934 [D loss: (0.372)(R 0.668, F 0.076)]  [G loss: -0.078] \n",
      "1934 [D loss: (0.305)(R 0.531, F 0.079)]  [G loss: -0.077] \n",
      "1935 [D loss: (0.287)(R 0.496, F 0.078)]  [G loss: -0.077] \n",
      "1935 [D loss: (0.359)(R 0.641, F 0.076)]  [G loss: -0.078] \n",
      "1936 [D loss: (0.263)(R 0.450, F 0.076)]  [G loss: -0.075] \n",
      "1936 [D loss: (0.286)(R 0.495, F 0.076)]  [G loss: -0.071] \n",
      "1937 [D loss: (0.309)(R 0.542, F 0.076)]  [G loss: -0.073] \n",
      "1937 [D loss: (0.299)(R 0.521, F 0.076)]  [G loss: -0.076] \n",
      "1938 [D loss: (0.292)(R 0.510, F 0.075)]  [G loss: -0.074] \n",
      "1938 [D loss: (0.285)(R 0.496, F 0.073)]  [G loss: -0.075] \n",
      "1939 [D loss: (0.193)(R 0.313, F 0.072)]  [G loss: -0.072] \n",
      "1939 [D loss: (0.338)(R 0.601, F 0.074)]  [G loss: -0.073] \n",
      "1940 [D loss: (0.295)(R 0.518, F 0.072)]  [G loss: -0.068] \n",
      "1940 [D loss: (0.358)(R 0.645, F 0.071)]  [G loss: -0.072] \n",
      "1941 [D loss: (0.309)(R 0.548, F 0.070)]  [G loss: -0.071] \n",
      "1941 [D loss: (0.291)(R 0.512, F 0.069)]  [G loss: -0.071] \n",
      "1942 [D loss: (0.243)(R 0.416, F 0.071)]  [G loss: -0.071] \n",
      "1942 [D loss: (0.208)(R 0.350, F 0.067)]  [G loss: -0.071] \n",
      "1943 [D loss: (0.287)(R 0.503, F 0.071)]  [G loss: -0.069] \n",
      "1943 [D loss: (0.291)(R 0.519, F 0.063)]  [G loss: -0.067] \n",
      "1944 [D loss: (0.243)(R 0.420, F 0.066)]  [G loss: -0.070] \n",
      "1944 [D loss: (0.258)(R 0.447, F 0.068)]  [G loss: -0.070] \n",
      "1945 [D loss: (0.218)(R 0.367, F 0.069)]  [G loss: -0.068] \n",
      "1945 [D loss: (0.292)(R 0.516, F 0.069)]  [G loss: -0.067] \n",
      "1946 [D loss: (0.188)(R 0.308, F 0.069)]  [G loss: -0.068] \n",
      "1946 [D loss: (0.214)(R 0.359, F 0.068)]  [G loss: -0.068] \n",
      "1947 [D loss: (0.240)(R 0.414, F 0.066)]  [G loss: -0.066] \n",
      "1947 [D loss: (0.219)(R 0.372, F 0.065)]  [G loss: -0.067] \n",
      "1948 [D loss: (0.337)(R 0.606, F 0.069)]  [G loss: -0.066] \n",
      "1948 [D loss: (0.182)(R 0.298, F 0.065)]  [G loss: -0.064] \n",
      "1949 [D loss: (0.252)(R 0.440, F 0.065)]  [G loss: -0.065] \n",
      "1949 [D loss: (0.188)(R 0.311, F 0.064)]  [G loss: -0.063] \n",
      "1950 [D loss: (0.350)(R 0.633, F 0.066)]  [G loss: -0.065] \n",
      "1950 [D loss: (0.157)(R 0.251, F 0.064)]  [G loss: -0.065] \n",
      "1951 [D loss: (0.261)(R 0.458, F 0.065)]  [G loss: -0.063] \n",
      "1951 [D loss: (0.189)(R 0.315, F 0.063)]  [G loss: -0.061] \n",
      "1952 [D loss: (0.182)(R 0.300, F 0.064)]  [G loss: -0.063] \n",
      "1952 [D loss: (0.201)(R 0.341, F 0.062)]  [G loss: -0.063] \n",
      "1953 [D loss: (0.241)(R 0.420, F 0.062)]  [G loss: -0.063] \n",
      "1953 [D loss: (0.187)(R 0.315, F 0.059)]  [G loss: -0.061] \n",
      "1954 [D loss: (0.206)(R 0.349, F 0.063)]  [G loss: -0.062] \n",
      "1954 [D loss: (0.185)(R 0.307, F 0.063)]  [G loss: -0.060] \n",
      "1955 [D loss: (0.209)(R 0.355, F 0.062)]  [G loss: -0.061] \n",
      "1955 [D loss: (0.184)(R 0.307, F 0.061)]  [G loss: -0.062] \n",
      "1956 [D loss: (0.215)(R 0.368, F 0.061)]  [G loss: -0.062] \n",
      "1956 [D loss: (0.150)(R 0.240, F 0.061)]  [G loss: -0.060] \n",
      "1957 [D loss: (0.139)(R 0.219, F 0.060)]  [G loss: -0.062] \n",
      "1957 [D loss: (0.138)(R 0.216, F 0.061)]  [G loss: -0.059] \n",
      "1958 [D loss: (0.137)(R 0.212, F 0.061)]  [G loss: -0.059] \n",
      "1958 [D loss: (0.120)(R 0.179, F 0.060)]  [G loss: -0.060] \n",
      "1959 [D loss: (0.162)(R 0.265, F 0.058)]  [G loss: -0.059] \n",
      "1959 [D loss: (0.177)(R 0.293, F 0.060)]  [G loss: -0.058] \n",
      "1960 [D loss: (0.195)(R 0.332, F 0.057)]  [G loss: -0.059] \n",
      "1960 [D loss: (0.165)(R 0.273, F 0.058)]  [G loss: -0.059] \n",
      "1961 [D loss: (0.176)(R 0.293, F 0.058)]  [G loss: -0.058] \n",
      "1961 [D loss: (0.124)(R 0.191, F 0.058)]  [G loss: -0.059] \n",
      "1962 [D loss: (0.187)(R 0.317, F 0.056)]  [G loss: -0.057] \n",
      "1962 [D loss: (0.114)(R 0.173, F 0.056)]  [G loss: -0.058] \n",
      "1963 [D loss: (0.113)(R 0.170, F 0.056)]  [G loss: -0.058] \n",
      "1963 [D loss: (0.150)(R 0.243, F 0.057)]  [G loss: -0.057] \n",
      "1964 [D loss: (0.167)(R 0.278, F 0.055)]  [G loss: -0.057] \n",
      "1964 [D loss: (0.146)(R 0.235, F 0.057)]  [G loss: -0.055] \n",
      "1965 [D loss: (0.198)(R 0.341, F 0.056)]  [G loss: -0.054] \n",
      "1965 [D loss: (0.162)(R 0.267, F 0.056)]  [G loss: -0.056] \n",
      "1966 [D loss: (0.158)(R 0.260, F 0.056)]  [G loss: -0.055] \n",
      "1966 [D loss: (0.177)(R 0.300, F 0.055)]  [G loss: -0.052] \n",
      "1967 [D loss: (0.173)(R 0.291, F 0.054)]  [G loss: -0.056] \n",
      "1967 [D loss: (0.165)(R 0.277, F 0.053)]  [G loss: -0.054] \n",
      "1968 [D loss: (0.087)(R 0.119, F 0.054)]  [G loss: -0.053] \n",
      "1968 [D loss: (0.140)(R 0.227, F 0.053)]  [G loss: -0.054] \n",
      "1969 [D loss: (0.148)(R 0.241, F 0.055)]  [G loss: -0.054] \n",
      "1969 [D loss: (0.151)(R 0.252, F 0.050)]  [G loss: -0.054] \n",
      "1970 [D loss: (0.187)(R 0.319, F 0.054)]  [G loss: -0.053] \n",
      "1970 [D loss: (0.124)(R 0.197, F 0.051)]  [G loss: -0.054] \n",
      "1971 [D loss: (0.127)(R 0.199, F 0.054)]  [G loss: -0.053] \n",
      "1971 [D loss: (0.136)(R 0.219, F 0.053)]  [G loss: -0.052] \n",
      "1972 [D loss: (0.140)(R 0.227, F 0.054)]  [G loss: -0.051] \n",
      "1972 [D loss: (0.099)(R 0.147, F 0.052)]  [G loss: -0.051] \n",
      "1973 [D loss: (0.146)(R 0.238, F 0.053)]  [G loss: -0.051] \n",
      "1973 [D loss: (0.221)(R 0.393, F 0.050)]  [G loss: -0.051] \n",
      "1974 [D loss: (0.151)(R 0.251, F 0.051)]  [G loss: -0.049] \n",
      "1974 [D loss: (0.153)(R 0.256, F 0.050)]  [G loss: -0.051] \n",
      "1975 [D loss: (0.106)(R 0.160, F 0.051)]  [G loss: -0.051] \n",
      "1975 [D loss: (0.119)(R 0.188, F 0.051)]  [G loss: -0.051] \n",
      "1976 [D loss: (0.107)(R 0.164, F 0.051)]  [G loss: -0.050] \n",
      "1976 [D loss: (0.089)(R 0.127, F 0.050)]  [G loss: -0.051] \n",
      "1977 [D loss: (0.136)(R 0.222, F 0.050)]  [G loss: -0.050] \n",
      "1977 [D loss: (0.145)(R 0.240, F 0.050)]  [G loss: -0.050] \n",
      "1978 [D loss: (0.083)(R 0.117, F 0.049)]  [G loss: -0.049] \n",
      "1978 [D loss: (0.091)(R 0.134, F 0.049)]  [G loss: -0.049] \n",
      "1979 [D loss: (0.168)(R 0.287, F 0.049)]  [G loss: -0.049] \n",
      "1979 [D loss: (0.109)(R 0.169, F 0.050)]  [G loss: -0.047] \n",
      "1980 [D loss: (0.092)(R 0.134, F 0.050)]  [G loss: -0.049] \n",
      "1980 [D loss: (0.090)(R 0.131, F 0.048)]  [G loss: -0.048] \n",
      "1981 [D loss: (0.119)(R 0.190, F 0.047)]  [G loss: -0.049] \n",
      "1981 [D loss: (0.107)(R 0.166, F 0.049)]  [G loss: -0.047] \n",
      "1982 [D loss: (0.115)(R 0.180, F 0.049)]  [G loss: -0.048] \n",
      "1982 [D loss: (0.120)(R 0.194, F 0.047)]  [G loss: -0.048] \n",
      "1983 [D loss: (0.083)(R 0.119, F 0.047)]  [G loss: -0.048] \n",
      "1983 [D loss: (0.083)(R 0.119, F 0.047)]  [G loss: -0.047] \n",
      "1984 [D loss: (0.087)(R 0.126, F 0.048)]  [G loss: -0.048] \n",
      "1984 [D loss: (0.091)(R 0.136, F 0.046)]  [G loss: -0.046] \n",
      "1985 [D loss: (0.082)(R 0.117, F 0.047)]  [G loss: -0.046] \n",
      "1985 [D loss: (0.060)(R 0.073, F 0.046)]  [G loss: -0.047] \n",
      "1986 [D loss: (0.107)(R 0.168, F 0.046)]  [G loss: -0.047] \n",
      "1986 [D loss: (0.077)(R 0.108, F 0.047)]  [G loss: -0.047] \n",
      "1987 [D loss: (0.108)(R 0.171, F 0.046)]  [G loss: -0.046] \n",
      "1987 [D loss: (0.115)(R 0.185, F 0.045)]  [G loss: -0.046] \n",
      "1988 [D loss: (0.066)(R 0.087, F 0.046)]  [G loss: -0.045] \n",
      "1988 [D loss: (0.059)(R 0.074, F 0.045)]  [G loss: -0.045] \n",
      "1989 [D loss: (0.104)(R 0.164, F 0.045)]  [G loss: -0.045] \n",
      "1989 [D loss: (0.096)(R 0.146, F 0.045)]  [G loss: -0.046] \n",
      "1990 [D loss: (0.102)(R 0.159, F 0.045)]  [G loss: -0.045] \n",
      "1990 [D loss: (0.040)(R 0.034, F 0.045)]  [G loss: -0.044] \n",
      "1991 [D loss: (0.082)(R 0.118, F 0.045)]  [G loss: -0.044] \n",
      "1991 [D loss: (0.112)(R 0.179, F 0.045)]  [G loss: -0.044] \n",
      "1992 [D loss: (0.077)(R 0.110, F 0.044)]  [G loss: -0.044] \n",
      "1992 [D loss: (0.049)(R 0.054, F 0.044)]  [G loss: -0.045] \n",
      "1993 [D loss: (0.072)(R 0.099, F 0.044)]  [G loss: -0.044] \n",
      "1993 [D loss: (0.077)(R 0.110, F 0.044)]  [G loss: -0.043] \n",
      "1994 [D loss: (0.075)(R 0.107, F 0.044)]  [G loss: -0.044] \n",
      "1994 [D loss: (0.049)(R 0.055, F 0.044)]  [G loss: -0.043] \n",
      "1995 [D loss: (0.052)(R 0.061, F 0.043)]  [G loss: -0.043] \n",
      "1995 [D loss: (0.087)(R 0.130, F 0.043)]  [G loss: -0.043] \n",
      "1996 [D loss: (0.068)(R 0.093, F 0.043)]  [G loss: -0.043] \n",
      "1996 [D loss: (0.076)(R 0.110, F 0.043)]  [G loss: -0.043] \n",
      "1997 [D loss: (0.085)(R 0.127, F 0.042)]  [G loss: -0.042] \n",
      "1997 [D loss: (0.049)(R 0.056, F 0.043)]  [G loss: -0.042] \n",
      "1998 [D loss: (0.079)(R 0.114, F 0.043)]  [G loss: -0.043] \n",
      "1998 [D loss: (0.045)(R 0.048, F 0.042)]  [G loss: -0.042] \n",
      "1999 [D loss: (0.059)(R 0.075, F 0.042)]  [G loss: -0.041] \n",
      "1999 [D loss: (0.070)(R 0.097, F 0.042)]  [G loss: -0.042] \n",
      "2000 [D loss: (0.052)(R 0.063, F 0.042)]  [G loss: -0.042] \n",
      "2000 [D loss: (0.074)(R 0.106, F 0.042)]  [G loss: -0.042] \n",
      "2001 [D loss: (0.106)(R 0.171, F 0.041)]  [G loss: -0.041] \n",
      "2001 [D loss: (0.039)(R 0.037, F 0.041)]  [G loss: -0.041] \n",
      "2002 [D loss: (0.055)(R 0.068, F 0.041)]  [G loss: -0.041] \n",
      "2002 [D loss: (0.067)(R 0.092, F 0.042)]  [G loss: -0.041] \n",
      "2003 [D loss: (0.063)(R 0.085, F 0.042)]  [G loss: -0.041] \n",
      "2003 [D loss: (0.074)(R 0.107, F 0.042)]  [G loss: -0.040] \n",
      "2004 [D loss: (0.067)(R 0.094, F 0.040)]  [G loss: -0.041] \n",
      "2004 [D loss: (0.071)(R 0.102, F 0.040)]  [G loss: -0.040] \n",
      "2005 [D loss: (0.025)(R 0.010, F 0.040)]  [G loss: -0.040] \n",
      "2005 [D loss: (0.044)(R 0.049, F 0.040)]  [G loss: -0.040] \n",
      "2006 [D loss: (0.040)(R 0.040, F 0.039)]  [G loss: -0.040] \n",
      "2006 [D loss: (0.029)(R 0.017, F 0.040)]  [G loss: -0.040] \n",
      "2007 [D loss: (0.050)(R 0.061, F 0.040)]  [G loss: -0.040] \n",
      "2007 [D loss: (0.042)(R 0.043, F 0.040)]  [G loss: -0.040] \n",
      "2008 [D loss: (0.064)(R 0.089, F 0.040)]  [G loss: -0.039] \n",
      "2008 [D loss: (0.062)(R 0.084, F 0.040)]  [G loss: -0.040] \n",
      "2009 [D loss: (0.055)(R 0.072, F 0.039)]  [G loss: -0.039] \n",
      "2009 [D loss: (0.060)(R 0.081, F 0.040)]  [G loss: -0.039] \n",
      "2010 [D loss: (0.049)(R 0.060, F 0.039)]  [G loss: -0.038] \n",
      "2010 [D loss: (0.033)(R 0.028, F 0.039)]  [G loss: -0.039] \n",
      "2011 [D loss: (0.047)(R 0.055, F 0.039)]  [G loss: -0.038] \n",
      "2011 [D loss: (0.050)(R 0.062, F 0.038)]  [G loss: -0.039] \n",
      "2012 [D loss: (0.050)(R 0.062, F 0.039)]  [G loss: -0.038] \n",
      "2012 [D loss: (0.043)(R 0.047, F 0.039)]  [G loss: -0.039] \n",
      "2013 [D loss: (0.038)(R 0.039, F 0.038)]  [G loss: -0.038] \n",
      "2013 [D loss: (0.032)(R 0.026, F 0.038)]  [G loss: -0.038] \n",
      "2014 [D loss: (0.043)(R 0.047, F 0.038)]  [G loss: -0.038] \n",
      "2014 [D loss: (0.045)(R 0.051, F 0.038)]  [G loss: -0.038] \n",
      "2015 [D loss: (0.040)(R 0.041, F 0.038)]  [G loss: -0.038] \n",
      "2015 [D loss: (0.033)(R 0.028, F 0.038)]  [G loss: -0.038] \n",
      "2016 [D loss: (0.045)(R 0.051, F 0.038)]  [G loss: -0.038] \n",
      "2016 [D loss: (0.044)(R 0.051, F 0.038)]  [G loss: -0.037] \n",
      "2017 [D loss: (0.037)(R 0.036, F 0.037)]  [G loss: -0.037] \n",
      "2017 [D loss: (0.034)(R 0.030, F 0.037)]  [G loss: -0.037] \n",
      "2018 [D loss: (0.047)(R 0.057, F 0.037)]  [G loss: -0.037] \n",
      "2018 [D loss: (0.030)(R 0.023, F 0.037)]  [G loss: -0.037] \n",
      "2019 [D loss: (0.038)(R 0.039, F 0.037)]  [G loss: -0.037] \n",
      "2019 [D loss: (0.052)(R 0.066, F 0.038)]  [G loss: -0.037] \n",
      "2020 [D loss: (0.017)(R -0.002, F 0.037)]  [G loss: -0.037] \n",
      "2020 [D loss: (0.056)(R 0.076, F 0.037)]  [G loss: -0.037] \n",
      "2021 [D loss: (0.021)(R 0.006, F 0.037)]  [G loss: -0.037] \n",
      "2021 [D loss: (0.043)(R 0.049, F 0.036)]  [G loss: -0.036] \n",
      "2022 [D loss: (0.031)(R 0.026, F 0.036)]  [G loss: -0.037] \n",
      "2022 [D loss: (0.038)(R 0.040, F 0.036)]  [G loss: -0.036] \n",
      "2023 [D loss: (0.042)(R 0.049, F 0.036)]  [G loss: -0.036] \n",
      "2023 [D loss: (0.036)(R 0.035, F 0.036)]  [G loss: -0.036] \n",
      "2024 [D loss: (0.019)(R 0.003, F 0.036)]  [G loss: -0.036] \n",
      "2024 [D loss: (0.031)(R 0.025, F 0.036)]  [G loss: -0.036] \n",
      "2025 [D loss: (0.041)(R 0.047, F 0.036)]  [G loss: -0.036] \n",
      "2025 [D loss: (0.024)(R 0.013, F 0.036)]  [G loss: -0.036] \n",
      "2026 [D loss: (0.025)(R 0.015, F 0.035)]  [G loss: -0.035] \n",
      "2026 [D loss: (0.036)(R 0.038, F 0.035)]  [G loss: -0.035] \n",
      "2027 [D loss: (0.023)(R 0.010, F 0.036)]  [G loss: -0.035] \n",
      "2027 [D loss: (0.035)(R 0.035, F 0.035)]  [G loss: -0.035] \n",
      "2028 [D loss: (0.035)(R 0.033, F 0.036)]  [G loss: -0.035] \n",
      "2028 [D loss: (0.025)(R 0.014, F 0.036)]  [G loss: -0.035] \n",
      "2029 [D loss: (0.044)(R 0.054, F 0.035)]  [G loss: -0.035] \n",
      "2029 [D loss: (0.016)(R -0.002, F 0.035)]  [G loss: -0.035] \n",
      "2030 [D loss: (0.018)(R 0.001, F 0.035)]  [G loss: -0.035] \n",
      "2030 [D loss: (0.032)(R 0.029, F 0.035)]  [G loss: -0.035] \n",
      "2031 [D loss: (0.030)(R 0.026, F 0.035)]  [G loss: -0.035] \n",
      "2031 [D loss: (0.018)(R 0.002, F 0.034)]  [G loss: -0.035] \n",
      "2032 [D loss: (0.029)(R 0.023, F 0.034)]  [G loss: -0.034] \n",
      "2032 [D loss: (0.019)(R 0.004, F 0.035)]  [G loss: -0.034] \n",
      "2033 [D loss: (0.034)(R 0.033, F 0.035)]  [G loss: -0.034] \n",
      "2033 [D loss: (0.032)(R 0.029, F 0.034)]  [G loss: -0.034] \n",
      "2034 [D loss: (0.030)(R 0.027, F 0.034)]  [G loss: -0.034] \n",
      "2034 [D loss: (0.027)(R 0.021, F 0.033)]  [G loss: -0.034] \n",
      "2035 [D loss: (0.017)(R -0.000, F 0.034)]  [G loss: -0.034] \n",
      "2035 [D loss: (0.036)(R 0.038, F 0.034)]  [G loss: -0.034] \n",
      "2036 [D loss: (0.018)(R 0.003, F 0.034)]  [G loss: -0.034] \n",
      "2036 [D loss: (0.024)(R 0.015, F 0.034)]  [G loss: -0.034] \n",
      "2037 [D loss: (0.024)(R 0.015, F 0.034)]  [G loss: -0.034] \n",
      "2037 [D loss: (0.014)(R -0.005, F 0.033)]  [G loss: -0.033] \n",
      "2038 [D loss: (0.020)(R 0.007, F 0.033)]  [G loss: -0.033] \n",
      "2038 [D loss: (0.027)(R 0.021, F 0.033)]  [G loss: -0.033] \n",
      "2039 [D loss: (0.032)(R 0.032, F 0.033)]  [G loss: -0.033] \n",
      "2039 [D loss: (0.018)(R 0.003, F 0.033)]  [G loss: -0.033] \n",
      "2040 [D loss: (0.023)(R 0.013, F 0.033)]  [G loss: -0.033] \n",
      "2040 [D loss: (0.025)(R 0.017, F 0.033)]  [G loss: -0.033] \n",
      "2041 [D loss: (0.027)(R 0.022, F 0.033)]  [G loss: -0.033] \n",
      "2041 [D loss: (0.019)(R 0.005, F 0.033)]  [G loss: -0.033] \n",
      "2042 [D loss: (0.007)(R -0.019, F 0.033)]  [G loss: -0.033] \n",
      "2042 [D loss: (0.038)(R 0.044, F 0.033)]  [G loss: -0.033] \n",
      "2043 [D loss: (0.034)(R 0.035, F 0.033)]  [G loss: -0.033] \n",
      "2043 [D loss: (0.024)(R 0.016, F 0.033)]  [G loss: -0.033] \n",
      "2044 [D loss: (0.024)(R 0.016, F 0.032)]  [G loss: -0.033] \n",
      "2044 [D loss: (0.015)(R -0.003, F 0.032)]  [G loss: -0.032] \n",
      "2045 [D loss: (0.023)(R 0.015, F 0.032)]  [G loss: -0.032] \n",
      "2045 [D loss: (0.016)(R -0.000, F 0.032)]  [G loss: -0.032] \n",
      "2046 [D loss: (0.008)(R -0.016, F 0.032)]  [G loss: -0.032] \n",
      "2046 [D loss: (0.027)(R 0.022, F 0.032)]  [G loss: -0.032] \n",
      "2047 [D loss: (0.014)(R -0.004, F 0.032)]  [G loss: -0.032] \n",
      "2047 [D loss: (0.008)(R -0.017, F 0.032)]  [G loss: -0.032] \n",
      "2048 [D loss: (0.020)(R 0.009, F 0.032)]  [G loss: -0.032] \n",
      "2048 [D loss: (0.017)(R 0.003, F 0.032)]  [G loss: -0.032] \n",
      "2049 [D loss: (0.017)(R 0.003, F 0.032)]  [G loss: -0.032] \n",
      "2049 [D loss: (0.018)(R 0.004, F 0.032)]  [G loss: -0.032] \n",
      "2050 [D loss: (0.015)(R -0.001, F 0.032)]  [G loss: -0.032] \n",
      "2050 [D loss: (0.017)(R 0.002, F 0.031)]  [G loss: -0.031] \n",
      "2051 [D loss: (0.026)(R 0.020, F 0.031)]  [G loss: -0.031] \n",
      "2051 [D loss: (0.020)(R 0.008, F 0.032)]  [G loss: -0.031] \n",
      "2052 [D loss: (0.012)(R -0.008, F 0.031)]  [G loss: -0.031] \n",
      "2052 [D loss: (0.011)(R -0.009, F 0.031)]  [G loss: -0.031] \n",
      "2053 [D loss: (0.011)(R -0.009, F 0.031)]  [G loss: -0.031] \n",
      "2053 [D loss: (0.013)(R -0.005, F 0.031)]  [G loss: -0.031] \n",
      "2054 [D loss: (0.014)(R -0.002, F 0.031)]  [G loss: -0.031] \n",
      "2054 [D loss: (0.012)(R -0.008, F 0.031)]  [G loss: -0.031] \n",
      "2055 [D loss: (0.012)(R -0.006, F 0.031)]  [G loss: -0.031] \n",
      "2055 [D loss: (0.009)(R -0.012, F 0.031)]  [G loss: -0.031] \n",
      "2056 [D loss: (0.007)(R -0.017, F 0.031)]  [G loss: -0.031] \n",
      "2056 [D loss: (0.009)(R -0.012, F 0.031)]  [G loss: -0.031] \n",
      "2057 [D loss: (0.015)(R -0.001, F 0.031)]  [G loss: -0.031] \n",
      "2057 [D loss: (0.008)(R -0.014, F 0.031)]  [G loss: -0.031] \n",
      "2058 [D loss: (0.010)(R -0.011, F 0.030)]  [G loss: -0.031] \n",
      "2058 [D loss: (0.005)(R -0.020, F 0.031)]  [G loss: -0.030] \n",
      "2059 [D loss: (0.011)(R -0.009, F 0.030)]  [G loss: -0.030] \n",
      "2059 [D loss: (0.012)(R -0.006, F 0.030)]  [G loss: -0.030] \n",
      "2060 [D loss: (0.014)(R -0.003, F 0.030)]  [G loss: -0.030] \n",
      "2060 [D loss: (0.012)(R -0.007, F 0.030)]  [G loss: -0.030] \n",
      "2061 [D loss: (0.010)(R -0.010, F 0.030)]  [G loss: -0.030] \n",
      "2061 [D loss: (0.005)(R -0.021, F 0.030)]  [G loss: -0.030] \n",
      "2062 [D loss: (0.006)(R -0.017, F 0.030)]  [G loss: -0.030] \n",
      "2062 [D loss: (0.006)(R -0.018, F 0.030)]  [G loss: -0.030] \n",
      "2063 [D loss: (0.007)(R -0.017, F 0.030)]  [G loss: -0.030] \n",
      "2063 [D loss: (0.010)(R -0.010, F 0.030)]  [G loss: -0.030] \n",
      "2064 [D loss: (0.009)(R -0.012, F 0.030)]  [G loss: -0.030] \n",
      "2064 [D loss: (0.007)(R -0.016, F 0.030)]  [G loss: -0.030] \n",
      "2065 [D loss: (0.006)(R -0.017, F 0.030)]  [G loss: -0.030] \n",
      "2065 [D loss: (0.012)(R -0.005, F 0.030)]  [G loss: -0.030] \n",
      "2066 [D loss: (0.004)(R -0.022, F 0.030)]  [G loss: -0.030] \n",
      "2066 [D loss: (0.005)(R -0.020, F 0.030)]  [G loss: -0.030] \n",
      "2067 [D loss: (0.013)(R -0.004, F 0.029)]  [G loss: -0.030] \n",
      "2067 [D loss: (0.008)(R -0.014, F 0.030)]  [G loss: -0.029] \n",
      "2068 [D loss: (0.012)(R -0.004, F 0.029)]  [G loss: -0.029] \n",
      "2068 [D loss: (0.011)(R -0.008, F 0.030)]  [G loss: -0.029] \n",
      "2069 [D loss: (0.006)(R -0.018, F 0.029)]  [G loss: -0.029] \n",
      "2069 [D loss: (0.010)(R -0.009, F 0.029)]  [G loss: -0.029] \n",
      "2070 [D loss: (0.009)(R -0.012, F 0.029)]  [G loss: -0.029] \n",
      "2070 [D loss: (0.007)(R -0.016, F 0.029)]  [G loss: -0.029] \n",
      "2071 [D loss: (0.004)(R -0.022, F 0.029)]  [G loss: -0.029] \n",
      "2071 [D loss: (0.003)(R -0.022, F 0.029)]  [G loss: -0.029] \n",
      "2072 [D loss: (0.008)(R -0.013, F 0.029)]  [G loss: -0.029] \n",
      "2072 [D loss: (0.008)(R -0.013, F 0.029)]  [G loss: -0.029] \n",
      "2073 [D loss: (0.002)(R -0.025, F 0.029)]  [G loss: -0.029] \n",
      "2073 [D loss: (0.005)(R -0.019, F 0.029)]  [G loss: -0.029] \n",
      "2074 [D loss: (0.017)(R 0.005, F 0.029)]  [G loss: -0.029] \n",
      "2074 [D loss: (0.006)(R -0.017, F 0.029)]  [G loss: -0.029] \n",
      "2075 [D loss: (0.007)(R -0.014, F 0.029)]  [G loss: -0.029] \n",
      "2075 [D loss: (0.005)(R -0.019, F 0.029)]  [G loss: -0.029] \n",
      "2076 [D loss: (0.004)(R -0.020, F 0.029)]  [G loss: -0.029] \n",
      "2076 [D loss: (0.004)(R -0.020, F 0.029)]  [G loss: -0.029] \n",
      "2077 [D loss: (0.007)(R -0.014, F 0.029)]  [G loss: -0.029] \n",
      "2077 [D loss: (0.005)(R -0.018, F 0.029)]  [G loss: -0.029] \n",
      "2078 [D loss: (0.005)(R -0.018, F 0.028)]  [G loss: -0.029] \n",
      "2078 [D loss: (0.007)(R -0.015, F 0.028)]  [G loss: -0.029] \n",
      "2079 [D loss: (0.005)(R -0.018, F 0.028)]  [G loss: -0.028] \n",
      "2079 [D loss: (0.008)(R -0.013, F 0.028)]  [G loss: -0.028] \n",
      "2080 [D loss: (0.006)(R -0.015, F 0.028)]  [G loss: -0.028] \n",
      "2080 [D loss: (0.006)(R -0.016, F 0.028)]  [G loss: -0.028] \n",
      "2081 [D loss: (0.005)(R -0.018, F 0.028)]  [G loss: -0.028] \n",
      "2081 [D loss: (0.004)(R -0.020, F 0.028)]  [G loss: -0.028] \n",
      "2082 [D loss: (0.002)(R -0.025, F 0.028)]  [G loss: -0.028] \n",
      "2082 [D loss: (0.004)(R -0.021, F 0.028)]  [G loss: -0.028] \n",
      "2083 [D loss: (0.008)(R -0.012, F 0.028)]  [G loss: -0.028] \n",
      "2083 [D loss: (0.006)(R -0.017, F 0.028)]  [G loss: -0.028] \n",
      "2084 [D loss: (0.004)(R -0.021, F 0.028)]  [G loss: -0.028] \n",
      "2084 [D loss: (0.005)(R -0.017, F 0.028)]  [G loss: -0.028] \n",
      "2085 [D loss: (0.005)(R -0.017, F 0.028)]  [G loss: -0.028] \n",
      "2085 [D loss: (0.008)(R -0.012, F 0.028)]  [G loss: -0.028] \n",
      "2086 [D loss: (0.002)(R -0.023, F 0.028)]  [G loss: -0.028] \n",
      "2086 [D loss: (0.003)(R -0.022, F 0.028)]  [G loss: -0.028] \n",
      "2087 [D loss: (0.003)(R -0.021, F 0.028)]  [G loss: -0.028] \n",
      "2087 [D loss: (0.005)(R -0.018, F 0.028)]  [G loss: -0.028] \n",
      "2088 [D loss: (0.007)(R -0.014, F 0.028)]  [G loss: -0.028] \n",
      "2088 [D loss: (0.004)(R -0.020, F 0.028)]  [G loss: -0.028] \n",
      "2089 [D loss: (0.003)(R -0.022, F 0.028)]  [G loss: -0.028] \n",
      "2089 [D loss: (0.002)(R -0.023, F 0.028)]  [G loss: -0.028] \n",
      "2090 [D loss: (0.005)(R -0.019, F 0.028)]  [G loss: -0.028] \n",
      "2090 [D loss: (0.005)(R -0.017, F 0.028)]  [G loss: -0.028] \n",
      "2091 [D loss: (0.003)(R -0.021, F 0.028)]  [G loss: -0.028] \n",
      "2091 [D loss: (0.002)(R -0.024, F 0.028)]  [G loss: -0.028] \n",
      "2092 [D loss: (0.004)(R -0.019, F 0.028)]  [G loss: -0.028] \n",
      "2092 [D loss: (0.003)(R -0.021, F 0.028)]  [G loss: -0.027] \n",
      "2093 [D loss: (0.003)(R -0.021, F 0.028)]  [G loss: -0.028] \n",
      "2093 [D loss: (0.005)(R -0.018, F 0.028)]  [G loss: -0.027] \n",
      "2094 [D loss: (0.002)(R -0.024, F 0.027)]  [G loss: -0.027] \n",
      "2094 [D loss: (0.005)(R -0.017, F 0.028)]  [G loss: -0.027] \n",
      "2095 [D loss: (0.002)(R -0.022, F 0.027)]  [G loss: -0.027] \n",
      "2095 [D loss: (0.004)(R -0.020, F 0.027)]  [G loss: -0.027] \n",
      "2096 [D loss: (0.004)(R -0.019, F 0.027)]  [G loss: -0.027] \n",
      "2096 [D loss: (0.002)(R -0.024, F 0.027)]  [G loss: -0.027] \n",
      "2097 [D loss: (0.002)(R -0.024, F 0.027)]  [G loss: -0.027] \n",
      "2097 [D loss: (0.001)(R -0.025, F 0.027)]  [G loss: -0.027] \n",
      "2098 [D loss: (0.001)(R -0.025, F 0.027)]  [G loss: -0.027] \n",
      "2098 [D loss: (0.003)(R -0.021, F 0.027)]  [G loss: -0.027] \n",
      "2099 [D loss: (0.003)(R -0.021, F 0.027)]  [G loss: -0.027] \n",
      "2099 [D loss: (0.004)(R -0.020, F 0.027)]  [G loss: -0.027] \n",
      "2100 [D loss: (0.003)(R -0.022, F 0.027)]  [G loss: -0.027] \n",
      "2100 [D loss: (0.000)(R -0.026, F 0.027)]  [G loss: -0.027] \n",
      "2101 [D loss: (0.003)(R -0.021, F 0.027)]  [G loss: -0.027] \n",
      "2101 [D loss: (0.001)(R -0.024, F 0.027)]  [G loss: -0.027] \n",
      "2102 [D loss: (0.004)(R -0.018, F 0.027)]  [G loss: -0.027] \n",
      "2102 [D loss: (0.000)(R -0.026, F 0.027)]  [G loss: -0.027] \n",
      "2103 [D loss: (0.002)(R -0.022, F 0.027)]  [G loss: -0.027] \n",
      "2103 [D loss: (0.003)(R -0.021, F 0.027)]  [G loss: -0.027] \n",
      "2104 [D loss: (0.002)(R -0.023, F 0.027)]  [G loss: -0.027] \n",
      "2104 [D loss: (0.002)(R -0.023, F 0.027)]  [G loss: -0.027] \n",
      "2105 [D loss: (0.001)(R -0.024, F 0.027)]  [G loss: -0.027] \n",
      "2105 [D loss: (0.001)(R -0.025, F 0.027)]  [G loss: -0.027] \n",
      "2106 [D loss: (0.000)(R -0.026, F 0.027)]  [G loss: -0.027] \n",
      "2106 [D loss: (0.001)(R -0.024, F 0.027)]  [G loss: -0.027] \n",
      "2107 [D loss: (0.000)(R -0.026, F 0.027)]  [G loss: -0.027] \n",
      "2107 [D loss: (0.002)(R -0.022, F 0.027)]  [G loss: -0.027] \n",
      "2108 [D loss: (0.002)(R -0.023, F 0.027)]  [G loss: -0.027] \n",
      "2108 [D loss: (0.002)(R -0.023, F 0.027)]  [G loss: -0.027] \n",
      "2109 [D loss: (0.001)(R -0.025, F 0.027)]  [G loss: -0.027] \n",
      "2109 [D loss: (0.001)(R -0.024, F 0.026)]  [G loss: -0.026] \n",
      "2110 [D loss: (0.001)(R -0.025, F 0.026)]  [G loss: -0.026] \n",
      "2110 [D loss: (0.001)(R -0.025, F 0.027)]  [G loss: -0.026] \n",
      "2111 [D loss: (0.001)(R -0.025, F 0.026)]  [G loss: -0.026] \n",
      "2111 [D loss: (0.001)(R -0.024, F 0.026)]  [G loss: -0.026] \n",
      "2112 [D loss: (0.001)(R -0.025, F 0.026)]  [G loss: -0.026] \n",
      "2112 [D loss: (0.001)(R -0.025, F 0.026)]  [G loss: -0.026] \n",
      "2113 [D loss: (0.001)(R -0.025, F 0.026)]  [G loss: -0.026] \n",
      "2113 [D loss: (0.001)(R -0.024, F 0.026)]  [G loss: -0.026] \n",
      "2114 [D loss: (0.001)(R -0.024, F 0.026)]  [G loss: -0.026] \n",
      "2114 [D loss: (0.001)(R -0.023, F 0.026)]  [G loss: -0.026] \n",
      "2115 [D loss: (0.001)(R -0.024, F 0.026)]  [G loss: -0.026] \n",
      "2115 [D loss: (0.002)(R -0.023, F 0.026)]  [G loss: -0.026] \n",
      "2116 [D loss: (0.001)(R -0.025, F 0.026)]  [G loss: -0.026] \n",
      "2116 [D loss: (0.001)(R -0.024, F 0.026)]  [G loss: -0.026] \n",
      "2117 [D loss: (0.001)(R -0.024, F 0.026)]  [G loss: -0.026] \n",
      "2117 [D loss: (0.001)(R -0.024, F 0.026)]  [G loss: -0.026] \n",
      "2118 [D loss: (0.001)(R -0.025, F 0.026)]  [G loss: -0.026] \n",
      "2118 [D loss: (0.001)(R -0.024, F 0.026)]  [G loss: -0.026] \n",
      "2119 [D loss: (0.001)(R -0.024, F 0.026)]  [G loss: -0.026] \n",
      "2119 [D loss: (0.001)(R -0.025, F 0.026)]  [G loss: -0.026] \n",
      "2120 [D loss: (0.001)(R -0.025, F 0.026)]  [G loss: -0.026] \n",
      "2120 [D loss: (0.001)(R -0.024, F 0.026)]  [G loss: -0.026] \n",
      "2121 [D loss: (0.001)(R -0.025, F 0.026)]  [G loss: -0.026] \n",
      "2121 [D loss: (0.000)(R -0.025, F 0.026)]  [G loss: -0.026] \n",
      "2122 [D loss: (0.001)(R -0.024, F 0.026)]  [G loss: -0.026] \n",
      "2122 [D loss: (0.001)(R -0.024, F 0.026)]  [G loss: -0.026] \n",
      "2123 [D loss: (0.000)(R -0.025, F 0.026)]  [G loss: -0.026] \n",
      "2123 [D loss: (0.001)(R -0.025, F 0.026)]  [G loss: -0.026] \n",
      "2124 [D loss: (0.000)(R -0.025, F 0.026)]  [G loss: -0.026] \n",
      "2124 [D loss: (0.000)(R -0.025, F 0.026)]  [G loss: -0.026] \n",
      "2125 [D loss: (0.000)(R -0.025, F 0.026)]  [G loss: -0.026] \n",
      "2125 [D loss: (0.000)(R -0.025, F 0.026)]  [G loss: -0.026] \n",
      "2126 [D loss: (0.000)(R -0.025, F 0.026)]  [G loss: -0.026] \n",
      "2126 [D loss: (0.000)(R -0.025, F 0.026)]  [G loss: -0.026] \n",
      "2127 [D loss: (0.000)(R -0.025, F 0.026)]  [G loss: -0.026] \n",
      "2127 [D loss: (0.000)(R -0.025, F 0.026)]  [G loss: -0.026] \n",
      "2128 [D loss: (0.000)(R -0.025, F 0.026)]  [G loss: -0.026] \n",
      "2128 [D loss: (0.000)(R -0.025, F 0.026)]  [G loss: -0.026] \n",
      "2129 [D loss: (0.001)(R -0.025, F 0.026)]  [G loss: -0.026] \n",
      "2129 [D loss: (0.000)(R -0.025, F 0.026)]  [G loss: -0.026] \n",
      "2130 [D loss: (0.000)(R -0.025, F 0.026)]  [G loss: -0.026] \n",
      "2130 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.026] \n",
      "2131 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2131 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2132 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2132 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2133 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2133 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2134 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2134 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2135 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2135 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2136 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2136 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2137 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2137 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2138 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2138 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2139 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2139 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2140 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2140 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2141 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2141 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2142 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2142 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2143 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2143 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2144 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2144 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2145 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2145 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2146 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2146 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2147 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2147 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2148 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2148 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2149 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2149 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2150 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2150 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2151 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2151 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2152 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2152 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2153 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2153 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2154 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2154 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2155 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2155 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2156 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2156 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2157 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2157 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2158 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2158 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2159 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2159 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2160 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2160 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2161 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2161 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2162 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2162 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2163 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2163 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2164 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2164 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2165 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2165 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2166 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2166 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2167 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2167 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2168 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2168 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2169 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2169 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2170 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2170 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2171 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2171 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2172 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2172 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2173 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2173 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2174 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2174 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2175 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2175 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2176 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2176 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2177 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2177 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2178 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2178 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2179 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2179 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2180 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2180 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2181 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2181 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2182 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2182 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2183 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2183 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2184 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2184 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2185 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2185 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2186 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2186 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2187 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2187 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2188 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2188 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2189 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2189 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2190 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2190 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2191 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2191 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2192 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2192 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2193 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2193 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2194 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2194 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2195 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2195 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2196 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2196 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2197 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2197 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2198 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2198 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2199 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2199 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2200 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2200 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2201 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2201 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2202 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2202 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2203 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2203 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2204 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2204 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2205 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2205 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2206 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2206 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2207 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2207 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2208 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2208 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2209 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2209 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2210 [D loss: (0.000)(R -0.024, F 0.025)]  [G loss: -0.025] \n",
      "2210 [D loss: (0.000)(R -0.025, F 0.025)]  [G loss: -0.025] \n",
      "2211 [D loss: (0.000)(R -0.024, F 0.025)]  [G loss: -0.025] \n",
      "2211 [D loss: (0.000)(R -0.024, F 0.025)]  [G loss: -0.025] \n",
      "2212 [D loss: (0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2212 [D loss: (0.000)(R -0.024, F 0.025)]  [G loss: -0.024] \n",
      "2213 [D loss: (0.000)(R -0.024, F 0.025)]  [G loss: -0.024] \n",
      "2213 [D loss: (0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2214 [D loss: (0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2214 [D loss: (0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2215 [D loss: (0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2215 [D loss: (0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2216 [D loss: (0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2216 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2217 [D loss: (0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2217 [D loss: (0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2218 [D loss: (0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2218 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2219 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2219 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2220 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2220 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2221 [D loss: (0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2221 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2222 [D loss: (0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2222 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2223 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2223 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2224 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2224 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2225 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2225 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2226 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2226 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2227 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2227 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2228 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2228 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2229 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2229 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2230 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2230 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2231 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2231 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2232 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2232 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2233 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2233 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2234 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2234 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2235 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2235 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2236 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2236 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2237 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2237 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2238 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2238 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2239 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2239 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2240 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2240 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2241 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2241 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2242 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2242 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2243 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2243 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2244 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2244 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2245 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2245 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2246 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2246 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2247 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2247 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2248 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2248 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2249 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2249 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2250 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2250 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2251 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2251 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2252 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2252 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2253 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2253 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2254 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2254 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2255 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2255 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2256 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2256 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2257 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2257 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2258 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2258 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2259 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2259 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2260 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2260 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2261 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2261 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2262 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2262 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2263 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2263 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2264 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2264 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2265 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2265 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2266 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2266 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2267 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2267 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2268 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2268 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2269 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2269 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2270 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2270 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2271 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2271 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2272 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2272 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2273 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2273 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2274 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2274 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2275 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2275 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2276 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2276 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2277 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2277 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2278 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2278 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2279 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2279 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2280 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2280 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.023] \n",
      "2281 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.023] \n",
      "2281 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2282 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2282 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2283 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2283 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2284 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2284 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2285 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2285 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2286 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2286 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2287 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2287 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2288 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2288 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2289 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2289 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2290 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2290 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2291 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2291 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2292 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2292 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2293 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2293 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2294 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2294 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2295 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2295 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2296 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2296 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2297 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2297 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2298 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2298 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2299 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2299 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2300 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2300 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2301 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2301 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2302 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2302 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2303 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2303 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2304 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2304 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2305 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2305 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2306 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2306 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2307 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2307 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2308 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2308 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2309 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2309 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2310 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2310 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2311 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2311 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2312 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2312 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2313 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2313 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2314 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2314 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2315 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2315 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2316 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2316 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2317 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2317 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2318 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2318 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2319 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2319 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2320 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2320 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2321 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2321 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2322 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2322 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2323 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2323 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2324 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2324 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.023] \n",
      "2325 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2325 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2326 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.022] \n",
      "2326 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2327 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2327 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2328 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2328 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2329 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2329 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2330 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2330 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2331 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2331 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2332 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2332 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2333 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2333 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2334 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2334 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2335 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2335 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2336 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2336 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2337 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2337 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2338 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2338 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2339 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2339 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2340 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2340 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2341 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2341 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2342 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2342 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2343 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2343 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2344 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2344 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2345 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2345 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2346 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2346 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2347 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2347 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2348 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2348 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2349 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2349 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2350 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2350 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2351 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2351 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2352 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2352 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2353 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2353 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2354 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2354 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2355 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2355 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2356 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2356 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2357 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2357 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2358 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2358 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2359 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2359 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2360 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2360 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2361 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2361 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2362 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2362 [D loss: (-0.000)(R -0.022, F 0.022)]  [G loss: -0.022] \n",
      "2363 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2363 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2364 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2364 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2365 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2365 [D loss: (-0.000)(R -0.022, F 0.022)]  [G loss: -0.022] \n",
      "2366 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2366 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2367 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2367 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2368 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2368 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2369 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2369 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2370 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2370 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2371 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2371 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2372 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2372 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2373 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2373 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2374 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2374 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2375 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2375 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2376 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2376 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2377 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2377 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2378 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2378 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2379 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2379 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2380 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2380 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2381 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2381 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2382 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2382 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2383 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2383 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2384 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2384 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2385 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2385 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2386 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2386 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2387 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2387 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2388 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2388 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2389 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2389 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2390 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2390 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2391 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2391 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2392 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2392 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2393 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2393 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2394 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2394 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2395 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2395 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2396 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2396 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2397 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2397 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2398 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2398 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2399 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2399 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2400 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2400 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2401 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2401 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2402 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2402 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2403 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2403 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2404 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2404 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2405 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2405 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2406 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2406 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2407 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2407 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2408 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2408 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2409 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2409 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2410 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2410 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2411 [D loss: (-0.001)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2411 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2412 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2412 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2413 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2413 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2414 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2414 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2415 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2415 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2416 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2416 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2417 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2417 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2418 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2418 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2419 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2419 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2420 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2420 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2421 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2421 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2422 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2422 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2423 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2423 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2424 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2424 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2425 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2425 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2426 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2426 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2427 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2427 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2428 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2428 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2429 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2429 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2430 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2430 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2431 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2431 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2432 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2432 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2433 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2433 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2434 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2434 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2435 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2435 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2436 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2436 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2437 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2437 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2438 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2438 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2439 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2439 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2440 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2440 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2441 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2441 [D loss: (-0.001)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2442 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2442 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2443 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2443 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2444 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2444 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2445 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2445 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2446 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2446 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2447 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2447 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2448 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.022] \n",
      "2448 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2449 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2449 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2450 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2450 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2451 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2451 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2452 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2452 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2453 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2453 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2454 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2454 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2455 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2455 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2456 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2456 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.022] \n",
      "2457 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2457 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2458 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2458 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2459 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2459 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2460 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2460 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.022] \n",
      "2461 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2461 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.022] \n",
      "2462 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.022] \n",
      "2462 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2463 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.022] \n",
      "2463 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2464 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2464 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.023] \n",
      "2465 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2465 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.022] \n",
      "2466 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.022] \n",
      "2466 [D loss: (-0.001)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2467 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.022] \n",
      "2467 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2468 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2468 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2469 [D loss: (-0.001)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2469 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2470 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.023] \n",
      "2470 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.022] \n",
      "2471 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.022] \n",
      "2471 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2472 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2472 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2473 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.022] \n",
      "2473 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.023] \n",
      "2474 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.022] \n",
      "2474 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2475 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2475 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2476 [D loss: (-0.001)(R -0.024, F 0.022)]  [G loss: -0.023] \n",
      "2476 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2477 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2477 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.023] \n",
      "2478 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2478 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2479 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2479 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2480 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2480 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2481 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2481 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2482 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2482 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2483 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2483 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2484 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2484 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2485 [D loss: (-0.000)(R -0.023, F 0.022)]  [G loss: -0.023] \n",
      "2485 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2486 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2486 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2487 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2487 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2488 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2488 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2489 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2489 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2490 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2490 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2491 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2491 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2492 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2492 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2493 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2493 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2494 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2494 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2495 [D loss: (-0.000)(R -0.023, F 0.023)]  [G loss: -0.023] \n",
      "2495 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2496 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2496 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2497 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2497 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2498 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2498 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2499 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2499 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2500 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2500 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2501 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2501 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2502 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2502 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2503 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2503 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2504 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2504 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2505 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2505 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2506 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2506 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2507 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2507 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2508 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2508 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2509 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2509 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2510 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2510 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2511 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2511 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2512 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2512 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2513 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2513 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2514 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2514 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2515 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2515 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2516 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2516 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2517 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2517 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2518 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2518 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2519 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2519 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2520 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2520 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2521 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2521 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2522 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2522 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2523 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2523 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2524 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2524 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2525 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2525 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2526 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2526 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2527 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2527 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2528 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2528 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2529 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2529 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2530 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2530 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2531 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2531 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2532 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2532 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2533 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2533 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2534 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2534 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2535 [D loss: (-0.001)(R -0.025, F 0.023)]  [G loss: -0.023] \n",
      "2535 [D loss: (-0.001)(R -0.025, F 0.023)]  [G loss: -0.023] \n",
      "2536 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.023] \n",
      "2536 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2537 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2537 [D loss: (-0.001)(R -0.025, F 0.023)]  [G loss: -0.023] \n",
      "2538 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.023] \n",
      "2538 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.023] \n",
      "2539 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.023] \n",
      "2539 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.023] \n",
      "2540 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.023] \n",
      "2540 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.023] \n",
      "2541 [D loss: (-0.001)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2541 [D loss: (-0.001)(R -0.025, F 0.023)]  [G loss: -0.023] \n",
      "2542 [D loss: (-0.000)(R -0.024, F 0.023)]  [G loss: -0.023] \n",
      "2542 [D loss: (-0.001)(R -0.025, F 0.023)]  [G loss: -0.023] \n",
      "2543 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2543 [D loss: (-0.001)(R -0.025, F 0.023)]  [G loss: -0.024] \n",
      "2544 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2544 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.023] \n",
      "2545 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2545 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2546 [D loss: (-0.001)(R -0.025, F 0.023)]  [G loss: -0.024] \n",
      "2546 [D loss: (-0.001)(R -0.025, F 0.023)]  [G loss: -0.024] \n",
      "2547 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2547 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2548 [D loss: (-0.000)(R -0.024, F 0.024)]  [G loss: -0.024] \n",
      "2548 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2549 [D loss: (-0.000)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2549 [D loss: (-0.000)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2550 [D loss: (-0.000)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2550 [D loss: (-0.000)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2551 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2551 [D loss: (-0.000)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2552 [D loss: (-0.000)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2552 [D loss: (-0.000)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2553 [D loss: (-0.000)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2553 [D loss: (-0.000)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2554 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2554 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2555 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2555 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2556 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2556 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2557 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2557 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2558 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2558 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2559 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2559 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2560 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2560 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2561 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2561 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2562 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2562 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2563 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2563 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2564 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2564 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2565 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2565 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2566 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2566 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2567 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2567 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2568 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2568 [D loss: (-0.001)(R -0.026, F 0.024)]  [G loss: -0.024] \n",
      "2569 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2569 [D loss: (-0.000)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2570 [D loss: (-0.000)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2570 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2571 [D loss: (-0.001)(R -0.026, F 0.024)]  [G loss: -0.024] \n",
      "2571 [D loss: (-0.001)(R -0.026, F 0.024)]  [G loss: -0.024] \n",
      "2572 [D loss: (-0.001)(R -0.026, F 0.024)]  [G loss: -0.024] \n",
      "2572 [D loss: (-0.001)(R -0.026, F 0.024)]  [G loss: -0.024] \n",
      "2573 [D loss: (-0.001)(R -0.026, F 0.024)]  [G loss: -0.024] \n",
      "2573 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2574 [D loss: (-0.001)(R -0.026, F 0.024)]  [G loss: -0.024] \n",
      "2574 [D loss: (-0.001)(R -0.025, F 0.024)]  [G loss: -0.024] \n",
      "2575 [D loss: (-0.001)(R -0.026, F 0.024)]  [G loss: -0.024] \n",
      "2575 [D loss: (-0.001)(R -0.026, F 0.024)]  [G loss: -0.024] \n",
      "2576 [D loss: (-0.001)(R -0.026, F 0.024)]  [G loss: -0.024] \n",
      "2576 [D loss: (-0.001)(R -0.026, F 0.024)]  [G loss: -0.024] \n",
      "2577 [D loss: (-0.001)(R -0.026, F 0.024)]  [G loss: -0.024] \n",
      "2577 [D loss: (-0.001)(R -0.026, F 0.024)]  [G loss: -0.024] \n",
      "2578 [D loss: (-0.001)(R -0.026, F 0.024)]  [G loss: -0.024] \n",
      "2578 [D loss: (-0.001)(R -0.026, F 0.024)]  [G loss: -0.024] \n",
      "2579 [D loss: (-0.001)(R -0.026, F 0.024)]  [G loss: -0.025] \n",
      "2579 [D loss: (-0.001)(R -0.026, F 0.024)]  [G loss: -0.024] \n",
      "2580 [D loss: (-0.001)(R -0.026, F 0.025)]  [G loss: -0.024] \n",
      "2580 [D loss: (-0.001)(R -0.026, F 0.025)]  [G loss: -0.025] \n",
      "2581 [D loss: (-0.001)(R -0.026, F 0.025)]  [G loss: -0.025] \n",
      "2581 [D loss: (-0.001)(R -0.026, F 0.025)]  [G loss: -0.025] \n",
      "2582 [D loss: (-0.001)(R -0.026, F 0.025)]  [G loss: -0.025] \n",
      "2582 [D loss: (-0.001)(R -0.026, F 0.025)]  [G loss: -0.025] \n",
      "2583 [D loss: (-0.001)(R -0.026, F 0.025)]  [G loss: -0.025] \n",
      "2583 [D loss: (-0.001)(R -0.026, F 0.025)]  [G loss: -0.025] \n",
      "2584 [D loss: (-0.001)(R -0.026, F 0.025)]  [G loss: -0.025] \n",
      "2584 [D loss: (-0.001)(R -0.026, F 0.025)]  [G loss: -0.025] \n",
      "2585 [D loss: (-0.001)(R -0.026, F 0.025)]  [G loss: -0.025] \n",
      "2585 [D loss: (-0.001)(R -0.026, F 0.025)]  [G loss: -0.025] \n",
      "2586 [D loss: (-0.001)(R -0.026, F 0.025)]  [G loss: -0.025] \n",
      "2586 [D loss: (-0.001)(R -0.026, F 0.025)]  [G loss: -0.025] \n",
      "2587 [D loss: (-0.001)(R -0.026, F 0.025)]  [G loss: -0.025] \n",
      "2587 [D loss: (-0.001)(R -0.026, F 0.025)]  [G loss: -0.025] \n",
      "2588 [D loss: (-0.001)(R -0.026, F 0.025)]  [G loss: -0.025] \n",
      "2588 [D loss: (-0.001)(R -0.026, F 0.025)]  [G loss: -0.025] \n",
      "2589 [D loss: (-0.001)(R -0.026, F 0.025)]  [G loss: -0.025] \n",
      "2589 [D loss: (-0.001)(R -0.026, F 0.025)]  [G loss: -0.025] \n",
      "2590 [D loss: (-0.001)(R -0.026, F 0.025)]  [G loss: -0.025] \n",
      "2590 [D loss: (-0.001)(R -0.026, F 0.025)]  [G loss: -0.025] \n",
      "2591 [D loss: (-0.001)(R -0.026, F 0.025)]  [G loss: -0.025] \n",
      "2591 [D loss: (-0.001)(R -0.026, F 0.025)]  [G loss: -0.025] \n",
      "2592 [D loss: (-0.001)(R -0.026, F 0.025)]  [G loss: -0.025] \n",
      "2592 [D loss: (-0.001)(R -0.027, F 0.025)]  [G loss: -0.025] \n",
      "2593 [D loss: (-0.001)(R -0.026, F 0.025)]  [G loss: -0.025] \n",
      "2593 [D loss: (-0.001)(R -0.027, F 0.025)]  [G loss: -0.025] \n",
      "2594 [D loss: (-0.001)(R -0.027, F 0.025)]  [G loss: -0.025] \n",
      "2594 [D loss: (-0.001)(R -0.027, F 0.025)]  [G loss: -0.025] \n",
      "2595 [D loss: (-0.001)(R -0.026, F 0.025)]  [G loss: -0.025] \n",
      "2595 [D loss: (-0.001)(R -0.027, F 0.025)]  [G loss: -0.025] \n",
      "2596 [D loss: (-0.001)(R -0.027, F 0.025)]  [G loss: -0.025] \n",
      "2596 [D loss: (-0.001)(R -0.027, F 0.025)]  [G loss: -0.025] \n",
      "2597 [D loss: (-0.001)(R -0.027, F 0.025)]  [G loss: -0.025] \n",
      "2597 [D loss: (-0.001)(R -0.027, F 0.025)]  [G loss: -0.025] \n",
      "2598 [D loss: (-0.001)(R -0.027, F 0.025)]  [G loss: -0.025] \n",
      "2598 [D loss: (-0.001)(R -0.027, F 0.025)]  [G loss: -0.025] \n",
      "2599 [D loss: (-0.001)(R -0.027, F 0.025)]  [G loss: -0.025] \n",
      "2599 [D loss: (-0.001)(R -0.027, F 0.025)]  [G loss: -0.025] \n",
      "2600 [D loss: (-0.001)(R -0.027, F 0.025)]  [G loss: -0.025] \n",
      "2600 [D loss: (-0.001)(R -0.027, F 0.025)]  [G loss: -0.025] \n",
      "2601 [D loss: (-0.001)(R -0.027, F 0.025)]  [G loss: -0.025] \n",
      "2601 [D loss: (-0.001)(R -0.027, F 0.025)]  [G loss: -0.025] \n",
      "2602 [D loss: (-0.001)(R -0.027, F 0.025)]  [G loss: -0.025] \n",
      "2602 [D loss: (-0.001)(R -0.027, F 0.025)]  [G loss: -0.025] \n",
      "2603 [D loss: (-0.001)(R -0.027, F 0.025)]  [G loss: -0.025] \n",
      "2603 [D loss: (-0.001)(R -0.027, F 0.025)]  [G loss: -0.025] \n",
      "2604 [D loss: (-0.001)(R -0.027, F 0.025)]  [G loss: -0.025] \n",
      "2604 [D loss: (-0.001)(R -0.027, F 0.025)]  [G loss: -0.025] \n",
      "2605 [D loss: (-0.001)(R -0.027, F 0.026)]  [G loss: -0.026] \n",
      "2605 [D loss: (-0.001)(R -0.027, F 0.026)]  [G loss: -0.025] \n",
      "2606 [D loss: (-0.001)(R -0.028, F 0.025)]  [G loss: -0.026] \n",
      "2606 [D loss: (-0.001)(R -0.027, F 0.025)]  [G loss: -0.026] \n",
      "2607 [D loss: (-0.001)(R -0.027, F 0.025)]  [G loss: -0.026] \n",
      "2607 [D loss: (-0.001)(R -0.027, F 0.025)]  [G loss: -0.026] \n",
      "2608 [D loss: (-0.001)(R -0.027, F 0.025)]  [G loss: -0.026] \n",
      "2608 [D loss: (-0.001)(R -0.027, F 0.026)]  [G loss: -0.026] \n",
      "2609 [D loss: (-0.001)(R -0.027, F 0.026)]  [G loss: -0.026] \n",
      "2609 [D loss: (-0.001)(R -0.027, F 0.026)]  [G loss: -0.026] \n",
      "2610 [D loss: (-0.001)(R -0.027, F 0.026)]  [G loss: -0.026] \n",
      "2610 [D loss: (-0.001)(R -0.028, F 0.026)]  [G loss: -0.026] \n",
      "2611 [D loss: (-0.001)(R -0.028, F 0.026)]  [G loss: -0.026] \n",
      "2611 [D loss: (-0.001)(R -0.027, F 0.026)]  [G loss: -0.026] \n",
      "2612 [D loss: (-0.001)(R -0.027, F 0.026)]  [G loss: -0.026] \n",
      "2612 [D loss: (-0.001)(R -0.028, F 0.026)]  [G loss: -0.026] \n",
      "2613 [D loss: (-0.001)(R -0.027, F 0.026)]  [G loss: -0.026] \n",
      "2613 [D loss: (-0.001)(R -0.027, F 0.026)]  [G loss: -0.026] \n",
      "2614 [D loss: (-0.001)(R -0.028, F 0.026)]  [G loss: -0.026] \n",
      "2614 [D loss: (-0.001)(R -0.028, F 0.026)]  [G loss: -0.026] \n",
      "2615 [D loss: (-0.001)(R -0.028, F 0.026)]  [G loss: -0.026] \n",
      "2615 [D loss: (-0.001)(R -0.028, F 0.026)]  [G loss: -0.026] \n",
      "2616 [D loss: (-0.001)(R -0.028, F 0.026)]  [G loss: -0.026] \n",
      "2616 [D loss: (-0.001)(R -0.028, F 0.026)]  [G loss: -0.026] \n",
      "2617 [D loss: (-0.001)(R -0.028, F 0.026)]  [G loss: -0.026] \n",
      "2617 [D loss: (-0.001)(R -0.028, F 0.026)]  [G loss: -0.026] \n",
      "2618 [D loss: (-0.001)(R -0.028, F 0.026)]  [G loss: -0.026] \n",
      "2618 [D loss: (-0.001)(R -0.028, F 0.026)]  [G loss: -0.026] \n",
      "2619 [D loss: (-0.001)(R -0.028, F 0.026)]  [G loss: -0.026] \n",
      "2619 [D loss: (-0.001)(R -0.028, F 0.026)]  [G loss: -0.026] \n",
      "2620 [D loss: (-0.001)(R -0.028, F 0.026)]  [G loss: -0.026] \n",
      "2620 [D loss: (-0.001)(R -0.028, F 0.026)]  [G loss: -0.026] \n",
      "2621 [D loss: (-0.001)(R -0.028, F 0.026)]  [G loss: -0.026] \n",
      "2621 [D loss: (-0.001)(R -0.028, F 0.026)]  [G loss: -0.026] \n",
      "2622 [D loss: (-0.001)(R -0.028, F 0.026)]  [G loss: -0.026] \n",
      "2622 [D loss: (-0.001)(R -0.028, F 0.026)]  [G loss: -0.026] \n",
      "2623 [D loss: (-0.001)(R -0.029, F 0.026)]  [G loss: -0.026] \n",
      "2623 [D loss: (-0.001)(R -0.029, F 0.026)]  [G loss: -0.026] \n",
      "2624 [D loss: (-0.001)(R -0.029, F 0.026)]  [G loss: -0.026] \n",
      "2624 [D loss: (-0.001)(R -0.029, F 0.026)]  [G loss: -0.026] \n",
      "2625 [D loss: (-0.001)(R -0.029, F 0.027)]  [G loss: -0.027] \n",
      "2625 [D loss: (-0.001)(R -0.029, F 0.027)]  [G loss: -0.026] \n",
      "2626 [D loss: (-0.001)(R -0.029, F 0.027)]  [G loss: -0.027] \n",
      "2626 [D loss: (-0.001)(R -0.028, F 0.027)]  [G loss: -0.027] \n",
      "2627 [D loss: (-0.001)(R -0.029, F 0.027)]  [G loss: -0.027] \n",
      "2627 [D loss: (-0.001)(R -0.029, F 0.027)]  [G loss: -0.027] \n",
      "2628 [D loss: (-0.001)(R -0.029, F 0.027)]  [G loss: -0.027] \n",
      "2628 [D loss: (-0.001)(R -0.029, F 0.027)]  [G loss: -0.027] \n",
      "2629 [D loss: (-0.001)(R -0.029, F 0.027)]  [G loss: -0.027] \n",
      "2629 [D loss: (-0.001)(R -0.029, F 0.027)]  [G loss: -0.027] \n",
      "2630 [D loss: (-0.001)(R -0.029, F 0.027)]  [G loss: -0.027] \n",
      "2630 [D loss: (-0.001)(R -0.029, F 0.027)]  [G loss: -0.027] \n",
      "2631 [D loss: (-0.001)(R -0.029, F 0.027)]  [G loss: -0.027] \n",
      "2631 [D loss: (-0.001)(R -0.029, F 0.027)]  [G loss: -0.027] \n",
      "2632 [D loss: (-0.001)(R -0.029, F 0.027)]  [G loss: -0.027] \n",
      "2632 [D loss: (-0.001)(R -0.029, F 0.027)]  [G loss: -0.027] \n",
      "2633 [D loss: (-0.001)(R -0.029, F 0.027)]  [G loss: -0.027] \n",
      "2633 [D loss: (-0.001)(R -0.030, F 0.027)]  [G loss: -0.027] \n",
      "2634 [D loss: (-0.001)(R -0.029, F 0.027)]  [G loss: -0.027] \n",
      "2634 [D loss: (-0.001)(R -0.029, F 0.027)]  [G loss: -0.027] \n",
      "2635 [D loss: (-0.001)(R -0.030, F 0.027)]  [G loss: -0.027] \n",
      "2635 [D loss: (-0.001)(R -0.029, F 0.027)]  [G loss: -0.027] \n",
      "2636 [D loss: (-0.001)(R -0.030, F 0.027)]  [G loss: -0.027] \n",
      "2636 [D loss: (-0.001)(R -0.030, F 0.027)]  [G loss: -0.027] \n",
      "2637 [D loss: (-0.001)(R -0.030, F 0.027)]  [G loss: -0.027] \n",
      "2637 [D loss: (-0.001)(R -0.030, F 0.027)]  [G loss: -0.027] \n",
      "2638 [D loss: (-0.001)(R -0.030, F 0.027)]  [G loss: -0.027] \n",
      "2638 [D loss: (-0.001)(R -0.030, F 0.027)]  [G loss: -0.027] \n",
      "2639 [D loss: (-0.001)(R -0.030, F 0.028)]  [G loss: -0.028] \n",
      "2639 [D loss: (-0.002)(R -0.031, F 0.028)]  [G loss: -0.028] \n",
      "2640 [D loss: (-0.001)(R -0.030, F 0.027)]  [G loss: -0.028] \n",
      "2640 [D loss: (-0.001)(R -0.030, F 0.028)]  [G loss: -0.028] \n",
      "2641 [D loss: (-0.001)(R -0.030, F 0.028)]  [G loss: -0.028] \n",
      "2641 [D loss: (-0.001)(R -0.030, F 0.028)]  [G loss: -0.028] \n",
      "2642 [D loss: (-0.001)(R -0.030, F 0.028)]  [G loss: -0.028] \n",
      "2642 [D loss: (-0.001)(R -0.031, F 0.028)]  [G loss: -0.028] \n",
      "2643 [D loss: (-0.001)(R -0.031, F 0.028)]  [G loss: -0.028] \n",
      "2643 [D loss: (-0.001)(R -0.031, F 0.028)]  [G loss: -0.028] \n",
      "2644 [D loss: (-0.001)(R -0.031, F 0.028)]  [G loss: -0.028] \n",
      "2644 [D loss: (-0.001)(R -0.031, F 0.028)]  [G loss: -0.028] \n",
      "2645 [D loss: (-0.001)(R -0.031, F 0.028)]  [G loss: -0.028] \n",
      "2645 [D loss: (-0.001)(R -0.030, F 0.028)]  [G loss: -0.028] \n",
      "2646 [D loss: (-0.001)(R -0.031, F 0.028)]  [G loss: -0.028] \n",
      "2646 [D loss: (-0.001)(R -0.031, F 0.028)]  [G loss: -0.028] \n",
      "2647 [D loss: (-0.002)(R -0.031, F 0.028)]  [G loss: -0.028] \n",
      "2647 [D loss: (-0.001)(R -0.031, F 0.028)]  [G loss: -0.028] \n",
      "2648 [D loss: (-0.001)(R -0.031, F 0.028)]  [G loss: -0.028] \n",
      "2648 [D loss: (-0.002)(R -0.032, F 0.028)]  [G loss: -0.028] \n",
      "2649 [D loss: (-0.001)(R -0.031, F 0.029)]  [G loss: -0.028] \n",
      "2649 [D loss: (-0.001)(R -0.031, F 0.029)]  [G loss: -0.028] \n",
      "2650 [D loss: (-0.002)(R -0.032, F 0.028)]  [G loss: -0.029] \n",
      "2650 [D loss: (-0.001)(R -0.031, F 0.029)]  [G loss: -0.028] \n",
      "2651 [D loss: (-0.001)(R -0.031, F 0.029)]  [G loss: -0.028] \n",
      "2651 [D loss: (-0.001)(R -0.032, F 0.029)]  [G loss: -0.029] \n",
      "2652 [D loss: (-0.001)(R -0.032, F 0.029)]  [G loss: -0.029] \n",
      "2652 [D loss: (-0.001)(R -0.032, F 0.029)]  [G loss: -0.029] \n",
      "2653 [D loss: (-0.002)(R -0.032, F 0.029)]  [G loss: -0.029] \n",
      "2653 [D loss: (-0.002)(R -0.032, F 0.029)]  [G loss: -0.029] \n",
      "2654 [D loss: (-0.001)(R -0.032, F 0.029)]  [G loss: -0.029] \n",
      "2654 [D loss: (-0.002)(R -0.032, F 0.029)]  [G loss: -0.029] \n",
      "2655 [D loss: (-0.002)(R -0.032, F 0.029)]  [G loss: -0.029] \n",
      "2655 [D loss: (-0.002)(R -0.033, F 0.029)]  [G loss: -0.029] \n",
      "2656 [D loss: (-0.002)(R -0.033, F 0.029)]  [G loss: -0.029] \n",
      "2656 [D loss: (-0.002)(R -0.033, F 0.029)]  [G loss: -0.029] \n",
      "2657 [D loss: (-0.002)(R -0.033, F 0.029)]  [G loss: -0.029] \n",
      "2657 [D loss: (-0.002)(R -0.033, F 0.029)]  [G loss: -0.029] \n",
      "2658 [D loss: (-0.002)(R -0.033, F 0.029)]  [G loss: -0.029] \n",
      "2658 [D loss: (-0.002)(R -0.033, F 0.029)]  [G loss: -0.029] \n",
      "2659 [D loss: (-0.002)(R -0.033, F 0.029)]  [G loss: -0.029] \n",
      "2659 [D loss: (-0.002)(R -0.033, F 0.030)]  [G loss: -0.029] \n",
      "2660 [D loss: (-0.002)(R -0.033, F 0.029)]  [G loss: -0.029] \n",
      "2660 [D loss: (-0.001)(R -0.032, F 0.030)]  [G loss: -0.029] \n",
      "2661 [D loss: (-0.002)(R -0.033, F 0.030)]  [G loss: -0.030] \n",
      "2661 [D loss: (-0.002)(R -0.034, F 0.030)]  [G loss: -0.029] \n",
      "2662 [D loss: (-0.002)(R -0.033, F 0.030)]  [G loss: -0.030] \n",
      "2662 [D loss: (-0.002)(R -0.034, F 0.030)]  [G loss: -0.030] \n",
      "2663 [D loss: (-0.002)(R -0.034, F 0.030)]  [G loss: -0.030] \n",
      "2663 [D loss: (-0.002)(R -0.034, F 0.030)]  [G loss: -0.030] \n",
      "2664 [D loss: (-0.002)(R -0.034, F 0.030)]  [G loss: -0.030] \n",
      "2664 [D loss: (-0.002)(R -0.034, F 0.030)]  [G loss: -0.030] \n",
      "2665 [D loss: (-0.002)(R -0.035, F 0.030)]  [G loss: -0.030] \n",
      "2665 [D loss: (-0.002)(R -0.035, F 0.030)]  [G loss: -0.030] \n",
      "2666 [D loss: (-0.002)(R -0.034, F 0.030)]  [G loss: -0.030] \n",
      "2666 [D loss: (-0.002)(R -0.035, F 0.030)]  [G loss: -0.030] \n",
      "2667 [D loss: (-0.002)(R -0.034, F 0.030)]  [G loss: -0.030] \n",
      "2667 [D loss: (-0.002)(R -0.035, F 0.030)]  [G loss: -0.030] \n",
      "2668 [D loss: (-0.002)(R -0.035, F 0.030)]  [G loss: -0.030] \n",
      "2668 [D loss: (-0.002)(R -0.035, F 0.030)]  [G loss: -0.030] \n",
      "2669 [D loss: (-0.003)(R -0.036, F 0.031)]  [G loss: -0.030] \n",
      "2669 [D loss: (-0.002)(R -0.035, F 0.031)]  [G loss: -0.030] \n",
      "2670 [D loss: (-0.002)(R -0.035, F 0.030)]  [G loss: -0.031] \n",
      "2670 [D loss: (-0.002)(R -0.035, F 0.031)]  [G loss: -0.031] \n",
      "2671 [D loss: (-0.003)(R -0.036, F 0.031)]  [G loss: -0.031] \n",
      "2671 [D loss: (-0.003)(R -0.036, F 0.031)]  [G loss: -0.031] \n",
      "2672 [D loss: (-0.003)(R -0.036, F 0.031)]  [G loss: -0.031] \n",
      "2672 [D loss: (-0.003)(R -0.036, F 0.031)]  [G loss: -0.031] \n",
      "2673 [D loss: (-0.003)(R -0.036, F 0.031)]  [G loss: -0.031] \n",
      "2673 [D loss: (-0.002)(R -0.035, F 0.031)]  [G loss: -0.031] \n",
      "2674 [D loss: (-0.003)(R -0.036, F 0.031)]  [G loss: -0.030] \n",
      "2674 [D loss: (-0.003)(R -0.036, F 0.030)]  [G loss: -0.030] \n",
      "2675 [D loss: (-0.003)(R -0.036, F 0.029)]  [G loss: -0.029] \n",
      "2675 [D loss: (-0.003)(R -0.034, F 0.027)]  [G loss: -0.027] \n",
      "2676 [D loss: (-0.004)(R -0.033, F 0.025)]  [G loss: -0.024] \n",
      "2676 [D loss: (-0.006)(R -0.031, F 0.018)]  [G loss: -0.013] \n",
      "2677 [D loss: (-0.158)(R 0.153, F -0.469)]  [G loss: 0.593] \n",
      "2677 [D loss: (-0.369)(R 0.793, F -1.531)]  [G loss: 1.664] \n",
      "2678 [D loss: (-0.590)(R 1.431, F -2.611)]  [G loss: 2.782] \n",
      "2678 [D loss: (-0.846)(R 2.108, F -3.800)]  [G loss: 4.037] \n",
      "2679 [D loss: (-1.016)(R 3.126, F -5.159)]  [G loss: 5.410] \n",
      "2679 [D loss: (-1.338)(R 3.938, F -6.613)]  [G loss: 6.838] \n",
      "2680 [D loss: (-1.631)(R 4.918, F -8.179)]  [G loss: 8.490] \n",
      "2680 [D loss: (-1.796)(R 6.268, F -9.860)]  [G loss: 10.201] \n",
      "2681 [D loss: (-2.230)(R 6.709, F -11.168)]  [G loss: 11.757] \n",
      "2681 [D loss: (-2.330)(R 8.223, F -12.883)]  [G loss: 13.502] \n",
      "2682 [D loss: (-2.847)(R 9.048, F -14.742)]  [G loss: 14.854] \n",
      "2682 [D loss: (-3.362)(R 9.947, F -16.671)]  [G loss: 16.697] \n",
      "2683 [D loss: (-3.592)(R 11.196, F -18.380)]  [G loss: 18.284] \n",
      "2683 [D loss: (-3.496)(R 12.458, F -19.449)]  [G loss: 20.068] \n",
      "2684 [D loss: (-4.138)(R 13.389, F -21.666)]  [G loss: 21.582] \n",
      "2684 [D loss: (-4.415)(R 14.433, F -23.263)]  [G loss: 23.165] \n",
      "2685 [D loss: (-4.199)(R 15.999, F -24.397)]  [G loss: 24.493] \n",
      "2685 [D loss: (-4.736)(R 16.590, F -26.062)]  [G loss: 25.824] \n",
      "2686 [D loss: (-4.768)(R 17.390, F -26.926)]  [G loss: 27.379] \n",
      "2686 [D loss: (-4.453)(R 19.585, F -28.492)]  [G loss: 28.536] \n",
      "2687 [D loss: (-5.041)(R 19.245, F -29.327)]  [G loss: 29.484] \n",
      "2687 [D loss: (-4.904)(R 20.260, F -30.068)]  [G loss: 30.467] \n",
      "2688 [D loss: (-5.463)(R 20.815, F -31.741)]  [G loss: 31.929] \n",
      "2688 [D loss: (-5.538)(R 21.420, F -32.496)]  [G loss: 32.330] \n",
      "2689 [D loss: (-5.868)(R 22.088, F -33.825)]  [G loss: 33.712] \n",
      "2689 [D loss: (-6.035)(R 22.643, F -34.713)]  [G loss: 34.475] \n",
      "2690 [D loss: (-5.790)(R 24.343, F -35.924)]  [G loss: 35.210] \n",
      "2690 [D loss: (-5.378)(R 24.944, F -35.700)]  [G loss: 36.512] \n",
      "2691 [D loss: (-6.324)(R 24.555, F -37.203)]  [G loss: 36.615] \n",
      "2691 [D loss: (-5.705)(R 25.844, F -37.255)]  [G loss: 37.173] \n",
      "2692 [D loss: (-5.289)(R 27.148, F -37.726)]  [G loss: 38.380] \n",
      "2692 [D loss: (-6.194)(R 26.398, F -38.786)]  [G loss: 39.014] \n",
      "2693 [D loss: (-6.551)(R 25.929, F -39.031)]  [G loss: 39.608] \n",
      "2693 [D loss: (-6.990)(R 26.910, F -40.891)]  [G loss: 40.372] \n",
      "2694 [D loss: (-6.865)(R 27.376, F -41.105)]  [G loss: 40.886] \n",
      "2694 [D loss: (-6.811)(R 27.562, F -41.185)]  [G loss: 41.848] \n",
      "2695 [D loss: (-5.582)(R 29.906, F -41.070)]  [G loss: 41.875] \n",
      "2695 [D loss: (-5.763)(R 29.842, F -41.368)]  [G loss: 42.594] \n",
      "2696 [D loss: (-6.515)(R 29.843, F -42.873)]  [G loss: 42.943] \n",
      "2696 [D loss: (-6.456)(R 29.682, F -42.593)]  [G loss: 43.856] \n",
      "2697 [D loss: (-7.120)(R 30.171, F -44.411)]  [G loss: 44.271] \n",
      "2697 [D loss: (-4.921)(R 33.173, F -43.015)]  [G loss: 44.755] \n",
      "2698 [D loss: (-7.404)(R 30.928, F -45.735)]  [G loss: 45.595] \n",
      "2698 [D loss: (-6.707)(R 31.918, F -45.333)]  [G loss: 45.759] \n",
      "2699 [D loss: (-6.133)(R 33.099, F -45.365)]  [G loss: 45.891] \n",
      "2699 [D loss: (-6.081)(R 33.773, F -45.935)]  [G loss: 46.242] \n",
      "2700 [D loss: (-7.498)(R 32.203, F -47.198)]  [G loss: 46.195] \n",
      "2700 [D loss: (-6.634)(R 34.534, F -47.803)]  [G loss: 47.255] \n",
      "2701 [D loss: (-7.306)(R 32.373, F -46.984)]  [G loss: 47.605] \n",
      "2701 [D loss: (-7.123)(R 33.100, F -47.346)]  [G loss: 47.768] \n",
      "2702 [D loss: (-6.486)(R 35.167, F -48.139)]  [G loss: 48.139] \n",
      "2702 [D loss: (-6.106)(R 35.803, F -48.015)]  [G loss: 48.811] \n",
      "2703 [D loss: (-6.202)(R 35.987, F -48.391)]  [G loss: 48.786] \n",
      "2703 [D loss: (-6.845)(R 34.000, F -47.689)]  [G loss: 48.693] \n",
      "2704 [D loss: (-5.365)(R 36.360, F -47.090)]  [G loss: 49.306] \n",
      "2704 [D loss: (-6.715)(R 36.350, F -49.780)]  [G loss: 49.288] \n",
      "2705 [D loss: (-5.895)(R 36.735, F -48.524)]  [G loss: 49.136] \n",
      "2705 [D loss: (-5.845)(R 37.411, F -49.102)]  [G loss: 49.799] \n",
      "2706 [D loss: (-5.822)(R 36.201, F -47.846)]  [G loss: 49.792] \n",
      "2706 [D loss: (-5.946)(R 38.071, F -49.962)]  [G loss: 50.076] \n",
      "2707 [D loss: (-6.179)(R 37.453, F -49.811)]  [G loss: 50.427] \n",
      "2707 [D loss: (-6.305)(R 37.874, F -50.484)]  [G loss: 50.106] \n",
      "2708 [D loss: (-4.886)(R 39.984, F -49.756)]  [G loss: 50.227] \n",
      "2708 [D loss: (-5.895)(R 37.936, F -49.725)]  [G loss: 50.307] \n",
      "2709 [D loss: (-5.665)(R 37.912, F -49.242)]  [G loss: 50.222] \n",
      "2709 [D loss: (-5.303)(R 40.303, F -50.910)]  [G loss: 50.557] \n",
      "2710 [D loss: (-5.767)(R 39.083, F -50.616)]  [G loss: 50.363] \n",
      "2710 [D loss: (-5.343)(R 39.970, F -50.655)]  [G loss: 50.342] \n",
      "2711 [D loss: (-5.727)(R 39.158, F -50.611)]  [G loss: 50.190] \n",
      "2711 [D loss: (-5.585)(R 39.139, F -50.309)]  [G loss: 50.620] \n",
      "2712 [D loss: (-5.275)(R 39.810, F -50.359)]  [G loss: 50.832] \n",
      "2712 [D loss: (-4.867)(R 40.820, F -50.554)]  [G loss: 50.852] \n",
      "2713 [D loss: (-4.155)(R 40.932, F -49.243)]  [G loss: 50.527] \n",
      "2713 [D loss: (-5.177)(R 39.992, F -50.346)]  [G loss: 51.242] \n",
      "2714 [D loss: (-5.117)(R 40.363, F -50.597)]  [G loss: 50.728] \n",
      "2714 [D loss: (-4.818)(R 41.602, F -51.239)]  [G loss: 50.119] \n",
      "2715 [D loss: (-4.826)(R 39.700, F -49.351)]  [G loss: 50.645] \n",
      "2715 [D loss: (-4.113)(R 41.132, F -49.357)]  [G loss: 50.022] \n",
      "2716 [D loss: (-4.656)(R 40.483, F -49.796)]  [G loss: 50.430] \n",
      "2716 [D loss: (-2.790)(R 42.633, F -48.214)]  [G loss: 50.357] \n",
      "2717 [D loss: (-4.754)(R 41.263, F -50.772)]  [G loss: 50.038] \n",
      "2717 [D loss: (-3.598)(R 40.388, F -47.583)]  [G loss: 50.162] \n",
      "2718 [D loss: (-4.160)(R 39.824, F -48.145)]  [G loss: 50.108] \n",
      "2718 [D loss: (-4.349)(R 41.646, F -50.343)]  [G loss: 49.547] \n",
      "2719 [D loss: (-2.085)(R 43.373, F -47.542)]  [G loss: 48.970] \n",
      "2719 [D loss: (-2.679)(R 41.350, F -46.708)]  [G loss: 49.662] \n",
      "2720 [D loss: (-3.671)(R 41.583, F -48.926)]  [G loss: 48.839] \n",
      "2720 [D loss: (-2.260)(R 44.217, F -48.737)]  [G loss: 48.977] \n",
      "2721 [D loss: (-3.917)(R 41.083, F -48.917)]  [G loss: 48.745] \n",
      "2721 [D loss: (-2.890)(R 42.435, F -48.215)]  [G loss: 48.777] \n",
      "2722 [D loss: (-2.701)(R 42.348, F -47.749)]  [G loss: 48.206] \n",
      "2722 [D loss: (-1.943)(R 42.627, F -46.513)]  [G loss: 47.464] \n",
      "2723 [D loss: (-3.777)(R 40.307, F -47.862)]  [G loss: 48.230] \n",
      "2723 [D loss: (-3.549)(R 40.954, F -48.051)]  [G loss: 48.080] \n",
      "2724 [D loss: (-1.708)(R 42.874, F -46.289)]  [G loss: 47.764] \n",
      "2724 [D loss: (-1.843)(R 41.674, F -45.359)]  [G loss: 47.591] \n",
      "2725 [D loss: (-3.097)(R 40.354, F -46.547)]  [G loss: 47.437] \n",
      "2725 [D loss: (-3.947)(R 39.413, F -47.308)]  [G loss: 47.801] \n",
      "2726 [D loss: (-2.711)(R 41.576, F -46.998)]  [G loss: 48.103] \n",
      "2726 [D loss: (-3.094)(R 41.156, F -47.344)]  [G loss: 47.219] \n",
      "2727 [D loss: (-3.097)(R 41.798, F -47.991)]  [G loss: 46.078] \n",
      "2727 [D loss: (-2.214)(R 43.367, F -47.795)]  [G loss: 46.565] \n",
      "2728 [D loss: (-5.044)(R 39.264, F -49.353)]  [G loss: 46.391] \n",
      "2728 [D loss: (-2.090)(R 41.400, F -45.579)]  [G loss: 46.192] \n",
      "2729 [D loss: (-1.538)(R 41.320, F -44.396)]  [G loss: 46.282] \n",
      "2729 [D loss: (-2.625)(R 39.581, F -44.831)]  [G loss: 46.405] \n",
      "2730 [D loss: (-2.025)(R 41.539, F -45.588)]  [G loss: 45.913] \n",
      "2730 [D loss: (-1.659)(R 42.203, F -45.522)]  [G loss: 46.162] \n",
      "2731 [D loss: (-3.238)(R 40.598, F -47.074)]  [G loss: 44.983] \n",
      "2731 [D loss: (-1.521)(R 41.462, F -44.505)]  [G loss: 45.724] \n",
      "2732 [D loss: (-2.181)(R 41.114, F -45.475)]  [G loss: 46.224] \n",
      "2732 [D loss: (-2.961)(R 39.494, F -45.417)]  [G loss: 45.399] \n",
      "2733 [D loss: (-0.251)(R 43.211, F -43.713)]  [G loss: 45.365] \n",
      "2733 [D loss: (-3.162)(R 40.376, F -46.700)]  [G loss: 45.356] \n",
      "2734 [D loss: (-0.748)(R 41.086, F -42.582)]  [G loss: 45.114] \n",
      "2734 [D loss: (0.111)(R 41.402, F -41.181)]  [G loss: 44.410] \n",
      "2735 [D loss: (-0.867)(R 41.154, F -42.887)]  [G loss: 44.610] \n",
      "2735 [D loss: (-2.718)(R 41.089, F -46.525)]  [G loss: 45.110] \n",
      "2736 [D loss: (-1.711)(R 42.627, F -46.049)]  [G loss: 44.870] \n",
      "2736 [D loss: (-1.177)(R 41.358, F -43.712)]  [G loss: 44.579] \n",
      "2737 [D loss: (-2.514)(R 39.730, F -44.759)]  [G loss: 44.135] \n",
      "2737 [D loss: (-1.481)(R 41.046, F -44.008)]  [G loss: 44.069] \n",
      "2738 [D loss: (-1.889)(R 40.937, F -44.715)]  [G loss: 42.714] \n",
      "2738 [D loss: (-2.276)(R 40.566, F -45.117)]  [G loss: 44.274] \n",
      "2739 [D loss: (-1.134)(R 40.016, F -42.285)]  [G loss: 43.382] \n",
      "2739 [D loss: (-1.790)(R 39.435, F -43.014)]  [G loss: 43.496] \n",
      "2740 [D loss: (-1.253)(R 41.266, F -43.771)]  [G loss: 43.207] \n",
      "2740 [D loss: (-3.150)(R 37.091, F -43.391)]  [G loss: 44.034] \n",
      "2741 [D loss: (-0.595)(R 39.420, F -40.610)]  [G loss: 43.586] \n",
      "2741 [D loss: (-0.167)(R 40.170, F -40.504)]  [G loss: 42.792] \n",
      "2742 [D loss: (-0.718)(R 39.964, F -41.399)]  [G loss: 42.529] \n",
      "2742 [D loss: (-0.992)(R 41.674, F -43.657)]  [G loss: 41.941] \n",
      "2743 [D loss: (-2.537)(R 40.523, F -45.597)]  [G loss: 41.888] \n",
      "2743 [D loss: (-0.375)(R 39.290, F -40.041)]  [G loss: 42.078] \n",
      "2744 [D loss: (-0.132)(R 41.063, F -41.327)]  [G loss: 41.890] \n",
      "2744 [D loss: (-0.102)(R 39.527, F -39.732)]  [G loss: 41.563] \n",
      "2745 [D loss: (-0.521)(R 37.338, F -38.381)]  [G loss: 42.026] \n",
      "2745 [D loss: (-2.569)(R 38.496, F -43.635)]  [G loss: 41.273] \n",
      "2746 [D loss: (-0.918)(R 38.580, F -40.416)]  [G loss: 41.635] \n",
      "2746 [D loss: (-1.135)(R 39.000, F -41.271)]  [G loss: 41.800] \n",
      "2747 [D loss: (-1.794)(R 38.032, F -41.619)]  [G loss: 40.867] \n",
      "2747 [D loss: (-0.481)(R 39.430, F -40.392)]  [G loss: 40.045] \n",
      "2748 [D loss: (-2.433)(R 39.661, F -44.528)]  [G loss: 41.105] \n",
      "2748 [D loss: (-1.716)(R 36.637, F -40.069)]  [G loss: 39.830] \n",
      "2749 [D loss: (-0.082)(R 36.434, F -36.599)]  [G loss: 40.802] \n",
      "2749 [D loss: (-2.268)(R 37.778, F -42.315)]  [G loss: 40.769] \n",
      "2750 [D loss: (0.619)(R 39.828, F -38.591)]  [G loss: 39.672] \n",
      "2750 [D loss: (-0.899)(R 37.644, F -39.442)]  [G loss: 40.845] \n",
      "2751 [D loss: (-0.870)(R 39.949, F -41.689)]  [G loss: 39.089] \n",
      "2751 [D loss: (-1.208)(R 37.724, F -40.140)]  [G loss: 40.030] \n",
      "2752 [D loss: (-1.350)(R 38.417, F -41.117)]  [G loss: 39.960] \n",
      "2752 [D loss: (-1.092)(R 36.440, F -38.625)]  [G loss: 38.889] \n",
      "2753 [D loss: (-1.246)(R 36.695, F -39.187)]  [G loss: 38.021] \n",
      "2753 [D loss: (-1.795)(R 35.698, F -39.288)]  [G loss: 39.180] \n",
      "2754 [D loss: (0.832)(R 36.707, F -35.043)]  [G loss: 38.965] \n",
      "2754 [D loss: (-0.332)(R 36.714, F -37.379)]  [G loss: 38.714] \n",
      "2755 [D loss: (-0.748)(R 37.367, F -38.862)]  [G loss: 38.116] \n",
      "2755 [D loss: (0.047)(R 36.410, F -36.315)]  [G loss: 38.941] \n",
      "2756 [D loss: (-0.927)(R 35.122, F -36.976)]  [G loss: 38.968] \n",
      "2756 [D loss: (-1.494)(R 35.900, F -38.889)]  [G loss: 37.929] \n",
      "2757 [D loss: (-0.658)(R 36.108, F -37.423)]  [G loss: 38.054] \n",
      "2757 [D loss: (-2.107)(R 35.514, F -39.728)]  [G loss: 38.289] \n",
      "2758 [D loss: (-2.092)(R 35.123, F -39.308)]  [G loss: 38.305] \n",
      "2758 [D loss: (0.255)(R 34.718, F -34.209)]  [G loss: 36.599] \n",
      "2759 [D loss: (0.327)(R 35.373, F -34.719)]  [G loss: 36.609] \n",
      "2759 [D loss: (-1.351)(R 34.110, F -36.813)]  [G loss: 37.880] \n",
      "2760 [D loss: (-0.016)(R 33.804, F -33.836)]  [G loss: 36.866] \n",
      "2760 [D loss: (0.794)(R 34.720, F -33.133)]  [G loss: 37.698] \n",
      "2761 [D loss: (-1.770)(R 33.585, F -37.124)]  [G loss: 37.437] \n",
      "2761 [D loss: (-3.625)(R 32.003, F -39.253)]  [G loss: 37.126] \n",
      "2762 [D loss: (-0.012)(R 33.583, F -33.606)]  [G loss: 35.810] \n",
      "2762 [D loss: (-0.834)(R 33.522, F -35.191)]  [G loss: 36.465] \n",
      "2763 [D loss: (-2.918)(R 31.504, F -37.341)]  [G loss: 37.583] \n",
      "2763 [D loss: (-1.051)(R 33.896, F -35.998)]  [G loss: 36.314] \n",
      "2764 [D loss: (-1.878)(R 34.393, F -38.148)]  [G loss: 36.776] \n",
      "2764 [D loss: (-2.004)(R 33.602, F -37.610)]  [G loss: 35.899] \n",
      "2765 [D loss: (-2.622)(R 33.253, F -38.496)]  [G loss: 36.457] \n",
      "2765 [D loss: (-1.256)(R 33.175, F -35.687)]  [G loss: 36.284] \n",
      "2766 [D loss: (-2.186)(R 33.802, F -38.175)]  [G loss: 35.140] \n",
      "2766 [D loss: (-0.735)(R 34.221, F -35.691)]  [G loss: 35.667] \n",
      "2767 [D loss: (-2.627)(R 32.277, F -37.530)]  [G loss: 36.046] \n",
      "2767 [D loss: (0.243)(R 32.820, F -32.333)]  [G loss: 34.199] \n",
      "2768 [D loss: (0.421)(R 32.831, F -31.989)]  [G loss: 36.286] \n",
      "2768 [D loss: (-2.463)(R 31.361, F -36.288)]  [G loss: 35.279] \n",
      "2769 [D loss: (-1.351)(R 32.132, F -34.834)]  [G loss: 35.799] \n",
      "2769 [D loss: (-0.053)(R 31.827, F -31.932)]  [G loss: 35.004] \n",
      "2770 [D loss: (-3.194)(R 29.724, F -36.112)]  [G loss: 33.506] \n",
      "2770 [D loss: (-1.438)(R 31.062, F -33.938)]  [G loss: 35.506] \n",
      "2771 [D loss: (-2.412)(R 30.532, F -35.356)]  [G loss: 35.150] \n",
      "2771 [D loss: (-2.017)(R 30.356, F -34.391)]  [G loss: 35.385] \n",
      "2772 [D loss: (-1.935)(R 30.307, F -34.176)]  [G loss: 34.264] \n",
      "2772 [D loss: (-1.401)(R 29.532, F -32.334)]  [G loss: 33.619] \n",
      "2773 [D loss: (-2.477)(R 30.140, F -35.094)]  [G loss: 34.481] \n",
      "2773 [D loss: (-2.248)(R 31.138, F -35.633)]  [G loss: 34.043] \n",
      "2774 [D loss: (-1.624)(R 29.728, F -32.976)]  [G loss: 34.787] \n",
      "2774 [D loss: (-2.267)(R 29.743, F -34.277)]  [G loss: 33.707] \n",
      "2775 [D loss: (-2.271)(R 28.899, F -33.442)]  [G loss: 33.389] \n",
      "2775 [D loss: (-2.476)(R 28.556, F -33.508)]  [G loss: 33.091] \n",
      "2776 [D loss: (-3.775)(R 27.269, F -34.819)]  [G loss: 32.974] \n",
      "2776 [D loss: (-0.868)(R 29.041, F -30.776)]  [G loss: 33.510] \n",
      "2777 [D loss: (-2.260)(R 28.847, F -33.367)]  [G loss: 33.693] \n",
      "2777 [D loss: (-3.287)(R 29.541, F -36.115)]  [G loss: 33.441] \n",
      "2778 [D loss: (-2.639)(R 28.420, F -33.698)]  [G loss: 33.835] \n",
      "2778 [D loss: (-0.606)(R 29.586, F -30.797)]  [G loss: 32.943] \n",
      "2779 [D loss: (-1.133)(R 28.758, F -31.023)]  [G loss: 33.015] \n",
      "2779 [D loss: (-4.454)(R 29.078, F -37.987)]  [G loss: 33.287] \n",
      "2780 [D loss: (-4.096)(R 27.017, F -35.210)]  [G loss: 33.855] \n",
      "2780 [D loss: (-1.847)(R 28.048, F -31.742)]  [G loss: 33.108] \n",
      "2781 [D loss: (-4.409)(R 27.260, F -36.077)]  [G loss: 32.604] \n",
      "2781 [D loss: (-3.377)(R 27.334, F -34.088)]  [G loss: 32.733] \n",
      "2782 [D loss: (-4.179)(R 25.672, F -34.030)]  [G loss: 32.563] \n",
      "2782 [D loss: (-3.282)(R 26.348, F -32.913)]  [G loss: 32.275] \n",
      "2783 [D loss: (-4.015)(R 25.774, F -33.805)]  [G loss: 32.233] \n",
      "2783 [D loss: (-2.045)(R 26.519, F -30.608)]  [G loss: 31.837] \n",
      "2784 [D loss: (-3.130)(R 25.822, F -32.081)]  [G loss: 32.784] \n",
      "2784 [D loss: (-6.042)(R 26.849, F -38.933)]  [G loss: 31.768] \n",
      "2785 [D loss: (-5.189)(R 27.259, F -37.637)]  [G loss: 32.095] \n",
      "2785 [D loss: (-4.106)(R 25.572, F -33.785)]  [G loss: 31.222] \n",
      "2786 [D loss: (-3.466)(R 27.185, F -34.116)]  [G loss: 31.283] \n",
      "2786 [D loss: (-2.871)(R 25.465, F -31.207)]  [G loss: 31.387] \n",
      "2787 [D loss: (-1.274)(R 25.080, F -27.629)]  [G loss: 32.776] \n",
      "2787 [D loss: (-3.215)(R 25.086, F -31.515)]  [G loss: 30.636] \n",
      "2788 [D loss: (-3.392)(R 24.464, F -31.249)]  [G loss: 31.903] \n",
      "2788 [D loss: (-5.246)(R 24.884, F -35.376)]  [G loss: 31.862] \n",
      "2789 [D loss: (-1.825)(R 23.319, F -26.970)]  [G loss: 32.203] \n",
      "2789 [D loss: (-1.819)(R 24.388, F -28.026)]  [G loss: 31.623] \n",
      "2790 [D loss: (-1.996)(R 23.912, F -27.904)]  [G loss: 30.198] \n",
      "2790 [D loss: (-3.305)(R 23.555, F -30.165)]  [G loss: 31.966] \n",
      "2791 [D loss: (-3.741)(R 22.756, F -30.237)]  [G loss: 30.634] \n",
      "2791 [D loss: (-3.144)(R 23.507, F -29.796)]  [G loss: 30.320] \n",
      "2792 [D loss: (-5.356)(R 22.499, F -33.212)]  [G loss: 30.205] \n",
      "2792 [D loss: (-3.544)(R 23.028, F -30.117)]  [G loss: 30.670] \n",
      "2793 [D loss: (-3.468)(R 21.565, F -28.501)]  [G loss: 29.728] \n",
      "2793 [D loss: (-5.162)(R 20.985, F -31.309)]  [G loss: 31.180] \n",
      "2794 [D loss: (-5.475)(R 22.920, F -33.870)]  [G loss: 30.010] \n",
      "2794 [D loss: (-5.291)(R 20.825, F -31.406)]  [G loss: 30.366] \n",
      "2795 [D loss: (-5.019)(R 21.493, F -31.531)]  [G loss: 30.194] \n",
      "2795 [D loss: (-4.419)(R 22.074, F -30.912)]  [G loss: 30.654] \n",
      "2796 [D loss: (-3.496)(R 20.533, F -27.524)]  [G loss: 30.161] \n",
      "2796 [D loss: (-4.280)(R 18.740, F -27.300)]  [G loss: 30.118] \n",
      "2797 [D loss: (-3.306)(R 21.391, F -28.002)]  [G loss: 30.479] \n",
      "2797 [D loss: (-4.949)(R 20.691, F -30.589)]  [G loss: 30.762] \n",
      "2798 [D loss: (-3.612)(R 20.463, F -27.687)]  [G loss: 29.576] \n",
      "2798 [D loss: (-3.908)(R 20.557, F -28.372)]  [G loss: 29.655] \n",
      "2799 [D loss: (-2.970)(R 18.886, F -24.826)]  [G loss: 30.184] \n",
      "2799 [D loss: (-4.709)(R 19.308, F -28.725)]  [G loss: 28.856] \n",
      "2800 [D loss: (-2.325)(R 19.429, F -24.078)]  [G loss: 28.894] \n",
      "2800 [D loss: (-4.360)(R 20.785, F -29.506)]  [G loss: 30.434] \n",
      "2801 [D loss: (-5.505)(R 18.736, F -29.746)]  [G loss: 28.879] \n",
      "2801 [D loss: (-3.546)(R 19.106, F -26.198)]  [G loss: 29.839] \n",
      "2802 [D loss: (-4.427)(R 19.071, F -27.926)]  [G loss: 29.182] \n",
      "2802 [D loss: (-4.783)(R 18.871, F -28.436)]  [G loss: 27.798] \n",
      "2803 [D loss: (-3.190)(R 18.606, F -24.986)]  [G loss: 28.783] \n",
      "2803 [D loss: (-7.520)(R 18.143, F -33.183)]  [G loss: 27.334] \n",
      "2804 [D loss: (-3.105)(R 17.921, F -24.130)]  [G loss: 29.268] \n",
      "2804 [D loss: (-4.891)(R 17.698, F -27.480)]  [G loss: 27.611] \n",
      "2805 [D loss: (-7.062)(R 17.144, F -31.267)]  [G loss: 29.888] \n",
      "2805 [D loss: (-6.109)(R 17.132, F -29.350)]  [G loss: 28.386] \n",
      "2806 [D loss: (-6.425)(R 15.959, F -28.810)]  [G loss: 27.219] \n",
      "2806 [D loss: (-6.934)(R 16.095, F -29.963)]  [G loss: 27.390] \n",
      "2807 [D loss: (-7.890)(R 15.425, F -31.205)]  [G loss: 27.728] \n",
      "2807 [D loss: (-6.033)(R 16.041, F -28.108)]  [G loss: 27.913] \n",
      "2808 [D loss: (-6.782)(R 14.367, F -27.931)]  [G loss: 29.246] \n",
      "2808 [D loss: (-4.978)(R 15.999, F -25.956)]  [G loss: 29.941] \n",
      "2809 [D loss: (-3.585)(R 14.650, F -21.821)]  [G loss: 29.048] \n",
      "2809 [D loss: (-7.058)(R 13.857, F -27.973)]  [G loss: 26.958] \n",
      "2810 [D loss: (-5.257)(R 14.724, F -25.239)]  [G loss: 27.997] \n",
      "2810 [D loss: (-5.949)(R 14.394, F -26.293)]  [G loss: 28.101] \n",
      "2811 [D loss: (-6.877)(R 14.147, F -27.902)]  [G loss: 29.429] \n",
      "2811 [D loss: (-6.100)(R 14.521, F -26.720)]  [G loss: 26.253] \n",
      "2812 [D loss: (-8.686)(R 13.081, F -30.454)]  [G loss: 27.912] \n",
      "2812 [D loss: (-7.751)(R 13.273, F -28.775)]  [G loss: 28.835] \n",
      "2813 [D loss: (-7.162)(R 12.897, F -27.222)]  [G loss: 27.251] \n",
      "2813 [D loss: (-7.532)(R 12.029, F -27.092)]  [G loss: 26.814] \n",
      "2814 [D loss: (-7.850)(R 13.148, F -28.849)]  [G loss: 28.186] \n",
      "2814 [D loss: (-5.804)(R 12.728, F -24.336)]  [G loss: 27.791] \n",
      "2815 [D loss: (-6.167)(R 13.346, F -25.679)]  [G loss: 27.404] \n",
      "2815 [D loss: (-6.407)(R 12.178, F -24.993)]  [G loss: 25.935] \n",
      "2816 [D loss: (-8.518)(R 12.204, F -29.239)]  [G loss: 28.957] \n",
      "2816 [D loss: (-7.154)(R 11.315, F -25.624)]  [G loss: 27.243] \n",
      "2817 [D loss: (-4.924)(R 13.730, F -23.578)]  [G loss: 24.777] \n",
      "2817 [D loss: (-6.860)(R 11.164, F -24.884)]  [G loss: 27.055] \n",
      "2818 [D loss: (-8.329)(R 10.595, F -27.252)]  [G loss: 26.824] \n",
      "2818 [D loss: (-8.732)(R 11.611, F -29.076)]  [G loss: 27.025] \n",
      "2819 [D loss: (-9.258)(R 8.962, F -27.478)]  [G loss: 27.485] \n",
      "2819 [D loss: (-8.242)(R 10.270, F -26.754)]  [G loss: 26.211] \n",
      "2820 [D loss: (-4.870)(R 10.162, F -19.903)]  [G loss: 26.044] \n",
      "2820 [D loss: (-8.772)(R 9.799, F -27.343)]  [G loss: 27.502] \n",
      "2821 [D loss: (-7.246)(R 10.259, F -24.750)]  [G loss: 27.368] \n",
      "2821 [D loss: (-11.931)(R 8.385, F -32.247)]  [G loss: 25.824] \n",
      "2822 [D loss: (-7.222)(R 10.242, F -24.686)]  [G loss: 27.972] \n",
      "2822 [D loss: (-7.210)(R 10.922, F -25.341)]  [G loss: 25.904] \n",
      "2823 [D loss: (-8.420)(R 9.836, F -26.676)]  [G loss: 28.711] \n",
      "2823 [D loss: (-9.305)(R 9.947, F -28.556)]  [G loss: 26.381] \n",
      "2824 [D loss: (-7.208)(R 8.926, F -23.341)]  [G loss: 26.890] \n",
      "2824 [D loss: (-7.784)(R 9.544, F -25.112)]  [G loss: 27.120] \n",
      "2825 [D loss: (-7.475)(R 9.387, F -24.337)]  [G loss: 26.124] \n",
      "2825 [D loss: (-9.277)(R 9.226, F -27.779)]  [G loss: 27.824] \n",
      "2826 [D loss: (-11.218)(R 9.098, F -31.534)]  [G loss: 26.738] \n",
      "2826 [D loss: (-6.878)(R 9.081, F -22.836)]  [G loss: 28.207] \n",
      "2827 [D loss: (-8.017)(R 9.812, F -25.846)]  [G loss: 26.615] \n",
      "2827 [D loss: (-9.516)(R 9.550, F -28.582)]  [G loss: 26.526] \n",
      "2828 [D loss: (-9.847)(R 11.222, F -30.916)]  [G loss: 27.394] \n",
      "2828 [D loss: (-9.283)(R 10.379, F -28.945)]  [G loss: 28.595] \n",
      "2829 [D loss: (-9.417)(R 9.772, F -28.605)]  [G loss: 29.455] \n",
      "2829 [D loss: (-6.900)(R 12.310, F -26.110)]  [G loss: 28.038] \n",
      "2830 [D loss: (-8.841)(R 11.161, F -28.844)]  [G loss: 28.403] \n",
      "2830 [D loss: (-7.417)(R 10.473, F -25.308)]  [G loss: 28.559] \n",
      "2831 [D loss: (-9.895)(R 10.237, F -30.026)]  [G loss: 28.264] \n",
      "2831 [D loss: (-9.902)(R 11.858, F -31.662)]  [G loss: 29.940] \n",
      "2832 [D loss: (-10.808)(R 10.508, F -32.125)]  [G loss: 29.417] \n",
      "2832 [D loss: (-13.121)(R 9.462, F -35.703)]  [G loss: 29.781] \n",
      "2833 [D loss: (-7.946)(R 11.515, F -27.407)]  [G loss: 27.109] \n",
      "2833 [D loss: (-11.315)(R 9.899, F -32.530)]  [G loss: 28.483] \n",
      "2834 [D loss: (-11.321)(R 10.976, F -33.619)]  [G loss: 29.877] \n",
      "2834 [D loss: (-10.084)(R 12.519, F -32.687)]  [G loss: 26.766] \n",
      "2835 [D loss: (-10.457)(R 11.721, F -32.636)]  [G loss: 28.061] \n",
      "2835 [D loss: (-12.027)(R 11.123, F -35.177)]  [G loss: 30.598] \n",
      "2836 [D loss: (-12.158)(R 11.083, F -35.398)]  [G loss: 29.632] \n",
      "2836 [D loss: (-5.891)(R 11.511, F -23.293)]  [G loss: 28.442] \n",
      "2837 [D loss: (-11.083)(R 12.103, F -34.270)]  [G loss: 31.022] \n",
      "2837 [D loss: (-12.341)(R 11.154, F -35.836)]  [G loss: 30.748] \n",
      "2838 [D loss: (-8.644)(R 12.247, F -29.535)]  [G loss: 30.584] \n",
      "2838 [D loss: (-8.832)(R 12.855, F -30.520)]  [G loss: 31.385] \n",
      "2839 [D loss: (-6.833)(R 13.685, F -27.350)]  [G loss: 30.607] \n",
      "2839 [D loss: (-6.753)(R 11.916, F -25.421)]  [G loss: 30.545] \n",
      "2840 [D loss: (-9.330)(R 12.294, F -30.954)]  [G loss: 31.464] \n",
      "2840 [D loss: (-8.374)(R 14.789, F -31.536)]  [G loss: 28.520] \n",
      "2841 [D loss: (-6.672)(R 12.616, F -25.960)]  [G loss: 29.448] \n",
      "2841 [D loss: (-7.568)(R 12.858, F -27.994)]  [G loss: 30.038] \n",
      "2842 [D loss: (-10.318)(R 14.121, F -34.757)]  [G loss: 30.392] \n",
      "2842 [D loss: (-12.274)(R 11.701, F -36.248)]  [G loss: 31.445] \n",
      "2843 [D loss: (-7.344)(R 13.361, F -28.050)]  [G loss: 31.281] \n",
      "2843 [D loss: (-10.972)(R 13.752, F -35.696)]  [G loss: 30.611] \n",
      "2844 [D loss: (-8.154)(R 11.432, F -27.741)]  [G loss: 32.274] \n",
      "2844 [D loss: (-6.275)(R 13.363, F -25.913)]  [G loss: 31.494] \n",
      "2845 [D loss: (-7.536)(R 15.335, F -30.406)]  [G loss: 32.623] \n",
      "2845 [D loss: (-12.675)(R 13.239, F -38.590)]  [G loss: 29.024] \n",
      "2846 [D loss: (-10.673)(R 13.230, F -34.576)]  [G loss: 30.604] \n",
      "2846 [D loss: (-7.731)(R 13.501, F -28.963)]  [G loss: 31.477] \n",
      "2847 [D loss: (-9.281)(R 13.052, F -31.614)]  [G loss: 30.507] \n",
      "2847 [D loss: (-7.792)(R 15.816, F -31.400)]  [G loss: 31.005] \n",
      "2848 [D loss: (-6.745)(R 13.048, F -26.538)]  [G loss: 30.851] \n",
      "2848 [D loss: (-6.096)(R 15.639, F -27.830)]  [G loss: 31.301] \n",
      "2849 [D loss: (-9.452)(R 14.177, F -33.082)]  [G loss: 31.313] \n",
      "2849 [D loss: (-8.218)(R 14.401, F -30.837)]  [G loss: 31.240] \n",
      "2850 [D loss: (-6.106)(R 14.525, F -26.736)]  [G loss: 30.071] \n",
      "2850 [D loss: (-6.738)(R 15.469, F -28.945)]  [G loss: 30.262] \n",
      "2851 [D loss: (-6.878)(R 14.779, F -28.535)]  [G loss: 29.285] \n",
      "2851 [D loss: (-9.983)(R 13.641, F -33.607)]  [G loss: 30.468] \n",
      "2852 [D loss: (-8.889)(R 16.089, F -33.867)]  [G loss: 30.131] \n",
      "2852 [D loss: (-10.621)(R 13.997, F -35.239)]  [G loss: 30.224] \n",
      "2853 [D loss: (-9.262)(R 13.084, F -31.609)]  [G loss: 30.142] \n",
      "2853 [D loss: (-9.965)(R 14.952, F -34.882)]  [G loss: 31.227] \n",
      "2854 [D loss: (-12.352)(R 14.296, F -38.999)]  [G loss: 30.302] \n",
      "2854 [D loss: (-6.908)(R 15.889, F -29.705)]  [G loss: 31.181] \n",
      "2855 [D loss: (-8.510)(R 15.275, F -32.294)]  [G loss: 28.587] \n",
      "2855 [D loss: (-8.836)(R 14.620, F -32.291)]  [G loss: 30.541] \n",
      "2856 [D loss: (-9.538)(R 15.265, F -34.341)]  [G loss: 29.476] \n",
      "2856 [D loss: (-8.462)(R 14.219, F -31.143)]  [G loss: 29.590] \n",
      "2857 [D loss: (-8.350)(R 14.603, F -31.304)]  [G loss: 30.215] \n",
      "2857 [D loss: (-9.629)(R 14.814, F -34.072)]  [G loss: 30.431] \n",
      "2858 [D loss: (-6.846)(R 14.529, F -28.221)]  [G loss: 26.866] \n",
      "2858 [D loss: (-8.542)(R 14.798, F -31.883)]  [G loss: 28.229] \n",
      "2859 [D loss: (-7.729)(R 15.219, F -30.676)]  [G loss: 30.398] \n",
      "2859 [D loss: (-5.454)(R 13.548, F -24.455)]  [G loss: 29.835] \n",
      "2860 [D loss: (-6.018)(R 14.450, F -26.486)]  [G loss: 27.179] \n",
      "2860 [D loss: (-11.602)(R 14.321, F -37.525)]  [G loss: 29.778] \n",
      "2861 [D loss: (-7.916)(R 14.142, F -29.973)]  [G loss: 28.798] \n",
      "2861 [D loss: (-7.225)(R 16.863, F -31.312)]  [G loss: 29.667] \n",
      "2862 [D loss: (-9.256)(R 14.648, F -33.161)]  [G loss: 27.651] \n",
      "2862 [D loss: (-7.564)(R 16.339, F -31.468)]  [G loss: 29.257] \n",
      "2863 [D loss: (-5.337)(R 13.457, F -24.130)]  [G loss: 27.231] \n",
      "2863 [D loss: (-4.681)(R 16.784, F -26.146)]  [G loss: 26.642] \n",
      "2864 [D loss: (-5.019)(R 16.536, F -26.574)]  [G loss: 28.644] \n",
      "2864 [D loss: (-5.201)(R 16.500, F -26.902)]  [G loss: 27.619] \n",
      "2865 [D loss: (-3.756)(R 16.513, F -24.025)]  [G loss: 26.174] \n",
      "2865 [D loss: (-8.938)(R 15.893, F -33.770)]  [G loss: 29.344] \n",
      "2866 [D loss: (-5.090)(R 15.438, F -25.618)]  [G loss: 27.651] \n",
      "2866 [D loss: (-6.141)(R 13.536, F -25.818)]  [G loss: 27.915] \n",
      "2867 [D loss: (-4.485)(R 16.247, F -25.218)]  [G loss: 25.237] \n",
      "2867 [D loss: (-8.838)(R 15.003, F -32.680)]  [G loss: 24.827] \n",
      "2868 [D loss: (-5.746)(R 15.638, F -27.129)]  [G loss: 26.647] \n",
      "2868 [D loss: (-5.224)(R 15.265, F -25.713)]  [G loss: 26.521] \n",
      "2869 [D loss: (-8.033)(R 16.197, F -32.264)]  [G loss: 23.517] \n",
      "2869 [D loss: (-2.373)(R 15.125, F -19.871)]  [G loss: 27.263] \n",
      "2870 [D loss: (-5.674)(R 15.288, F -26.636)]  [G loss: 24.947] \n",
      "2870 [D loss: (-6.564)(R 16.685, F -29.813)]  [G loss: 25.811] \n",
      "2871 [D loss: (-5.155)(R 16.604, F -26.915)]  [G loss: 22.799] \n",
      "2871 [D loss: (-8.907)(R 14.520, F -32.333)]  [G loss: 23.999] \n",
      "2872 [D loss: (-5.251)(R 16.802, F -27.304)]  [G loss: 23.500] \n",
      "2872 [D loss: (-6.096)(R 15.973, F -28.166)]  [G loss: 23.338] \n",
      "2873 [D loss: (-4.297)(R 15.882, F -24.476)]  [G loss: 22.834] \n",
      "2873 [D loss: (-1.798)(R 17.441, F -21.036)]  [G loss: 24.191] \n",
      "2874 [D loss: (-3.072)(R 17.196, F -23.340)]  [G loss: 24.157] \n",
      "2874 [D loss: (-4.072)(R 16.441, F -24.585)]  [G loss: 22.530] \n",
      "2875 [D loss: (-1.519)(R 16.817, F -19.854)]  [G loss: 22.578] \n",
      "2875 [D loss: (-2.898)(R 13.951, F -19.748)]  [G loss: 21.939] \n",
      "2876 [D loss: (-2.291)(R 15.674, F -20.256)]  [G loss: 22.333] \n",
      "2876 [D loss: (-2.735)(R 17.671, F -23.141)]  [G loss: 24.009] \n",
      "2877 [D loss: (-2.377)(R 14.649, F -19.402)]  [G loss: 20.591] \n",
      "2877 [D loss: (-1.497)(R 16.277, F -19.271)]  [G loss: 21.364] \n",
      "2878 [D loss: (0.571)(R 18.667, F -17.524)]  [G loss: 21.031] \n",
      "2878 [D loss: (-0.958)(R 16.382, F -18.298)]  [G loss: 19.823] \n",
      "2879 [D loss: (-3.447)(R 15.163, F -22.057)]  [G loss: 21.501] \n",
      "2879 [D loss: (-2.157)(R 15.870, F -20.184)]  [G loss: 17.538] \n",
      "2880 [D loss: (0.540)(R 15.392, F -14.312)]  [G loss: 18.872] \n",
      "2880 [D loss: (0.912)(R 18.076, F -16.251)]  [G loss: 19.222] \n",
      "2881 [D loss: (1.214)(R 15.565, F -13.137)]  [G loss: 20.777] \n",
      "2881 [D loss: (0.908)(R 15.404, F -13.588)]  [G loss: 19.141] \n",
      "2882 [D loss: (-1.956)(R 16.979, F -20.892)]  [G loss: 19.010] \n",
      "2882 [D loss: (0.763)(R 16.452, F -14.927)]  [G loss: 17.430] \n",
      "2883 [D loss: (-0.572)(R 16.116, F -17.260)]  [G loss: 17.873] \n",
      "2883 [D loss: (0.645)(R 16.165, F -14.876)]  [G loss: 18.379] \n",
      "2884 [D loss: (-3.173)(R 12.783, F -19.129)]  [G loss: 16.304] \n",
      "2884 [D loss: (0.511)(R 15.792, F -14.770)]  [G loss: 16.149] \n",
      "2885 [D loss: (0.348)(R 16.980, F -16.284)]  [G loss: 17.046] \n",
      "2885 [D loss: (1.508)(R 15.573, F -12.557)]  [G loss: 16.604] \n",
      "2886 [D loss: (-0.928)(R 16.157, F -18.012)]  [G loss: 15.932] \n",
      "2886 [D loss: (-0.305)(R 16.060, F -16.669)]  [G loss: 15.189] \n",
      "2887 [D loss: (-1.633)(R 15.696, F -18.961)]  [G loss: 15.250] \n",
      "2887 [D loss: (-0.935)(R 15.344, F -17.214)]  [G loss: 14.670] \n",
      "2888 [D loss: (1.423)(R 14.870, F -12.024)]  [G loss: 14.752] \n",
      "2888 [D loss: (1.649)(R 16.371, F -13.074)]  [G loss: 14.389] \n",
      "2889 [D loss: (0.234)(R 15.272, F -14.805)]  [G loss: 14.162] \n",
      "2889 [D loss: (1.395)(R 16.415, F -13.625)]  [G loss: 13.245] \n",
      "2890 [D loss: (2.590)(R 15.840, F -10.660)]  [G loss: 13.403] \n",
      "2890 [D loss: (2.969)(R 15.714, F -9.776)]  [G loss: 13.676] \n",
      "2891 [D loss: (1.748)(R 13.683, F -10.186)]  [G loss: 12.389] \n",
      "2891 [D loss: (0.795)(R 15.113, F -13.522)]  [G loss: 10.971] \n",
      "2892 [D loss: (0.523)(R 12.791, F -11.745)]  [G loss: 12.037] \n",
      "2892 [D loss: (1.835)(R 14.770, F -11.101)]  [G loss: 11.628] \n",
      "2893 [D loss: (3.178)(R 16.841, F -10.485)]  [G loss: 10.724] \n",
      "2893 [D loss: (-0.349)(R 13.298, F -13.996)]  [G loss: 9.589] \n",
      "2894 [D loss: (0.350)(R 15.063, F -14.364)]  [G loss: 11.483] \n",
      "2894 [D loss: (2.358)(R 14.741, F -10.025)]  [G loss: 10.915] \n",
      "2895 [D loss: (1.115)(R 14.427, F -12.197)]  [G loss: 9.164] \n",
      "2895 [D loss: (1.864)(R 14.253, F -10.526)]  [G loss: 9.046] \n",
      "2896 [D loss: (2.868)(R 15.488, F -9.752)]  [G loss: 10.185] \n",
      "2896 [D loss: (1.071)(R 14.740, F -12.599)]  [G loss: 8.445] \n",
      "2897 [D loss: (3.182)(R 14.567, F -8.204)]  [G loss: 8.672] \n",
      "2897 [D loss: (0.394)(R 14.248, F -13.459)]  [G loss: 7.270] \n",
      "2898 [D loss: (3.389)(R 14.439, F -7.661)]  [G loss: 7.905] \n",
      "2898 [D loss: (1.956)(R 11.978, F -8.066)]  [G loss: 7.391] \n",
      "2899 [D loss: (3.482)(R 12.955, F -5.991)]  [G loss: 7.629] \n",
      "2899 [D loss: (3.159)(R 14.732, F -8.414)]  [G loss: 6.501] \n",
      "2900 [D loss: (2.757)(R 13.866, F -8.353)]  [G loss: 6.629] \n",
      "2900 [D loss: (3.937)(R 12.428, F -4.553)]  [G loss: 7.022] \n",
      "2901 [D loss: (2.581)(R 12.811, F -7.650)]  [G loss: 5.756] \n",
      "2901 [D loss: (3.183)(R 12.781, F -6.415)]  [G loss: 5.875] \n",
      "2902 [D loss: (3.604)(R 12.953, F -5.745)]  [G loss: 5.352] \n",
      "2902 [D loss: (3.058)(R 12.679, F -6.564)]  [G loss: 5.539] \n",
      "2903 [D loss: (2.431)(R 11.123, F -6.261)]  [G loss: 5.143] \n",
      "2903 [D loss: (4.019)(R 12.477, F -4.438)]  [G loss: 5.927] \n",
      "2904 [D loss: (2.836)(R 11.322, F -5.649)]  [G loss: 4.643] \n",
      "2904 [D loss: (1.871)(R 11.325, F -7.584)]  [G loss: 5.131] \n",
      "2905 [D loss: (6.030)(R 14.112, F -2.053)]  [G loss: 4.518] \n",
      "2905 [D loss: (5.206)(R 12.976, F -2.565)]  [G loss: 3.742] \n",
      "2906 [D loss: (3.093)(R 11.418, F -5.232)]  [G loss: 4.009] \n",
      "2906 [D loss: (3.780)(R 10.878, F -3.317)]  [G loss: 3.949] \n",
      "2907 [D loss: (4.324)(R 11.055, F -2.407)]  [G loss: 3.487] \n",
      "2907 [D loss: (3.641)(R 11.811, F -4.528)]  [G loss: 3.704] \n",
      "2908 [D loss: (4.080)(R 12.128, F -3.967)]  [G loss: 2.899] \n",
      "2908 [D loss: (3.459)(R 10.867, F -3.949)]  [G loss: 3.970] \n",
      "2909 [D loss: (3.532)(R 11.016, F -3.952)]  [G loss: 2.908] \n",
      "2909 [D loss: (4.065)(R 11.890, F -3.759)]  [G loss: 3.157] \n",
      "2910 [D loss: (2.976)(R 9.279, F -3.327)]  [G loss: 2.461] \n",
      "2910 [D loss: (3.882)(R 9.834, F -2.070)]  [G loss: 2.115] \n",
      "2911 [D loss: (4.180)(R 11.217, F -2.857)]  [G loss: 1.921] \n",
      "2911 [D loss: (5.003)(R 11.164, F -1.158)]  [G loss: 2.713] \n",
      "2912 [D loss: (3.445)(R 9.455, F -2.566)]  [G loss: 1.606] \n",
      "2912 [D loss: (3.550)(R 9.681, F -2.581)]  [G loss: 1.953] \n",
      "2913 [D loss: (3.811)(R 9.660, F -2.039)]  [G loss: 1.915] \n",
      "2913 [D loss: (3.009)(R 8.710, F -2.691)]  [G loss: 1.279] \n",
      "2914 [D loss: (3.800)(R 9.411, F -1.812)]  [G loss: 1.908] \n",
      "2914 [D loss: (3.963)(R 8.745, F -0.820)]  [G loss: 1.036] \n",
      "2915 [D loss: (3.719)(R 9.633, F -2.196)]  [G loss: 1.621] \n",
      "2915 [D loss: (3.792)(R 9.191, F -1.607)]  [G loss: 0.982] \n",
      "2916 [D loss: (3.942)(R 8.887, F -1.003)]  [G loss: 1.242] \n",
      "2916 [D loss: (3.755)(R 9.417, F -1.907)]  [G loss: 0.551] \n",
      "2917 [D loss: (3.390)(R 8.441, F -1.661)]  [G loss: 0.673] \n",
      "2917 [D loss: (3.531)(R 9.063, F -2.002)]  [G loss: 1.037] \n",
      "2918 [D loss: (3.386)(R 8.301, F -1.530)]  [G loss: 0.811] \n",
      "2918 [D loss: (2.909)(R 7.573, F -1.756)]  [G loss: 0.516] \n",
      "2919 [D loss: (3.681)(R 9.155, F -1.794)]  [G loss: 0.975] \n",
      "2919 [D loss: (4.212)(R 8.603, F -0.179)]  [G loss: 0.389] \n",
      "2920 [D loss: (3.882)(R 8.084, F -0.319)]  [G loss: 0.419] \n",
      "2920 [D loss: (3.927)(R 8.408, F -0.555)]  [G loss: -0.044] \n",
      "2921 [D loss: (4.354)(R 8.743, F -0.035)]  [G loss: 0.320] \n",
      "2921 [D loss: (3.174)(R 7.663, F -1.314)]  [G loss: 0.243] \n",
      "2922 [D loss: (3.142)(R 6.298, F -0.014)]  [G loss: 0.395] \n",
      "2922 [D loss: (4.003)(R 8.302, F -0.295)]  [G loss: 0.496] \n",
      "2923 [D loss: (3.058)(R 6.332, F -0.216)]  [G loss: 0.160] \n",
      "2923 [D loss: (3.251)(R 7.628, F -1.126)]  [G loss: 0.505] \n",
      "2924 [D loss: (3.199)(R 6.721, F -0.323)]  [G loss: 0.150] \n",
      "2924 [D loss: (3.503)(R 7.481, F -0.475)]  [G loss: 0.157] \n",
      "2925 [D loss: (3.711)(R 7.901, F -0.478)]  [G loss: -0.139] \n",
      "2925 [D loss: (3.565)(R 7.322, F -0.192)]  [G loss: -0.222] \n",
      "2926 [D loss: (2.892)(R 6.194, F -0.409)]  [G loss: -0.423] \n",
      "2926 [D loss: (3.506)(R 7.212, F -0.199)]  [G loss: -0.021] \n",
      "2927 [D loss: (3.274)(R 6.518, F 0.030)]  [G loss: -0.101] \n",
      "2927 [D loss: (3.358)(R 6.233, F 0.482)]  [G loss: -0.293] \n",
      "2928 [D loss: (3.377)(R 6.426, F 0.329)]  [G loss: -0.003] \n",
      "2928 [D loss: (3.656)(R 6.558, F 0.753)]  [G loss: 0.047] \n",
      "2929 [D loss: (3.285)(R 6.109, F 0.460)]  [G loss: -0.368] \n",
      "2929 [D loss: (3.058)(R 5.553, F 0.562)]  [G loss: -0.379] \n",
      "2930 [D loss: (3.033)(R 5.310, F 0.756)]  [G loss: -0.488] \n",
      "2930 [D loss: (2.898)(R 5.327, F 0.470)]  [G loss: -0.434] \n",
      "2931 [D loss: (3.091)(R 5.761, F 0.421)]  [G loss: -0.247] \n",
      "2931 [D loss: (3.210)(R 6.132, F 0.289)]  [G loss: -0.623] \n",
      "2932 [D loss: (3.382)(R 6.392, F 0.372)]  [G loss: -0.078] \n",
      "2932 [D loss: (3.030)(R 5.554, F 0.506)]  [G loss: -0.308] \n",
      "2933 [D loss: (2.822)(R 5.330, F 0.315)]  [G loss: -0.494] \n",
      "2933 [D loss: (2.705)(R 5.050, F 0.359)]  [G loss: -0.319] \n",
      "2934 [D loss: (2.483)(R 5.126, F -0.161)]  [G loss: -0.409] \n",
      "2934 [D loss: (3.068)(R 5.576, F 0.560)]  [G loss: -0.705] \n",
      "2935 [D loss: (2.813)(R 4.780, F 0.845)]  [G loss: -0.487] \n",
      "2935 [D loss: (3.093)(R 5.576, F 0.611)]  [G loss: -0.494] \n",
      "2936 [D loss: (2.839)(R 4.713, F 0.964)]  [G loss: -0.624] \n",
      "2936 [D loss: (2.932)(R 5.119, F 0.744)]  [G loss: -0.654] \n",
      "2937 [D loss: (2.731)(R 4.798, F 0.665)]  [G loss: -0.494] \n",
      "2937 [D loss: (2.367)(R 4.328, F 0.406)]  [G loss: -0.602] \n",
      "2938 [D loss: (2.692)(R 4.649, F 0.736)]  [G loss: -0.686] \n",
      "2938 [D loss: (2.653)(R 4.648, F 0.659)]  [G loss: -0.637] \n",
      "2939 [D loss: (2.663)(R 4.588, F 0.738)]  [G loss: -0.691] \n",
      "2939 [D loss: (2.549)(R 4.302, F 0.796)]  [G loss: -0.621] \n",
      "2940 [D loss: (2.853)(R 4.897, F 0.809)]  [G loss: -0.536] \n",
      "2940 [D loss: (2.244)(R 4.136, F 0.352)]  [G loss: -0.672] \n",
      "2941 [D loss: (2.602)(R 4.921, F 0.283)]  [G loss: -0.789] \n",
      "2941 [D loss: (2.692)(R 4.480, F 0.904)]  [G loss: -0.700] \n",
      "2942 [D loss: (2.393)(R 4.290, F 0.497)]  [G loss: -0.682] \n",
      "2942 [D loss: (2.268)(R 3.731, F 0.805)]  [G loss: -0.777] \n",
      "2943 [D loss: (2.264)(R 3.596, F 0.932)]  [G loss: -0.743] \n",
      "2943 [D loss: (2.321)(R 3.908, F 0.734)]  [G loss: -0.773] \n",
      "2944 [D loss: (2.202)(R 3.984, F 0.420)]  [G loss: -0.650] \n",
      "2944 [D loss: (2.010)(R 3.700, F 0.320)]  [G loss: -0.729] \n",
      "2945 [D loss: (1.911)(R 3.251, F 0.572)]  [G loss: -0.717] \n",
      "2945 [D loss: (2.152)(R 3.575, F 0.728)]  [G loss: -0.822] \n",
      "2946 [D loss: (1.985)(R 3.318, F 0.653)]  [G loss: -0.733] \n",
      "2946 [D loss: (2.218)(R 3.814, F 0.622)]  [G loss: -0.701] \n",
      "2947 [D loss: (2.158)(R 3.451, F 0.864)]  [G loss: -0.807] \n",
      "2947 [D loss: (1.941)(R 3.330, F 0.552)]  [G loss: -0.746] \n",
      "2948 [D loss: (1.759)(R 2.832, F 0.686)]  [G loss: -0.808] \n",
      "2948 [D loss: (1.924)(R 3.054, F 0.795)]  [G loss: -0.825] \n",
      "2949 [D loss: (1.884)(R 2.992, F 0.777)]  [G loss: -0.878] \n",
      "2949 [D loss: (1.878)(R 3.011, F 0.745)]  [G loss: -0.821] \n",
      "2950 [D loss: (1.852)(R 3.008, F 0.696)]  [G loss: -0.777] \n",
      "2950 [D loss: (1.745)(R 2.799, F 0.691)]  [G loss: -0.753] \n",
      "2951 [D loss: (1.716)(R 2.743, F 0.689)]  [G loss: -0.800] \n",
      "2951 [D loss: (1.850)(R 2.954, F 0.746)]  [G loss: -0.794] \n",
      "2952 [D loss: (1.640)(R 2.613, F 0.666)]  [G loss: -0.717] \n",
      "2952 [D loss: (1.897)(R 2.961, F 0.833)]  [G loss: -0.869] \n",
      "2953 [D loss: (1.680)(R 2.654, F 0.707)]  [G loss: -0.780] \n",
      "2953 [D loss: (1.793)(R 2.676, F 0.910)]  [G loss: -0.744] \n",
      "2954 [D loss: (1.603)(R 2.380, F 0.827)]  [G loss: -0.787] \n",
      "2954 [D loss: (1.523)(R 2.264, F 0.781)]  [G loss: -0.810] \n",
      "2955 [D loss: (1.702)(R 2.601, F 0.803)]  [G loss: -0.841] \n",
      "2955 [D loss: (1.493)(R 2.208, F 0.779)]  [G loss: -0.867] \n",
      "2956 [D loss: (1.527)(R 2.236, F 0.817)]  [G loss: -0.907] \n",
      "2956 [D loss: (1.477)(R 2.147, F 0.807)]  [G loss: -0.819] \n",
      "2957 [D loss: (1.330)(R 1.889, F 0.770)]  [G loss: -0.860] \n",
      "2957 [D loss: (1.277)(R 1.783, F 0.771)]  [G loss: -0.809] \n",
      "2958 [D loss: (1.399)(R 1.986, F 0.812)]  [G loss: -0.800] \n",
      "2958 [D loss: (1.375)(R 1.923, F 0.826)]  [G loss: -0.828] \n",
      "2959 [D loss: (1.413)(R 1.921, F 0.906)]  [G loss: -0.795] \n",
      "2959 [D loss: (1.358)(R 1.893, F 0.823)]  [G loss: -0.844] \n",
      "2960 [D loss: (1.254)(R 1.638, F 0.870)]  [G loss: -0.910] \n",
      "2960 [D loss: (1.434)(R 1.925, F 0.942)]  [G loss: -0.777] \n",
      "2961 [D loss: (1.171)(R 1.512, F 0.830)]  [G loss: -0.773] \n",
      "2961 [D loss: (1.234)(R 1.626, F 0.842)]  [G loss: -0.791] \n",
      "2962 [D loss: (1.183)(R 1.495, F 0.872)]  [G loss: -0.822] \n",
      "2962 [D loss: (1.350)(R 1.859, F 0.842)]  [G loss: -0.858] \n",
      "2963 [D loss: (1.171)(R 1.613, F 0.730)]  [G loss: -0.826] \n",
      "2963 [D loss: (1.214)(R 1.677, F 0.752)]  [G loss: -0.790] \n",
      "2964 [D loss: (1.125)(R 1.457, F 0.793)]  [G loss: -0.804] \n",
      "2964 [D loss: (1.072)(R 1.373, F 0.771)]  [G loss: -0.751] \n",
      "2965 [D loss: (1.117)(R 1.429, F 0.805)]  [G loss: -0.805] \n",
      "2965 [D loss: (1.165)(R 1.533, F 0.797)]  [G loss: -0.805] \n",
      "2966 [D loss: (0.950)(R 1.158, F 0.742)]  [G loss: -0.849] \n",
      "2966 [D loss: (1.102)(R 1.452, F 0.751)]  [G loss: -0.808] \n",
      "2967 [D loss: (1.086)(R 1.347, F 0.825)]  [G loss: -0.798] \n",
      "2967 [D loss: (1.016)(R 1.234, F 0.797)]  [G loss: -0.824] \n",
      "2968 [D loss: (0.906)(R 0.963, F 0.850)]  [G loss: -0.806] \n",
      "2968 [D loss: (0.898)(R 0.996, F 0.799)]  [G loss: -0.758] \n",
      "2969 [D loss: (0.989)(R 1.213, F 0.766)]  [G loss: -0.824] \n",
      "2969 [D loss: (0.984)(R 1.125, F 0.842)]  [G loss: -0.795] \n",
      "2970 [D loss: (0.927)(R 1.073, F 0.781)]  [G loss: -0.763] \n",
      "2970 [D loss: (0.924)(R 1.141, F 0.707)]  [G loss: -0.765] \n",
      "2971 [D loss: (0.945)(R 1.151, F 0.738)]  [G loss: -0.795] \n",
      "2971 [D loss: (0.988)(R 1.223, F 0.754)]  [G loss: -0.725] \n",
      "2972 [D loss: (0.956)(R 1.153, F 0.760)]  [G loss: -0.738] \n",
      "2972 [D loss: (0.816)(R 0.914, F 0.718)]  [G loss: -0.726] \n",
      "2973 [D loss: (0.918)(R 1.094, F 0.743)]  [G loss: -0.763] \n",
      "2973 [D loss: (0.856)(R 1.003, F 0.709)]  [G loss: -0.740] \n",
      "2974 [D loss: (0.913)(R 1.109, F 0.717)]  [G loss: -0.769] \n",
      "2974 [D loss: (0.806)(R 0.917, F 0.695)]  [G loss: -0.744] \n",
      "2975 [D loss: (0.751)(R 0.828, F 0.673)]  [G loss: -0.748] \n",
      "2975 [D loss: (0.809)(R 0.904, F 0.713)]  [G loss: -0.753] \n",
      "2976 [D loss: (0.820)(R 0.872, F 0.768)]  [G loss: -0.744] \n",
      "2976 [D loss: (0.744)(R 0.781, F 0.707)]  [G loss: -0.713] \n",
      "2977 [D loss: (0.852)(R 1.031, F 0.674)]  [G loss: -0.729] \n",
      "2977 [D loss: (0.811)(R 0.901, F 0.721)]  [G loss: -0.709] \n",
      "2978 [D loss: (0.803)(R 0.854, F 0.752)]  [G loss: -0.723] \n",
      "2978 [D loss: (0.784)(R 0.890, F 0.679)]  [G loss: -0.709] \n",
      "2979 [D loss: (0.649)(R 0.571, F 0.728)]  [G loss: -0.688] \n",
      "2979 [D loss: (0.840)(R 0.974, F 0.705)]  [G loss: -0.710] \n",
      "2980 [D loss: (0.714)(R 0.691, F 0.736)]  [G loss: -0.670] \n",
      "2980 [D loss: (0.743)(R 0.825, F 0.661)]  [G loss: -0.687] \n",
      "2981 [D loss: (0.731)(R 0.727, F 0.735)]  [G loss: -0.704] \n",
      "2981 [D loss: (0.722)(R 0.745, F 0.699)]  [G loss: -0.705] \n",
      "2982 [D loss: (0.741)(R 0.796, F 0.685)]  [G loss: -0.667] \n",
      "2982 [D loss: (0.721)(R 0.747, F 0.696)]  [G loss: -0.678] \n",
      "2983 [D loss: (0.614)(R 0.577, F 0.651)]  [G loss: -0.679] \n",
      "2983 [D loss: (0.638)(R 0.594, F 0.682)]  [G loss: -0.686] \n",
      "2984 [D loss: (0.721)(R 0.775, F 0.668)]  [G loss: -0.661] \n",
      "2984 [D loss: (0.574)(R 0.529, F 0.619)]  [G loss: -0.659] \n",
      "2985 [D loss: (0.693)(R 0.716, F 0.671)]  [G loss: -0.633] \n",
      "2985 [D loss: (0.606)(R 0.596, F 0.617)]  [G loss: -0.658] \n",
      "2986 [D loss: (0.588)(R 0.535, F 0.640)]  [G loss: -0.651] \n",
      "2986 [D loss: (0.615)(R 0.614, F 0.617)]  [G loss: -0.647] \n",
      "2987 [D loss: (0.630)(R 0.588, F 0.672)]  [G loss: -0.659] \n",
      "2987 [D loss: (0.620)(R 0.614, F 0.626)]  [G loss: -0.654] \n",
      "2988 [D loss: (0.642)(R 0.634, F 0.650)]  [G loss: -0.621] \n",
      "2988 [D loss: (0.617)(R 0.597, F 0.636)]  [G loss: -0.648] \n",
      "2989 [D loss: (0.659)(R 0.669, F 0.649)]  [G loss: -0.643] \n",
      "2989 [D loss: (0.633)(R 0.630, F 0.637)]  [G loss: -0.645] \n",
      "2990 [D loss: (0.557)(R 0.492, F 0.621)]  [G loss: -0.626] \n",
      "2990 [D loss: (0.609)(R 0.581, F 0.636)]  [G loss: -0.625] \n",
      "2991 [D loss: (0.673)(R 0.760, F 0.586)]  [G loss: -0.636] \n",
      "2991 [D loss: (0.562)(R 0.487, F 0.636)]  [G loss: -0.624] \n",
      "2992 [D loss: (0.535)(R 0.424, F 0.647)]  [G loss: -0.619] \n",
      "2992 [D loss: (0.618)(R 0.623, F 0.612)]  [G loss: -0.611] \n",
      "2993 [D loss: (0.631)(R 0.637, F 0.624)]  [G loss: -0.614] \n",
      "2993 [D loss: (0.531)(R 0.449, F 0.612)]  [G loss: -0.612] \n",
      "2994 [D loss: (0.571)(R 0.530, F 0.611)]  [G loss: -0.604] \n",
      "2994 [D loss: (0.543)(R 0.468, F 0.617)]  [G loss: -0.595] \n",
      "2995 [D loss: (0.495)(R 0.415, F 0.575)]  [G loss: -0.577] \n",
      "2995 [D loss: (0.551)(R 0.469, F 0.633)]  [G loss: -0.601] \n",
      "2996 [D loss: (0.505)(R 0.474, F 0.536)]  [G loss: -0.587] \n",
      "2996 [D loss: (0.508)(R 0.454, F 0.563)]  [G loss: -0.587] \n",
      "2997 [D loss: (0.541)(R 0.489, F 0.593)]  [G loss: -0.575] \n",
      "2997 [D loss: (0.493)(R 0.417, F 0.569)]  [G loss: -0.573] \n",
      "2998 [D loss: (0.584)(R 0.617, F 0.551)]  [G loss: -0.573] \n",
      "2998 [D loss: (0.515)(R 0.477, F 0.553)]  [G loss: -0.584] \n",
      "2999 [D loss: (0.450)(R 0.315, F 0.585)]  [G loss: -0.551] \n",
      "2999 [D loss: (0.522)(R 0.466, F 0.577)]  [G loss: -0.565] \n",
      "3000 [D loss: (0.563)(R 0.551, F 0.574)]  [G loss: -0.563] \n",
      "3000 [D loss: (0.439)(R 0.345, F 0.533)]  [G loss: -0.560] \n",
      "3001 [D loss: (0.509)(R 0.455, F 0.563)]  [G loss: -0.579] \n",
      "3001 [D loss: (0.450)(R 0.391, F 0.508)]  [G loss: -0.555] \n",
      "3002 [D loss: (0.514)(R 0.465, F 0.562)]  [G loss: -0.542] \n",
      "3002 [D loss: (0.470)(R 0.358, F 0.582)]  [G loss: -0.551] \n",
      "3003 [D loss: (0.466)(R 0.407, F 0.525)]  [G loss: -0.548] \n",
      "3003 [D loss: (0.421)(R 0.336, F 0.506)]  [G loss: -0.540] \n",
      "3004 [D loss: (0.438)(R 0.335, F 0.542)]  [G loss: -0.526] \n",
      "3004 [D loss: (0.404)(R 0.284, F 0.523)]  [G loss: -0.529] \n",
      "3005 [D loss: (0.408)(R 0.315, F 0.501)]  [G loss: -0.516] \n",
      "3005 [D loss: (0.418)(R 0.313, F 0.524)]  [G loss: -0.536] \n",
      "3006 [D loss: (0.413)(R 0.288, F 0.539)]  [G loss: -0.514] \n",
      "3006 [D loss: (0.416)(R 0.278, F 0.553)]  [G loss: -0.521] \n",
      "3007 [D loss: (0.403)(R 0.278, F 0.528)]  [G loss: -0.522] \n",
      "3007 [D loss: (0.445)(R 0.381, F 0.509)]  [G loss: -0.506] \n",
      "3008 [D loss: (0.407)(R 0.298, F 0.516)]  [G loss: -0.509] \n",
      "3008 [D loss: (0.454)(R 0.398, F 0.510)]  [G loss: -0.511] \n",
      "3009 [D loss: (0.430)(R 0.318, F 0.542)]  [G loss: -0.498] \n",
      "3009 [D loss: (0.448)(R 0.390, F 0.506)]  [G loss: -0.504] \n",
      "3010 [D loss: (0.361)(R 0.236, F 0.485)]  [G loss: -0.491] \n",
      "3010 [D loss: (0.373)(R 0.247, F 0.500)]  [G loss: -0.496] \n",
      "3011 [D loss: (0.357)(R 0.223, F 0.491)]  [G loss: -0.488] \n",
      "3011 [D loss: (0.388)(R 0.261, F 0.515)]  [G loss: -0.502] \n",
      "3012 [D loss: (0.381)(R 0.297, F 0.466)]  [G loss: -0.496] \n",
      "3012 [D loss: (0.373)(R 0.266, F 0.479)]  [G loss: -0.489] \n",
      "3013 [D loss: (0.388)(R 0.305, F 0.471)]  [G loss: -0.486] \n",
      "3013 [D loss: (0.404)(R 0.319, F 0.488)]  [G loss: -0.484] \n",
      "3014 [D loss: (0.353)(R 0.220, F 0.487)]  [G loss: -0.481] \n",
      "3014 [D loss: (0.378)(R 0.292, F 0.465)]  [G loss: -0.489] \n",
      "3015 [D loss: (0.403)(R 0.316, F 0.491)]  [G loss: -0.466] \n",
      "3015 [D loss: (0.358)(R 0.249, F 0.466)]  [G loss: -0.472] \n",
      "3016 [D loss: (0.370)(R 0.280, F 0.459)]  [G loss: -0.459] \n",
      "3016 [D loss: (0.374)(R 0.294, F 0.454)]  [G loss: -0.460] \n",
      "3017 [D loss: (0.368)(R 0.258, F 0.478)]  [G loss: -0.461] \n",
      "3017 [D loss: (0.368)(R 0.263, F 0.473)]  [G loss: -0.449] \n",
      "3018 [D loss: (0.333)(R 0.203, F 0.463)]  [G loss: -0.467] \n",
      "3018 [D loss: (0.342)(R 0.247, F 0.436)]  [G loss: -0.449] \n",
      "3019 [D loss: (0.335)(R 0.226, F 0.445)]  [G loss: -0.433] \n",
      "3019 [D loss: (0.336)(R 0.228, F 0.444)]  [G loss: -0.448] \n",
      "3020 [D loss: (0.365)(R 0.278, F 0.452)]  [G loss: -0.447] \n",
      "3020 [D loss: (0.333)(R 0.229, F 0.436)]  [G loss: -0.445] \n",
      "3021 [D loss: (0.336)(R 0.243, F 0.429)]  [G loss: -0.437] \n",
      "3021 [D loss: (0.313)(R 0.187, F 0.438)]  [G loss: -0.437] \n",
      "3022 [D loss: (0.318)(R 0.201, F 0.435)]  [G loss: -0.417] \n",
      "3022 [D loss: (0.284)(R 0.153, F 0.414)]  [G loss: -0.417] \n",
      "3023 [D loss: (0.345)(R 0.247, F 0.443)]  [G loss: -0.441] \n",
      "3023 [D loss: (0.316)(R 0.230, F 0.402)]  [G loss: -0.427] \n",
      "3024 [D loss: (0.285)(R 0.144, F 0.426)]  [G loss: -0.426] \n",
      "3024 [D loss: (0.326)(R 0.231, F 0.421)]  [G loss: -0.422] \n",
      "3025 [D loss: (0.327)(R 0.217, F 0.436)]  [G loss: -0.419] \n",
      "3025 [D loss: (0.311)(R 0.203, F 0.418)]  [G loss: -0.419] \n",
      "3026 [D loss: (0.347)(R 0.248, F 0.445)]  [G loss: -0.426] \n",
      "3026 [D loss: (0.289)(R 0.175, F 0.404)]  [G loss: -0.421] \n",
      "3027 [D loss: (0.330)(R 0.222, F 0.439)]  [G loss: -0.413] \n",
      "3027 [D loss: (0.277)(R 0.160, F 0.393)]  [G loss: -0.412] \n",
      "3028 [D loss: (0.320)(R 0.208, F 0.432)]  [G loss: -0.410] \n",
      "3028 [D loss: (0.320)(R 0.226, F 0.413)]  [G loss: -0.406] \n",
      "3029 [D loss: (0.302)(R 0.199, F 0.405)]  [G loss: -0.408] \n",
      "3029 [D loss: (0.291)(R 0.184, F 0.397)]  [G loss: -0.389] \n",
      "3030 [D loss: (0.301)(R 0.185, F 0.416)]  [G loss: -0.393] \n",
      "3030 [D loss: (0.276)(R 0.174, F 0.378)]  [G loss: -0.396] \n",
      "3031 [D loss: (0.299)(R 0.209, F 0.390)]  [G loss: -0.397] \n",
      "3031 [D loss: (0.285)(R 0.176, F 0.394)]  [G loss: -0.375] \n",
      "3032 [D loss: (0.273)(R 0.167, F 0.379)]  [G loss: -0.388] \n",
      "3032 [D loss: (0.260)(R 0.151, F 0.369)]  [G loss: -0.383] \n",
      "3033 [D loss: (0.252)(R 0.117, F 0.387)]  [G loss: -0.386] \n",
      "3033 [D loss: (0.239)(R 0.098, F 0.380)]  [G loss: -0.382] \n",
      "3034 [D loss: (0.279)(R 0.194, F 0.364)]  [G loss: -0.379] \n",
      "3034 [D loss: (0.262)(R 0.136, F 0.388)]  [G loss: -0.373] \n",
      "3035 [D loss: (0.269)(R 0.159, F 0.379)]  [G loss: -0.371] \n",
      "3035 [D loss: (0.272)(R 0.162, F 0.382)]  [G loss: -0.372] \n",
      "3036 [D loss: (0.253)(R 0.143, F 0.364)]  [G loss: -0.372] \n",
      "3036 [D loss: (0.262)(R 0.153, F 0.371)]  [G loss: -0.373] \n",
      "3037 [D loss: (0.261)(R 0.167, F 0.355)]  [G loss: -0.360] \n",
      "3037 [D loss: (0.224)(R 0.078, F 0.370)]  [G loss: -0.371] \n",
      "3038 [D loss: (0.237)(R 0.109, F 0.364)]  [G loss: -0.367] \n",
      "3038 [D loss: (0.250)(R 0.126, F 0.375)]  [G loss: -0.364] \n",
      "3039 [D loss: (0.248)(R 0.151, F 0.346)]  [G loss: -0.357] \n",
      "3039 [D loss: (0.293)(R 0.239, F 0.348)]  [G loss: -0.362] \n",
      "3040 [D loss: (0.244)(R 0.131, F 0.356)]  [G loss: -0.344] \n",
      "3040 [D loss: (0.233)(R 0.124, F 0.341)]  [G loss: -0.353] \n",
      "3041 [D loss: (0.230)(R 0.098, F 0.361)]  [G loss: -0.352] \n",
      "3041 [D loss: (0.254)(R 0.162, F 0.346)]  [G loss: -0.351] \n",
      "3042 [D loss: (0.221)(R 0.089, F 0.354)]  [G loss: -0.350] \n",
      "3042 [D loss: (0.239)(R 0.119, F 0.358)]  [G loss: -0.349] \n",
      "3043 [D loss: (0.218)(R 0.077, F 0.358)]  [G loss: -0.344] \n",
      "3043 [D loss: (0.230)(R 0.103, F 0.357)]  [G loss: -0.344] \n",
      "3044 [D loss: (0.216)(R 0.084, F 0.348)]  [G loss: -0.339] \n",
      "3044 [D loss: (0.237)(R 0.128, F 0.346)]  [G loss: -0.334] \n",
      "3045 [D loss: (0.219)(R 0.098, F 0.340)]  [G loss: -0.333] \n",
      "3045 [D loss: (0.230)(R 0.131, F 0.330)]  [G loss: -0.328] \n",
      "3046 [D loss: (0.216)(R 0.114, F 0.318)]  [G loss: -0.332] \n",
      "3046 [D loss: (0.238)(R 0.149, F 0.326)]  [G loss: -0.330] \n",
      "3047 [D loss: (0.217)(R 0.090, F 0.345)]  [G loss: -0.336] \n",
      "3047 [D loss: (0.229)(R 0.124, F 0.335)]  [G loss: -0.328] \n",
      "3048 [D loss: (0.231)(R 0.119, F 0.344)]  [G loss: -0.322] \n",
      "3048 [D loss: (0.212)(R 0.086, F 0.338)]  [G loss: -0.326] \n",
      "3049 [D loss: (0.205)(R 0.084, F 0.326)]  [G loss: -0.323] \n",
      "3049 [D loss: (0.190)(R 0.072, F 0.307)]  [G loss: -0.319] \n",
      "3050 [D loss: (0.198)(R 0.081, F 0.316)]  [G loss: -0.318] \n",
      "3050 [D loss: (0.198)(R 0.083, F 0.313)]  [G loss: -0.308] \n",
      "3051 [D loss: (0.223)(R 0.138, F 0.308)]  [G loss: -0.301] \n",
      "3051 [D loss: (0.206)(R 0.086, F 0.326)]  [G loss: -0.318] \n",
      "3052 [D loss: (0.188)(R 0.056, F 0.320)]  [G loss: -0.312] \n",
      "3052 [D loss: (0.188)(R 0.069, F 0.307)]  [G loss: -0.306] \n",
      "3053 [D loss: (0.184)(R 0.060, F 0.307)]  [G loss: -0.308] \n",
      "3053 [D loss: (0.255)(R 0.198, F 0.311)]  [G loss: -0.309] \n",
      "3054 [D loss: (0.213)(R 0.119, F 0.306)]  [G loss: -0.297] \n",
      "3054 [D loss: (0.168)(R 0.044, F 0.291)]  [G loss: -0.298] \n",
      "3055 [D loss: (0.193)(R 0.090, F 0.295)]  [G loss: -0.292] \n",
      "3055 [D loss: (0.182)(R 0.068, F 0.296)]  [G loss: -0.297] \n",
      "3056 [D loss: (0.183)(R 0.073, F 0.294)]  [G loss: -0.294] \n",
      "3056 [D loss: (0.183)(R 0.074, F 0.293)]  [G loss: -0.301] \n",
      "3057 [D loss: (0.178)(R 0.057, F 0.300)]  [G loss: -0.289] \n",
      "3057 [D loss: (0.173)(R 0.046, F 0.299)]  [G loss: -0.291] \n",
      "3058 [D loss: (0.174)(R 0.069, F 0.278)]  [G loss: -0.293] \n",
      "3058 [D loss: (0.167)(R 0.048, F 0.287)]  [G loss: -0.275] \n",
      "3059 [D loss: (0.196)(R 0.097, F 0.294)]  [G loss: -0.285] \n",
      "3059 [D loss: (0.181)(R 0.082, F 0.280)]  [G loss: -0.285] \n",
      "3060 [D loss: (0.203)(R 0.104, F 0.302)]  [G loss: -0.281] \n",
      "3060 [D loss: (0.189)(R 0.105, F 0.274)]  [G loss: -0.277] \n",
      "3061 [D loss: (0.183)(R 0.096, F 0.270)]  [G loss: -0.278] \n",
      "3061 [D loss: (0.171)(R 0.066, F 0.276)]  [G loss: -0.275] \n",
      "3062 [D loss: (0.165)(R 0.068, F 0.262)]  [G loss: -0.276] \n",
      "3062 [D loss: (0.171)(R 0.068, F 0.274)]  [G loss: -0.273] \n",
      "3063 [D loss: (0.154)(R 0.054, F 0.254)]  [G loss: -0.268] \n",
      "3063 [D loss: (0.179)(R 0.089, F 0.270)]  [G loss: -0.274] \n",
      "3064 [D loss: (0.186)(R 0.107, F 0.265)]  [G loss: -0.275] \n",
      "3064 [D loss: (0.177)(R 0.078, F 0.275)]  [G loss: -0.269] \n",
      "3065 [D loss: (0.152)(R 0.038, F 0.266)]  [G loss: -0.259] \n",
      "3065 [D loss: (0.180)(R 0.091, F 0.268)]  [G loss: -0.264] \n",
      "3066 [D loss: (0.149)(R 0.037, F 0.261)]  [G loss: -0.266] \n",
      "3066 [D loss: (0.171)(R 0.081, F 0.261)]  [G loss: -0.260] \n",
      "3067 [D loss: (0.159)(R 0.069, F 0.248)]  [G loss: -0.259] \n",
      "3067 [D loss: (0.163)(R 0.072, F 0.254)]  [G loss: -0.256] \n",
      "3068 [D loss: (0.190)(R 0.109, F 0.270)]  [G loss: -0.263] \n",
      "3068 [D loss: (0.150)(R 0.037, F 0.262)]  [G loss: -0.259] \n",
      "3069 [D loss: (0.171)(R 0.078, F 0.263)]  [G loss: -0.252] \n",
      "3069 [D loss: (0.162)(R 0.080, F 0.245)]  [G loss: -0.248] \n",
      "3070 [D loss: (0.164)(R 0.079, F 0.250)]  [G loss: -0.250] \n",
      "3070 [D loss: (0.153)(R 0.053, F 0.253)]  [G loss: -0.247] \n",
      "3071 [D loss: (0.155)(R 0.056, F 0.254)]  [G loss: -0.245] \n",
      "3071 [D loss: (0.159)(R 0.074, F 0.244)]  [G loss: -0.245] \n",
      "3072 [D loss: (0.158)(R 0.072, F 0.244)]  [G loss: -0.243] \n",
      "3072 [D loss: (0.157)(R 0.066, F 0.248)]  [G loss: -0.242] \n",
      "3073 [D loss: (0.151)(R 0.055, F 0.248)]  [G loss: -0.238] \n",
      "3073 [D loss: (0.142)(R 0.036, F 0.248)]  [G loss: -0.242] \n",
      "3074 [D loss: (0.144)(R 0.040, F 0.249)]  [G loss: -0.240] \n",
      "3074 [D loss: (0.140)(R 0.044, F 0.236)]  [G loss: -0.238] \n",
      "3075 [D loss: (0.141)(R 0.035, F 0.247)]  [G loss: -0.235] \n",
      "3075 [D loss: (0.145)(R 0.063, F 0.227)]  [G loss: -0.238] \n",
      "3076 [D loss: (0.133)(R 0.042, F 0.224)]  [G loss: -0.235] \n",
      "3076 [D loss: (0.124)(R 0.021, F 0.227)]  [G loss: -0.229] \n",
      "3077 [D loss: (0.148)(R 0.056, F 0.239)]  [G loss: -0.232] \n",
      "3077 [D loss: (0.136)(R 0.031, F 0.241)]  [G loss: -0.223] \n",
      "3078 [D loss: (0.140)(R 0.050, F 0.231)]  [G loss: -0.227] \n",
      "3078 [D loss: (0.146)(R 0.063, F 0.230)]  [G loss: -0.227] \n",
      "3079 [D loss: (0.136)(R 0.048, F 0.224)]  [G loss: -0.227] \n",
      "3079 [D loss: (0.129)(R 0.045, F 0.214)]  [G loss: -0.222] \n",
      "3080 [D loss: (0.126)(R 0.042, F 0.210)]  [G loss: -0.225] \n",
      "3080 [D loss: (0.122)(R 0.022, F 0.222)]  [G loss: -0.222] \n",
      "3081 [D loss: (0.131)(R 0.057, F 0.205)]  [G loss: -0.219] \n",
      "3081 [D loss: (0.133)(R 0.045, F 0.222)]  [G loss: -0.217] \n",
      "3082 [D loss: (0.126)(R 0.040, F 0.212)]  [G loss: -0.216] \n",
      "3082 [D loss: (0.127)(R 0.035, F 0.219)]  [G loss: -0.215] \n",
      "3083 [D loss: (0.139)(R 0.058, F 0.219)]  [G loss: -0.214] \n",
      "3083 [D loss: (0.132)(R 0.037, F 0.226)]  [G loss: -0.220] \n",
      "3084 [D loss: (0.127)(R 0.042, F 0.212)]  [G loss: -0.209] \n",
      "3084 [D loss: (0.132)(R 0.048, F 0.215)]  [G loss: -0.207] \n",
      "3085 [D loss: (0.131)(R 0.045, F 0.217)]  [G loss: -0.203] \n",
      "3085 [D loss: (0.134)(R 0.053, F 0.214)]  [G loss: -0.207] \n",
      "3086 [D loss: (0.128)(R 0.051, F 0.206)]  [G loss: -0.206] \n",
      "3086 [D loss: (0.115)(R 0.028, F 0.201)]  [G loss: -0.205] \n",
      "3087 [D loss: (0.115)(R 0.033, F 0.198)]  [G loss: -0.205] \n",
      "3087 [D loss: (0.114)(R 0.022, F 0.206)]  [G loss: -0.202] \n",
      "3088 [D loss: (0.111)(R 0.020, F 0.201)]  [G loss: -0.203] \n",
      "3088 [D loss: (0.114)(R 0.030, F 0.197)]  [G loss: -0.195] \n",
      "3089 [D loss: (0.121)(R 0.044, F 0.197)]  [G loss: -0.200] \n",
      "3089 [D loss: (0.101)(R 0.013, F 0.189)]  [G loss: -0.197] \n",
      "3090 [D loss: (0.117)(R 0.040, F 0.195)]  [G loss: -0.196] \n",
      "3090 [D loss: (0.109)(R 0.020, F 0.198)]  [G loss: -0.194] \n",
      "3091 [D loss: (0.109)(R 0.020, F 0.198)]  [G loss: -0.191] \n",
      "3091 [D loss: (0.112)(R 0.040, F 0.183)]  [G loss: -0.194] \n",
      "3092 [D loss: (0.108)(R 0.023, F 0.193)]  [G loss: -0.186] \n",
      "3092 [D loss: (0.105)(R 0.020, F 0.190)]  [G loss: -0.190] \n",
      "3093 [D loss: (0.115)(R 0.040, F 0.190)]  [G loss: -0.188] \n",
      "3093 [D loss: (0.106)(R 0.033, F 0.179)]  [G loss: -0.188] \n",
      "3094 [D loss: (0.117)(R 0.043, F 0.190)]  [G loss: -0.189] \n",
      "3094 [D loss: (0.111)(R 0.029, F 0.193)]  [G loss: -0.190] \n",
      "3095 [D loss: (0.117)(R 0.054, F 0.180)]  [G loss: -0.183] \n",
      "3095 [D loss: (0.108)(R 0.042, F 0.174)]  [G loss: -0.187] \n",
      "3096 [D loss: (0.107)(R 0.033, F 0.180)]  [G loss: -0.181] \n",
      "3096 [D loss: (0.101)(R 0.018, F 0.184)]  [G loss: -0.182] \n",
      "3097 [D loss: (0.113)(R 0.035, F 0.191)]  [G loss: -0.180] \n",
      "3097 [D loss: (0.103)(R 0.024, F 0.181)]  [G loss: -0.178] \n",
      "3098 [D loss: (0.099)(R 0.028, F 0.170)]  [G loss: -0.175] \n",
      "3098 [D loss: (0.099)(R 0.013, F 0.184)]  [G loss: -0.179] \n",
      "3099 [D loss: (0.102)(R 0.025, F 0.178)]  [G loss: -0.174] \n",
      "3099 [D loss: (0.094)(R 0.018, F 0.169)]  [G loss: -0.174] \n",
      "3100 [D loss: (0.103)(R 0.030, F 0.176)]  [G loss: -0.174] \n",
      "3100 [D loss: (0.105)(R 0.039, F 0.171)]  [G loss: -0.175] \n",
      "3101 [D loss: (0.099)(R 0.021, F 0.178)]  [G loss: -0.170] \n",
      "3101 [D loss: (0.099)(R 0.024, F 0.174)]  [G loss: -0.173] \n",
      "3102 [D loss: (0.094)(R 0.022, F 0.166)]  [G loss: -0.170] \n",
      "3102 [D loss: (0.089)(R 0.011, F 0.166)]  [G loss: -0.167] \n",
      "3103 [D loss: (0.090)(R 0.017, F 0.164)]  [G loss: -0.165] \n",
      "3103 [D loss: (0.094)(R 0.020, F 0.168)]  [G loss: -0.167] \n",
      "3104 [D loss: (0.096)(R 0.017, F 0.174)]  [G loss: -0.162] \n",
      "3104 [D loss: (0.101)(R 0.030, F 0.171)]  [G loss: -0.164] \n",
      "3105 [D loss: (0.092)(R 0.031, F 0.153)]  [G loss: -0.162] \n",
      "3105 [D loss: (0.090)(R 0.020, F 0.159)]  [G loss: -0.160] \n",
      "3106 [D loss: (0.093)(R 0.018, F 0.169)]  [G loss: -0.161] \n",
      "3106 [D loss: (0.088)(R 0.017, F 0.160)]  [G loss: -0.161] \n",
      "3107 [D loss: (0.088)(R 0.015, F 0.162)]  [G loss: -0.157] \n",
      "3107 [D loss: (0.082)(R 0.008, F 0.157)]  [G loss: -0.154] \n",
      "3108 [D loss: (0.096)(R 0.031, F 0.160)]  [G loss: -0.159] \n",
      "3108 [D loss: (0.098)(R 0.035, F 0.161)]  [G loss: -0.155] \n",
      "3109 [D loss: (0.085)(R 0.013, F 0.158)]  [G loss: -0.155] \n",
      "3109 [D loss: (0.076)(R 0.001, F 0.151)]  [G loss: -0.156] \n",
      "3110 [D loss: (0.088)(R 0.015, F 0.161)]  [G loss: -0.153] \n",
      "3110 [D loss: (0.085)(R 0.014, F 0.155)]  [G loss: -0.150] \n",
      "3111 [D loss: (0.090)(R 0.026, F 0.153)]  [G loss: -0.150] \n",
      "3111 [D loss: (0.088)(R 0.027, F 0.148)]  [G loss: -0.148] \n",
      "3112 [D loss: (0.084)(R 0.018, F 0.150)]  [G loss: -0.148] \n",
      "3112 [D loss: (0.084)(R 0.022, F 0.147)]  [G loss: -0.149] \n",
      "3113 [D loss: (0.075)(R 0.005, F 0.145)]  [G loss: -0.147] \n",
      "3113 [D loss: (0.072)(R 0.000, F 0.144)]  [G loss: -0.146] \n",
      "3114 [D loss: (0.084)(R 0.028, F 0.139)]  [G loss: -0.145] \n",
      "3114 [D loss: (0.074)(R 0.008, F 0.139)]  [G loss: -0.142] \n",
      "3115 [D loss: (0.074)(R 0.010, F 0.138)]  [G loss: -0.141] \n",
      "3115 [D loss: (0.083)(R 0.021, F 0.145)]  [G loss: -0.140] \n",
      "3116 [D loss: (0.075)(R 0.013, F 0.137)]  [G loss: -0.144] \n",
      "3116 [D loss: (0.074)(R 0.001, F 0.147)]  [G loss: -0.144] \n",
      "3117 [D loss: (0.085)(R 0.034, F 0.136)]  [G loss: -0.138] \n",
      "3117 [D loss: (0.075)(R 0.014, F 0.137)]  [G loss: -0.137] \n",
      "3118 [D loss: (0.083)(R 0.029, F 0.138)]  [G loss: -0.135] \n",
      "3118 [D loss: (0.064)(R -0.004, F 0.133)]  [G loss: -0.133] \n",
      "3119 [D loss: (0.078)(R 0.022, F 0.134)]  [G loss: -0.136] \n",
      "3119 [D loss: (0.082)(R 0.025, F 0.139)]  [G loss: -0.134] \n",
      "3120 [D loss: (0.074)(R 0.010, F 0.138)]  [G loss: -0.134] \n",
      "3120 [D loss: (0.077)(R 0.019, F 0.134)]  [G loss: -0.132] \n",
      "3121 [D loss: (0.069)(R 0.004, F 0.134)]  [G loss: -0.130] \n",
      "3121 [D loss: (0.065)(R -0.005, F 0.134)]  [G loss: -0.132] \n",
      "3122 [D loss: (0.066)(R 0.006, F 0.126)]  [G loss: -0.130] \n",
      "3122 [D loss: (0.070)(R 0.005, F 0.136)]  [G loss: -0.131] \n",
      "3123 [D loss: (0.071)(R 0.010, F 0.133)]  [G loss: -0.126] \n",
      "3123 [D loss: (0.062)(R 0.002, F 0.123)]  [G loss: -0.131] \n",
      "3124 [D loss: (0.066)(R 0.004, F 0.128)]  [G loss: -0.126] \n",
      "3124 [D loss: (0.071)(R 0.010, F 0.133)]  [G loss: -0.124] \n",
      "3125 [D loss: (0.067)(R 0.015, F 0.119)]  [G loss: -0.126] \n",
      "3125 [D loss: (0.065)(R 0.004, F 0.126)]  [G loss: -0.124] \n",
      "3126 [D loss: (0.063)(R 0.002, F 0.123)]  [G loss: -0.121] \n",
      "3126 [D loss: (0.064)(R 0.007, F 0.122)]  [G loss: -0.124] \n",
      "3127 [D loss: (0.070)(R 0.014, F 0.125)]  [G loss: -0.121] \n",
      "3127 [D loss: (0.062)(R 0.008, F 0.115)]  [G loss: -0.120] \n",
      "3128 [D loss: (0.067)(R 0.023, F 0.112)]  [G loss: -0.121] \n",
      "3128 [D loss: (0.062)(R 0.007, F 0.118)]  [G loss: -0.119] \n",
      "3129 [D loss: (0.078)(R 0.035, F 0.120)]  [G loss: -0.119] \n",
      "3129 [D loss: (0.063)(R 0.012, F 0.115)]  [G loss: -0.114] \n",
      "3130 [D loss: (0.062)(R 0.003, F 0.121)]  [G loss: -0.118] \n",
      "3130 [D loss: (0.068)(R 0.020, F 0.116)]  [G loss: -0.116] \n",
      "3131 [D loss: (0.064)(R 0.017, F 0.112)]  [G loss: -0.115] \n",
      "3131 [D loss: (0.064)(R 0.013, F 0.115)]  [G loss: -0.115] \n",
      "3132 [D loss: (0.056)(R -0.000, F 0.113)]  [G loss: -0.112] \n",
      "3132 [D loss: (0.063)(R 0.013, F 0.112)]  [G loss: -0.115] \n",
      "3133 [D loss: (0.056)(R -0.001, F 0.112)]  [G loss: -0.115] \n",
      "3133 [D loss: (0.060)(R 0.006, F 0.113)]  [G loss: -0.112] \n",
      "3134 [D loss: (0.059)(R 0.013, F 0.104)]  [G loss: -0.111] \n",
      "3134 [D loss: (0.060)(R 0.009, F 0.111)]  [G loss: -0.109] \n",
      "3135 [D loss: (0.061)(R 0.012, F 0.111)]  [G loss: -0.108] \n",
      "3135 [D loss: (0.052)(R -0.000, F 0.104)]  [G loss: -0.109] \n",
      "3136 [D loss: (0.053)(R -0.000, F 0.105)]  [G loss: -0.109] \n",
      "3136 [D loss: (0.053)(R 0.004, F 0.102)]  [G loss: -0.105] \n",
      "3137 [D loss: (0.048)(R -0.008, F 0.105)]  [G loss: -0.105] \n",
      "3137 [D loss: (0.057)(R 0.011, F 0.103)]  [G loss: -0.107] \n",
      "3138 [D loss: (0.050)(R -0.004, F 0.104)]  [G loss: -0.103] \n",
      "3138 [D loss: (0.058)(R 0.014, F 0.103)]  [G loss: -0.103] \n",
      "3139 [D loss: (0.052)(R 0.004, F 0.101)]  [G loss: -0.102] \n",
      "3139 [D loss: (0.058)(R 0.009, F 0.108)]  [G loss: -0.101] \n",
      "3140 [D loss: (0.054)(R 0.006, F 0.102)]  [G loss: -0.102] \n",
      "3140 [D loss: (0.058)(R 0.015, F 0.101)]  [G loss: -0.100] \n",
      "3141 [D loss: (0.050)(R 0.000, F 0.100)]  [G loss: -0.099] \n",
      "3141 [D loss: (0.054)(R 0.007, F 0.100)]  [G loss: -0.100] \n",
      "3142 [D loss: (0.055)(R 0.007, F 0.102)]  [G loss: -0.097] \n",
      "3142 [D loss: (0.051)(R 0.005, F 0.097)]  [G loss: -0.097] \n",
      "3143 [D loss: (0.049)(R 0.002, F 0.097)]  [G loss: -0.098] \n",
      "3143 [D loss: (0.053)(R 0.007, F 0.099)]  [G loss: -0.097] \n",
      "3144 [D loss: (0.047)(R -0.006, F 0.100)]  [G loss: -0.097] \n",
      "3144 [D loss: (0.055)(R 0.013, F 0.096)]  [G loss: -0.096] \n",
      "3145 [D loss: (0.048)(R 0.003, F 0.094)]  [G loss: -0.094] \n",
      "3145 [D loss: (0.045)(R -0.002, F 0.091)]  [G loss: -0.092] \n",
      "3146 [D loss: (0.052)(R 0.017, F 0.088)]  [G loss: -0.093] \n",
      "3146 [D loss: (0.054)(R 0.014, F 0.094)]  [G loss: -0.092] \n",
      "3147 [D loss: (0.048)(R 0.003, F 0.092)]  [G loss: -0.093] \n",
      "3147 [D loss: (0.050)(R 0.009, F 0.090)]  [G loss: -0.091] \n",
      "3148 [D loss: (0.052)(R 0.013, F 0.090)]  [G loss: -0.090] \n",
      "3148 [D loss: (0.044)(R 0.000, F 0.088)]  [G loss: -0.088] \n",
      "3149 [D loss: (0.048)(R 0.004, F 0.091)]  [G loss: -0.088] \n",
      "3149 [D loss: (0.045)(R 0.003, F 0.087)]  [G loss: -0.088] \n",
      "3150 [D loss: (0.045)(R -0.000, F 0.090)]  [G loss: -0.087] \n",
      "3150 [D loss: (0.045)(R 0.001, F 0.090)]  [G loss: -0.087] \n",
      "3151 [D loss: (0.048)(R 0.008, F 0.088)]  [G loss: -0.086] \n",
      "3151 [D loss: (0.046)(R 0.005, F 0.087)]  [G loss: -0.085] \n",
      "3152 [D loss: (0.043)(R 0.002, F 0.084)]  [G loss: -0.086] \n",
      "3152 [D loss: (0.043)(R 0.002, F 0.083)]  [G loss: -0.082] \n",
      "3153 [D loss: (0.040)(R 0.000, F 0.079)]  [G loss: -0.084] \n",
      "3153 [D loss: (0.041)(R 0.002, F 0.080)]  [G loss: -0.085] \n",
      "3154 [D loss: (0.039)(R -0.001, F 0.079)]  [G loss: -0.083] \n",
      "3154 [D loss: (0.038)(R -0.002, F 0.078)]  [G loss: -0.081] \n",
      "3155 [D loss: (0.043)(R 0.006, F 0.080)]  [G loss: -0.081] \n",
      "3155 [D loss: (0.041)(R 0.003, F 0.079)]  [G loss: -0.081] \n",
      "3156 [D loss: (0.042)(R 0.005, F 0.078)]  [G loss: -0.079] \n",
      "3156 [D loss: (0.046)(R 0.010, F 0.081)]  [G loss: -0.079] \n",
      "3157 [D loss: (0.040)(R 0.002, F 0.079)]  [G loss: -0.077] \n",
      "3157 [D loss: (0.039)(R -0.002, F 0.080)]  [G loss: -0.078] \n",
      "3158 [D loss: (0.044)(R 0.007, F 0.082)]  [G loss: -0.077] \n",
      "3158 [D loss: (0.042)(R 0.010, F 0.075)]  [G loss: -0.078] \n",
      "3159 [D loss: (0.041)(R 0.004, F 0.078)]  [G loss: -0.077] \n",
      "3159 [D loss: (0.038)(R 0.001, F 0.075)]  [G loss: -0.075] \n",
      "3160 [D loss: (0.035)(R -0.005, F 0.075)]  [G loss: -0.074] \n",
      "3160 [D loss: (0.042)(R 0.009, F 0.074)]  [G loss: -0.074] \n",
      "3161 [D loss: (0.038)(R -0.001, F 0.078)]  [G loss: -0.074] \n",
      "3161 [D loss: (0.037)(R 0.001, F 0.074)]  [G loss: -0.072] \n",
      "3162 [D loss: (0.036)(R 0.002, F 0.070)]  [G loss: -0.071] \n",
      "3162 [D loss: (0.038)(R 0.002, F 0.074)]  [G loss: -0.073] \n",
      "3163 [D loss: (0.041)(R 0.009, F 0.072)]  [G loss: -0.073] \n",
      "3163 [D loss: (0.038)(R 0.003, F 0.073)]  [G loss: -0.070] \n",
      "3164 [D loss: (0.037)(R 0.005, F 0.070)]  [G loss: -0.070] \n",
      "3164 [D loss: (0.033)(R -0.002, F 0.068)]  [G loss: -0.070] \n",
      "3165 [D loss: (0.036)(R 0.001, F 0.072)]  [G loss: -0.067] \n",
      "3165 [D loss: (0.035)(R -0.001, F 0.071)]  [G loss: -0.068] \n",
      "3166 [D loss: (0.034)(R 0.001, F 0.067)]  [G loss: -0.069] \n",
      "3166 [D loss: (0.033)(R 0.002, F 0.064)]  [G loss: -0.066] \n",
      "3167 [D loss: (0.032)(R -0.001, F 0.064)]  [G loss: -0.067] \n",
      "3167 [D loss: (0.030)(R -0.006, F 0.066)]  [G loss: -0.067] \n",
      "3168 [D loss: (0.030)(R -0.008, F 0.067)]  [G loss: -0.067] \n",
      "3168 [D loss: (0.031)(R -0.002, F 0.065)]  [G loss: -0.066] \n",
      "3169 [D loss: (0.031)(R -0.003, F 0.065)]  [G loss: -0.065] \n",
      "3169 [D loss: (0.034)(R 0.002, F 0.065)]  [G loss: -0.064] \n",
      "3170 [D loss: (0.028)(R -0.006, F 0.062)]  [G loss: -0.064] \n",
      "3170 [D loss: (0.030)(R -0.001, F 0.062)]  [G loss: -0.064] \n",
      "3171 [D loss: (0.031)(R -0.000, F 0.062)]  [G loss: -0.063] \n",
      "3171 [D loss: (0.032)(R 0.002, F 0.062)]  [G loss: -0.060] \n",
      "3172 [D loss: (0.031)(R 0.001, F 0.060)]  [G loss: -0.061] \n",
      "3172 [D loss: (0.034)(R 0.005, F 0.062)]  [G loss: -0.062] \n",
      "3173 [D loss: (0.033)(R 0.009, F 0.058)]  [G loss: -0.061] \n",
      "3173 [D loss: (0.032)(R 0.001, F 0.063)]  [G loss: -0.061] \n",
      "3174 [D loss: (0.030)(R 0.001, F 0.059)]  [G loss: -0.058] \n",
      "3174 [D loss: (0.027)(R -0.003, F 0.058)]  [G loss: -0.060] \n",
      "3175 [D loss: (0.027)(R -0.003, F 0.057)]  [G loss: -0.058] \n",
      "3175 [D loss: (0.033)(R 0.005, F 0.060)]  [G loss: -0.059] \n",
      "3176 [D loss: (0.031)(R -0.000, F 0.062)]  [G loss: -0.059] \n",
      "3176 [D loss: (0.028)(R -0.001, F 0.057)]  [G loss: -0.057] \n",
      "3177 [D loss: (0.027)(R -0.004, F 0.057)]  [G loss: -0.057] \n",
      "3177 [D loss: (0.031)(R 0.005, F 0.058)]  [G loss: -0.056] \n",
      "3178 [D loss: (0.027)(R -0.002, F 0.056)]  [G loss: -0.055] \n",
      "3178 [D loss: (0.027)(R 0.001, F 0.054)]  [G loss: -0.056] \n",
      "3179 [D loss: (0.030)(R 0.003, F 0.057)]  [G loss: -0.055] \n",
      "3179 [D loss: (0.028)(R 0.003, F 0.053)]  [G loss: -0.054] \n",
      "3180 [D loss: (0.028)(R 0.003, F 0.054)]  [G loss: -0.053] \n",
      "3180 [D loss: (0.026)(R 0.001, F 0.051)]  [G loss: -0.053] \n",
      "3181 [D loss: (0.024)(R -0.005, F 0.053)]  [G loss: -0.052] \n",
      "3181 [D loss: (0.029)(R 0.004, F 0.054)]  [G loss: -0.051] \n",
      "3182 [D loss: (0.026)(R 0.004, F 0.047)]  [G loss: -0.052] \n",
      "3182 [D loss: (0.027)(R 0.004, F 0.049)]  [G loss: -0.052] \n",
      "3183 [D loss: (0.029)(R 0.005, F 0.054)]  [G loss: -0.049] \n",
      "3183 [D loss: (0.029)(R 0.005, F 0.053)]  [G loss: -0.050] \n",
      "3184 [D loss: (0.025)(R -0.000, F 0.050)]  [G loss: -0.050] \n",
      "3184 [D loss: (0.027)(R 0.002, F 0.052)]  [G loss: -0.049] \n",
      "3185 [D loss: (0.026)(R 0.003, F 0.048)]  [G loss: -0.048] \n",
      "3185 [D loss: (0.025)(R 0.002, F 0.048)]  [G loss: -0.048] \n",
      "3186 [D loss: (0.026)(R 0.005, F 0.047)]  [G loss: -0.047] \n",
      "3186 [D loss: (0.025)(R 0.005, F 0.046)]  [G loss: -0.048] \n",
      "3187 [D loss: (0.024)(R 0.001, F 0.047)]  [G loss: -0.046] \n",
      "3187 [D loss: (0.025)(R 0.003, F 0.046)]  [G loss: -0.047] \n",
      "3188 [D loss: (0.022)(R -0.001, F 0.045)]  [G loss: -0.046] \n",
      "3188 [D loss: (0.023)(R 0.002, F 0.044)]  [G loss: -0.046] \n",
      "3189 [D loss: (0.022)(R -0.000, F 0.043)]  [G loss: -0.045] \n",
      "3189 [D loss: (0.022)(R 0.000, F 0.043)]  [G loss: -0.043] \n",
      "3190 [D loss: (0.020)(R -0.002, F 0.042)]  [G loss: -0.043] \n",
      "3190 [D loss: (0.023)(R 0.002, F 0.044)]  [G loss: -0.044] \n",
      "3191 [D loss: (0.021)(R 0.002, F 0.041)]  [G loss: -0.044] \n",
      "3191 [D loss: (0.024)(R 0.004, F 0.044)]  [G loss: -0.042] \n",
      "3192 [D loss: (0.019)(R -0.002, F 0.040)]  [G loss: -0.042] \n",
      "3192 [D loss: (0.020)(R 0.001, F 0.038)]  [G loss: -0.042] \n",
      "3193 [D loss: (0.021)(R 0.003, F 0.039)]  [G loss: -0.040] \n",
      "3193 [D loss: (0.022)(R 0.003, F 0.041)]  [G loss: -0.041] \n",
      "3194 [D loss: (0.021)(R 0.002, F 0.040)]  [G loss: -0.041] \n",
      "3194 [D loss: (0.019)(R -0.001, F 0.039)]  [G loss: -0.040] \n",
      "3195 [D loss: (0.021)(R 0.004, F 0.038)]  [G loss: -0.039] \n",
      "3195 [D loss: (0.021)(R 0.003, F 0.039)]  [G loss: -0.039] \n",
      "3196 [D loss: (0.019)(R 0.002, F 0.037)]  [G loss: -0.038] \n",
      "3196 [D loss: (0.021)(R 0.005, F 0.036)]  [G loss: -0.037] \n",
      "3197 [D loss: (0.022)(R 0.003, F 0.040)]  [G loss: -0.038] \n",
      "3197 [D loss: (0.020)(R 0.003, F 0.036)]  [G loss: -0.039] \n",
      "3198 [D loss: (0.018)(R 0.000, F 0.037)]  [G loss: -0.037] \n",
      "3198 [D loss: (0.020)(R 0.002, F 0.037)]  [G loss: -0.035] \n",
      "3199 [D loss: (0.018)(R 0.000, F 0.035)]  [G loss: -0.035] \n",
      "3199 [D loss: (0.020)(R 0.003, F 0.037)]  [G loss: -0.035] \n",
      "3200 [D loss: (0.017)(R 0.001, F 0.034)]  [G loss: -0.035] \n",
      "3200 [D loss: (0.018)(R 0.002, F 0.034)]  [G loss: -0.035] \n",
      "3201 [D loss: (0.018)(R 0.002, F 0.033)]  [G loss: -0.034] \n",
      "3201 [D loss: (0.017)(R 0.000, F 0.033)]  [G loss: -0.033] \n",
      "3202 [D loss: (0.018)(R 0.001, F 0.034)]  [G loss: -0.033] \n",
      "3202 [D loss: (0.019)(R 0.002, F 0.035)]  [G loss: -0.033] \n",
      "3203 [D loss: (0.016)(R 0.001, F 0.031)]  [G loss: -0.033] \n",
      "3203 [D loss: (0.018)(R 0.003, F 0.032)]  [G loss: -0.033] \n",
      "3204 [D loss: (0.017)(R 0.003, F 0.032)]  [G loss: -0.031] \n",
      "3204 [D loss: (0.015)(R -0.000, F 0.031)]  [G loss: -0.032] \n",
      "3205 [D loss: (0.015)(R 0.000, F 0.029)]  [G loss: -0.031] \n",
      "3205 [D loss: (0.017)(R 0.003, F 0.032)]  [G loss: -0.031] \n",
      "3206 [D loss: (0.017)(R 0.004, F 0.030)]  [G loss: -0.031] \n",
      "3206 [D loss: (0.017)(R 0.003, F 0.030)]  [G loss: -0.030] \n",
      "3207 [D loss: (0.014)(R -0.001, F 0.029)]  [G loss: -0.029] \n",
      "3207 [D loss: (0.016)(R 0.002, F 0.029)]  [G loss: -0.028] \n",
      "3208 [D loss: (0.016)(R 0.003, F 0.028)]  [G loss: -0.028] \n",
      "3208 [D loss: (0.014)(R 0.000, F 0.027)]  [G loss: -0.027] \n",
      "3209 [D loss: (0.015)(R 0.004, F 0.026)]  [G loss: -0.027] \n",
      "3209 [D loss: (0.014)(R 0.002, F 0.027)]  [G loss: -0.027] \n",
      "3210 [D loss: (0.016)(R 0.006, F 0.026)]  [G loss: -0.027] \n",
      "3210 [D loss: (0.015)(R 0.003, F 0.027)]  [G loss: -0.027] \n",
      "3211 [D loss: (0.015)(R 0.002, F 0.027)]  [G loss: -0.026] \n",
      "3211 [D loss: (0.015)(R 0.005, F 0.026)]  [G loss: -0.026] \n",
      "3212 [D loss: (0.014)(R 0.004, F 0.024)]  [G loss: -0.025] \n",
      "3212 [D loss: (0.013)(R 0.001, F 0.025)]  [G loss: -0.025] \n",
      "3213 [D loss: (0.013)(R 0.003, F 0.024)]  [G loss: -0.024] \n",
      "3213 [D loss: (0.015)(R 0.006, F 0.025)]  [G loss: -0.024] \n",
      "3214 [D loss: (0.012)(R 0.001, F 0.023)]  [G loss: -0.024] \n",
      "3214 [D loss: (0.014)(R 0.004, F 0.024)]  [G loss: -0.024] \n",
      "3215 [D loss: (0.011)(R 0.002, F 0.021)]  [G loss: -0.024] \n",
      "3215 [D loss: (0.013)(R 0.004, F 0.023)]  [G loss: -0.023] \n",
      "3216 [D loss: (0.013)(R 0.003, F 0.023)]  [G loss: -0.022] \n",
      "3216 [D loss: (0.013)(R 0.003, F 0.023)]  [G loss: -0.022] \n",
      "3217 [D loss: (0.012)(R 0.001, F 0.023)]  [G loss: -0.020] \n",
      "3217 [D loss: (0.013)(R 0.005, F 0.021)]  [G loss: -0.022] \n",
      "3218 [D loss: (0.013)(R 0.004, F 0.021)]  [G loss: -0.020] \n",
      "3218 [D loss: (0.010)(R 0.001, F 0.020)]  [G loss: -0.020] \n",
      "3219 [D loss: (0.010)(R -0.000, F 0.020)]  [G loss: -0.020] \n",
      "3219 [D loss: (0.013)(R 0.005, F 0.020)]  [G loss: -0.019] \n",
      "3220 [D loss: (0.011)(R 0.003, F 0.020)]  [G loss: -0.018] \n",
      "3220 [D loss: (0.011)(R 0.004, F 0.018)]  [G loss: -0.018] \n",
      "3221 [D loss: (0.010)(R 0.002, F 0.017)]  [G loss: -0.019] \n",
      "3221 [D loss: (0.011)(R 0.003, F 0.018)]  [G loss: -0.017] \n",
      "3222 [D loss: (0.011)(R 0.003, F 0.018)]  [G loss: -0.018] \n",
      "3222 [D loss: (0.010)(R 0.003, F 0.016)]  [G loss: -0.017] \n",
      "3223 [D loss: (0.010)(R 0.003, F 0.017)]  [G loss: -0.016] \n",
      "3223 [D loss: (0.011)(R 0.005, F 0.017)]  [G loss: -0.016] \n",
      "3224 [D loss: (0.010)(R 0.004, F 0.015)]  [G loss: -0.016] \n",
      "3224 [D loss: (0.009)(R 0.003, F 0.016)]  [G loss: -0.015] \n",
      "3225 [D loss: (0.011)(R 0.007, F 0.016)]  [G loss: -0.016] \n",
      "3225 [D loss: (0.010)(R 0.005, F 0.015)]  [G loss: -0.014] \n",
      "3226 [D loss: (0.009)(R 0.002, F 0.015)]  [G loss: -0.014] \n",
      "3226 [D loss: (0.011)(R 0.006, F 0.015)]  [G loss: -0.014] \n",
      "3227 [D loss: (0.009)(R 0.004, F 0.015)]  [G loss: -0.014] \n",
      "3227 [D loss: (0.009)(R 0.004, F 0.013)]  [G loss: -0.014] \n",
      "3228 [D loss: (0.008)(R 0.003, F 0.013)]  [G loss: -0.013] \n",
      "3228 [D loss: (0.009)(R 0.005, F 0.013)]  [G loss: -0.013] \n",
      "3229 [D loss: (0.009)(R 0.006, F 0.013)]  [G loss: -0.012] \n",
      "3229 [D loss: (0.009)(R 0.005, F 0.012)]  [G loss: -0.012] \n",
      "3230 [D loss: (0.008)(R 0.004, F 0.011)]  [G loss: -0.012] \n",
      "3230 [D loss: (0.009)(R 0.006, F 0.011)]  [G loss: -0.011] \n",
      "3231 [D loss: (0.007)(R 0.005, F 0.010)]  [G loss: -0.011] \n",
      "3231 [D loss: (0.008)(R 0.006, F 0.010)]  [G loss: -0.010] \n",
      "3232 [D loss: (0.008)(R 0.006, F 0.011)]  [G loss: -0.011] \n",
      "3232 [D loss: (0.007)(R 0.005, F 0.010)]  [G loss: -0.009] \n",
      "3233 [D loss: (0.006)(R 0.004, F 0.009)]  [G loss: -0.009] \n",
      "3233 [D loss: (0.008)(R 0.007, F 0.009)]  [G loss: -0.009] \n",
      "3234 [D loss: (0.007)(R 0.004, F 0.009)]  [G loss: -0.009] \n",
      "3234 [D loss: (0.007)(R 0.005, F 0.009)]  [G loss: -0.008] \n",
      "3235 [D loss: (0.006)(R 0.006, F 0.007)]  [G loss: -0.008] \n",
      "3235 [D loss: (0.006)(R 0.006, F 0.007)]  [G loss: -0.007] \n",
      "3236 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.006] \n",
      "3236 [D loss: (0.006)(R 0.006, F 0.006)]  [G loss: -0.006] \n",
      "3237 [D loss: (0.006)(R 0.007, F 0.006)]  [G loss: -0.006] \n",
      "3237 [D loss: (0.006)(R 0.006, F 0.005)]  [G loss: -0.006] \n",
      "3238 [D loss: (0.004)(R 0.004, F 0.005)]  [G loss: -0.005] \n",
      "3238 [D loss: (0.005)(R 0.006, F 0.005)]  [G loss: -0.004] \n",
      "3239 [D loss: (0.004)(R 0.005, F 0.003)]  [G loss: -0.005] \n",
      "3239 [D loss: (0.006)(R 0.008, F 0.003)]  [G loss: -0.004] \n",
      "3240 [D loss: (0.005)(R 0.007, F 0.003)]  [G loss: -0.003] \n",
      "3240 [D loss: (0.005)(R 0.007, F 0.003)]  [G loss: -0.003] \n",
      "3241 [D loss: (0.005)(R 0.007, F 0.003)]  [G loss: -0.003] \n",
      "3241 [D loss: (0.004)(R 0.006, F 0.002)]  [G loss: -0.002] \n",
      "3242 [D loss: (0.004)(R 0.006, F 0.001)]  [G loss: -0.002] \n",
      "3242 [D loss: (0.003)(R 0.006, F 0.000)]  [G loss: -0.001] \n",
      "3243 [D loss: (0.004)(R 0.007, F 0.001)]  [G loss: -0.001] \n",
      "3243 [D loss: (0.003)(R 0.007, F -0.001)]  [G loss: 0.000] \n",
      "3244 [D loss: (0.004)(R 0.007, F 0.000)]  [G loss: 0.000] \n",
      "3244 [D loss: (0.003)(R 0.008, F -0.001)]  [G loss: 0.001] \n",
      "3245 [D loss: (0.003)(R 0.007, F -0.002)]  [G loss: 0.001] \n",
      "3245 [D loss: (0.004)(R 0.009, F -0.002)]  [G loss: 0.002] \n",
      "3246 [D loss: (0.003)(R 0.008, F -0.003)]  [G loss: 0.003] \n",
      "3246 [D loss: (0.003)(R 0.008, F -0.002)]  [G loss: 0.003] \n",
      "3247 [D loss: (0.003)(R 0.008, F -0.003)]  [G loss: 0.004] \n",
      "3247 [D loss: (0.002)(R 0.009, F -0.004)]  [G loss: 0.005] \n",
      "3248 [D loss: (0.002)(R 0.010, F -0.005)]  [G loss: 0.005] \n",
      "3248 [D loss: (0.003)(R 0.010, F -0.005)]  [G loss: 0.006] \n",
      "3249 [D loss: (0.002)(R 0.010, F -0.006)]  [G loss: 0.006] \n",
      "3249 [D loss: (0.001)(R 0.009, F -0.007)]  [G loss: 0.007] \n",
      "3250 [D loss: (0.001)(R 0.009, F -0.007)]  [G loss: 0.007] \n",
      "3250 [D loss: (0.001)(R 0.009, F -0.008)]  [G loss: 0.008] \n",
      "3251 [D loss: (-0.000)(R 0.008, F -0.009)]  [G loss: 0.008] \n",
      "3251 [D loss: (0.000)(R 0.010, F -0.010)]  [G loss: 0.009] \n",
      "3252 [D loss: (0.000)(R 0.010, F -0.009)]  [G loss: 0.011] \n",
      "3252 [D loss: (-0.000)(R 0.011, F -0.011)]  [G loss: 0.010] \n",
      "3253 [D loss: (-0.001)(R 0.011, F -0.012)]  [G loss: 0.011] \n",
      "3253 [D loss: (-0.001)(R 0.010, F -0.012)]  [G loss: 0.012] \n",
      "3254 [D loss: (-0.001)(R 0.011, F -0.013)]  [G loss: 0.013] \n",
      "3254 [D loss: (-0.001)(R 0.011, F -0.013)]  [G loss: 0.013] \n",
      "3255 [D loss: (-0.001)(R 0.011, F -0.014)]  [G loss: 0.014] \n",
      "3255 [D loss: (-0.002)(R 0.011, F -0.015)]  [G loss: 0.014] \n",
      "3256 [D loss: (-0.003)(R 0.010, F -0.015)]  [G loss: 0.015] \n",
      "3256 [D loss: (-0.003)(R 0.011, F -0.016)]  [G loss: 0.015] \n",
      "3257 [D loss: (-0.002)(R 0.012, F -0.017)]  [G loss: 0.017] \n",
      "3257 [D loss: (-0.003)(R 0.011, F -0.017)]  [G loss: 0.017] \n",
      "3258 [D loss: (-0.003)(R 0.013, F -0.018)]  [G loss: 0.017] \n",
      "3258 [D loss: (-0.004)(R 0.011, F -0.019)]  [G loss: 0.018] \n",
      "3259 [D loss: (-0.003)(R 0.013, F -0.019)]  [G loss: 0.020] \n",
      "3259 [D loss: (-0.003)(R 0.013, F -0.020)]  [G loss: 0.020] \n",
      "3260 [D loss: (-0.003)(R 0.014, F -0.021)]  [G loss: 0.021] \n",
      "3260 [D loss: (-0.005)(R 0.012, F -0.022)]  [G loss: 0.021] \n",
      "3261 [D loss: (-0.004)(R 0.014, F -0.023)]  [G loss: 0.023] \n",
      "3261 [D loss: (-0.005)(R 0.014, F -0.023)]  [G loss: 0.023] \n",
      "3262 [D loss: (-0.005)(R 0.014, F -0.023)]  [G loss: 0.024] \n",
      "3262 [D loss: (-0.006)(R 0.014, F -0.025)]  [G loss: 0.025] \n",
      "3263 [D loss: (-0.006)(R 0.014, F -0.026)]  [G loss: 0.026] \n",
      "3263 [D loss: (-0.006)(R 0.015, F -0.027)]  [G loss: 0.026] \n",
      "3264 [D loss: (-0.007)(R 0.013, F -0.028)]  [G loss: 0.027] \n",
      "3264 [D loss: (-0.007)(R 0.014, F -0.028)]  [G loss: 0.028] \n",
      "3265 [D loss: (-0.007)(R 0.016, F -0.029)]  [G loss: 0.028] \n",
      "3265 [D loss: (-0.007)(R 0.015, F -0.030)]  [G loss: 0.030] \n",
      "3266 [D loss: (-0.008)(R 0.015, F -0.031)]  [G loss: 0.031] \n",
      "3266 [D loss: (-0.008)(R 0.016, F -0.032)]  [G loss: 0.032] \n",
      "3267 [D loss: (-0.008)(R 0.016, F -0.033)]  [G loss: 0.033] \n",
      "3267 [D loss: (-0.009)(R 0.016, F -0.034)]  [G loss: 0.034] \n",
      "3268 [D loss: (-0.010)(R 0.016, F -0.036)]  [G loss: 0.035] \n",
      "3268 [D loss: (-0.010)(R 0.017, F -0.037)]  [G loss: 0.035] \n",
      "3269 [D loss: (-0.010)(R 0.016, F -0.037)]  [G loss: 0.036] \n",
      "3269 [D loss: (-0.010)(R 0.018, F -0.038)]  [G loss: 0.038] \n",
      "3270 [D loss: (-0.009)(R 0.020, F -0.038)]  [G loss: 0.039] \n",
      "3270 [D loss: (-0.011)(R 0.018, F -0.040)]  [G loss: 0.039] \n",
      "3271 [D loss: (-0.011)(R 0.019, F -0.040)]  [G loss: 0.040] \n",
      "3271 [D loss: (-0.012)(R 0.019, F -0.042)]  [G loss: 0.042] \n",
      "3272 [D loss: (-0.011)(R 0.019, F -0.042)]  [G loss: 0.043] \n",
      "3272 [D loss: (-0.012)(R 0.020, F -0.043)]  [G loss: 0.044] \n",
      "3273 [D loss: (-0.012)(R 0.020, F -0.044)]  [G loss: 0.045] \n",
      "3273 [D loss: (-0.012)(R 0.021, F -0.045)]  [G loss: 0.047] \n",
      "3274 [D loss: (-0.012)(R 0.022, F -0.045)]  [G loss: 0.047] \n",
      "3274 [D loss: (-0.014)(R 0.020, F -0.049)]  [G loss: 0.049] \n",
      "3275 [D loss: (-0.015)(R 0.020, F -0.051)]  [G loss: 0.050] \n",
      "3275 [D loss: (-0.014)(R 0.022, F -0.051)]  [G loss: 0.052] \n",
      "3276 [D loss: (-0.015)(R 0.022, F -0.051)]  [G loss: 0.052] \n",
      "3276 [D loss: (-0.016)(R 0.020, F -0.053)]  [G loss: 0.054] \n",
      "3277 [D loss: (-0.017)(R 0.021, F -0.055)]  [G loss: 0.055] \n",
      "3277 [D loss: (-0.017)(R 0.024, F -0.058)]  [G loss: 0.057] \n",
      "3278 [D loss: (-0.017)(R 0.024, F -0.058)]  [G loss: 0.059] \n",
      "3278 [D loss: (-0.018)(R 0.023, F -0.059)]  [G loss: 0.059] \n",
      "3279 [D loss: (-0.018)(R 0.025, F -0.062)]  [G loss: 0.061] \n",
      "3279 [D loss: (-0.020)(R 0.022, F -0.061)]  [G loss: 0.062] \n",
      "3280 [D loss: (-0.020)(R 0.025, F -0.064)]  [G loss: 0.064] \n",
      "3280 [D loss: (-0.020)(R 0.025, F -0.065)]  [G loss: 0.065] \n",
      "3281 [D loss: (-0.020)(R 0.026, F -0.067)]  [G loss: 0.067] \n",
      "3281 [D loss: (-0.022)(R 0.026, F -0.069)]  [G loss: 0.069] \n",
      "3282 [D loss: (-0.021)(R 0.026, F -0.068)]  [G loss: 0.070] \n",
      "3282 [D loss: (-0.022)(R 0.027, F -0.072)]  [G loss: 0.073] \n",
      "3283 [D loss: (-0.023)(R 0.029, F -0.074)]  [G loss: 0.074] \n",
      "3283 [D loss: (-0.022)(R 0.031, F -0.074)]  [G loss: 0.076] \n",
      "3284 [D loss: (-0.025)(R 0.027, F -0.076)]  [G loss: 0.077] \n",
      "3284 [D loss: (-0.025)(R 0.028, F -0.079)]  [G loss: 0.079] \n",
      "3285 [D loss: (-0.026)(R 0.030, F -0.083)]  [G loss: 0.082] \n",
      "3285 [D loss: (-0.028)(R 0.027, F -0.082)]  [G loss: 0.083] \n",
      "3286 [D loss: (-0.026)(R 0.033, F -0.085)]  [G loss: 0.085] \n",
      "3286 [D loss: (-0.027)(R 0.031, F -0.086)]  [G loss: 0.087] \n",
      "3287 [D loss: (-0.027)(R 0.032, F -0.086)]  [G loss: 0.088] \n",
      "3287 [D loss: (-0.029)(R 0.032, F -0.091)]  [G loss: 0.091] \n",
      "3288 [D loss: (-0.030)(R 0.031, F -0.091)]  [G loss: 0.094] \n",
      "3288 [D loss: (-0.031)(R 0.031, F -0.094)]  [G loss: 0.096] \n",
      "3289 [D loss: (-0.032)(R 0.033, F -0.096)]  [G loss: 0.099] \n",
      "3289 [D loss: (-0.033)(R 0.033, F -0.100)]  [G loss: 0.099] \n",
      "3290 [D loss: (-0.033)(R 0.036, F -0.101)]  [G loss: 0.103] \n",
      "3290 [D loss: (-0.036)(R 0.034, F -0.105)]  [G loss: 0.105] \n",
      "3291 [D loss: (-0.035)(R 0.036, F -0.107)]  [G loss: 0.108] \n",
      "3291 [D loss: (-0.040)(R 0.033, F -0.113)]  [G loss: 0.111] \n",
      "3292 [D loss: (-0.037)(R 0.037, F -0.111)]  [G loss: 0.113] \n",
      "3292 [D loss: (-0.040)(R 0.038, F -0.117)]  [G loss: 0.118] \n",
      "3293 [D loss: (-0.040)(R 0.041, F -0.120)]  [G loss: 0.120] \n",
      "3293 [D loss: (-0.041)(R 0.040, F -0.122)]  [G loss: 0.123] \n",
      "3294 [D loss: (-0.044)(R 0.036, F -0.125)]  [G loss: 0.126] \n",
      "3294 [D loss: (-0.042)(R 0.044, F -0.127)]  [G loss: 0.129] \n",
      "3295 [D loss: (-0.044)(R 0.045, F -0.134)]  [G loss: 0.133] \n",
      "3295 [D loss: (-0.048)(R 0.043, F -0.138)]  [G loss: 0.135] \n",
      "3296 [D loss: (-0.048)(R 0.044, F -0.140)]  [G loss: 0.142] \n",
      "3296 [D loss: (-0.047)(R 0.046, F -0.140)]  [G loss: 0.145] \n",
      "3297 [D loss: (-0.052)(R 0.044, F -0.147)]  [G loss: 0.148] \n",
      "3297 [D loss: (-0.051)(R 0.047, F -0.149)]  [G loss: 0.152] \n",
      "3298 [D loss: (-0.056)(R 0.040, F -0.152)]  [G loss: 0.155] \n",
      "3298 [D loss: (-0.056)(R 0.048, F -0.160)]  [G loss: 0.162] \n",
      "3299 [D loss: (-0.056)(R 0.047, F -0.159)]  [G loss: 0.164] \n",
      "3299 [D loss: (-0.061)(R 0.045, F -0.167)]  [G loss: 0.169] \n",
      "3300 [D loss: (-0.062)(R 0.050, F -0.173)]  [G loss: 0.176] \n",
      "3300 [D loss: (-0.062)(R 0.051, F -0.175)]  [G loss: 0.180] \n",
      "3301 [D loss: (-0.071)(R 0.044, F -0.187)]  [G loss: 0.185] \n",
      "3301 [D loss: (-0.071)(R 0.046, F -0.188)]  [G loss: 0.188] \n",
      "3302 [D loss: (-0.067)(R 0.053, F -0.186)]  [G loss: 0.196] \n",
      "3302 [D loss: (-0.071)(R 0.055, F -0.198)]  [G loss: 0.201] \n",
      "3303 [D loss: (-0.077)(R 0.053, F -0.207)]  [G loss: 0.208] \n",
      "3303 [D loss: (-0.075)(R 0.059, F -0.209)]  [G loss: 0.213] \n",
      "3304 [D loss: (-0.079)(R 0.058, F -0.216)]  [G loss: 0.222] \n",
      "3304 [D loss: (-0.085)(R 0.057, F -0.227)]  [G loss: 0.228] \n",
      "3305 [D loss: (-0.090)(R 0.054, F -0.233)]  [G loss: 0.235] \n",
      "3305 [D loss: (-0.089)(R 0.059, F -0.237)]  [G loss: 0.240] \n",
      "3306 [D loss: (-0.097)(R 0.060, F -0.253)]  [G loss: 0.250] \n",
      "3306 [D loss: (-0.100)(R 0.060, F -0.260)]  [G loss: 0.261] \n",
      "3307 [D loss: (-0.109)(R 0.046, F -0.265)]  [G loss: 0.268] \n",
      "3307 [D loss: (-0.109)(R 0.057, F -0.275)]  [G loss: 0.277] \n",
      "3308 [D loss: (-0.115)(R 0.050, F -0.279)]  [G loss: 0.285] \n",
      "3308 [D loss: (-0.117)(R 0.057, F -0.292)]  [G loss: 0.300] \n",
      "3309 [D loss: (-0.131)(R 0.046, F -0.307)]  [G loss: 0.310] \n",
      "3309 [D loss: (-0.126)(R 0.051, F -0.303)]  [G loss: 0.316] \n",
      "3310 [D loss: (-0.132)(R 0.073, F -0.338)]  [G loss: 0.330] \n",
      "3310 [D loss: (-0.134)(R 0.056, F -0.324)]  [G loss: 0.344] \n",
      "3311 [D loss: (-0.149)(R 0.050, F -0.348)]  [G loss: 0.358] \n",
      "3311 [D loss: (-0.163)(R 0.044, F -0.370)]  [G loss: 0.373] \n",
      "3312 [D loss: (-0.174)(R 0.044, F -0.392)]  [G loss: 0.378] \n",
      "3312 [D loss: (-0.171)(R 0.052, F -0.394)]  [G loss: 0.405] \n",
      "3313 [D loss: (-0.180)(R 0.056, F -0.417)]  [G loss: 0.417] \n",
      "3313 [D loss: (-0.192)(R 0.045, F -0.429)]  [G loss: 0.436] \n",
      "3314 [D loss: (-0.212)(R 0.026, F -0.449)]  [G loss: 0.450] \n",
      "3314 [D loss: (-0.251)(R 0.002, F -0.504)]  [G loss: 0.469] \n",
      "3315 [D loss: (-0.256)(R -0.010, F -0.502)]  [G loss: 0.493] \n",
      "3315 [D loss: (-0.252)(R 0.020, F -0.525)]  [G loss: 0.524] \n",
      "3316 [D loss: (-0.265)(R 0.000, F -0.529)]  [G loss: 0.544] \n",
      "3316 [D loss: (-0.263)(R 0.018, F -0.543)]  [G loss: 0.583] \n",
      "3317 [D loss: (-0.325)(R -0.049, F -0.600)]  [G loss: 0.604] \n",
      "3317 [D loss: (-0.334)(R -0.033, F -0.635)]  [G loss: 0.635] \n",
      "3318 [D loss: (-0.354)(R -0.046, F -0.663)]  [G loss: 0.669] \n",
      "3318 [D loss: (-0.363)(R -0.047, F -0.679)]  [G loss: 0.694] \n",
      "3319 [D loss: (-0.430)(R -0.128, F -0.733)]  [G loss: 0.746] \n",
      "3319 [D loss: (-0.499)(R -0.203, F -0.794)]  [G loss: 0.791] \n",
      "3320 [D loss: (-0.539)(R -0.226, F -0.852)]  [G loss: 0.839] \n",
      "3320 [D loss: (-0.606)(R -0.335, F -0.876)]  [G loss: 0.907] \n",
      "3321 [D loss: (-0.658)(R -0.358, F -0.957)]  [G loss: 0.943] \n",
      "3321 [D loss: (-0.705)(R -0.349, F -1.060)]  [G loss: 1.024] \n",
      "3322 [D loss: (-0.869)(R -0.676, F -1.062)]  [G loss: 1.094] \n",
      "3322 [D loss: (-0.916)(R -0.672, F -1.159)]  [G loss: 1.170] \n",
      "3323 [D loss: (-1.042)(R -0.810, F -1.274)]  [G loss: 1.206] \n",
      "3323 [D loss: (-1.428)(R -1.582, F -1.275)]  [G loss: 1.286] \n",
      "3324 [D loss: (-1.832)(R -2.272, F -1.393)]  [G loss: 1.326] \n",
      "3324 [D loss: (-2.434)(R -3.377, F -1.491)]  [G loss: 1.245] \n",
      "3325 [D loss: (-2.698)(R -4.379, F -1.017)]  [G loss: 1.133] \n",
      "3325 [D loss: (-3.573)(R -6.403, F -0.743)]  [G loss: 1.002] \n",
      "3326 [D loss: (-4.051)(R -8.155, F 0.053)]  [G loss: 0.461] \n",
      "3326 [D loss: (-4.998)(R -9.962, F -0.034)]  [G loss: -0.357] \n",
      "3327 [D loss: (-6.044)(R -12.074, F -0.014)]  [G loss: -0.874] \n",
      "3327 [D loss: (-7.389)(R -15.671, F 0.893)]  [G loss: -2.220] \n",
      "3328 [D loss: (-7.559)(R -17.856, F 2.737)]  [G loss: -2.627] \n",
      "3328 [D loss: (-8.229)(R -20.760, F 4.303)]  [G loss: -4.508] \n",
      "3329 [D loss: (-9.090)(R -23.115, F 4.935)]  [G loss: -5.480] \n",
      "3329 [D loss: (-9.093)(R -25.538, F 7.351)]  [G loss: -6.683] \n",
      "3330 [D loss: (-10.409)(R -29.606, F 8.788)]  [G loss: -7.566] \n",
      "3330 [D loss: (-10.516)(R -30.132, F 9.100)]  [G loss: -10.194] \n",
      "3331 [D loss: (-11.601)(R -31.717, F 8.515)]  [G loss: -9.278] \n",
      "3331 [D loss: (-12.277)(R -35.920, F 11.366)]  [G loss: -10.887] \n",
      "3332 [D loss: (-12.023)(R -36.851, F 12.805)]  [G loss: -13.057] \n",
      "3332 [D loss: (-14.088)(R -41.246, F 13.069)]  [G loss: -13.557] \n",
      "3333 [D loss: (-14.016)(R -41.631, F 13.599)]  [G loss: -14.399] \n",
      "3333 [D loss: (-15.124)(R -45.949, F 15.701)]  [G loss: -15.276] \n",
      "3334 [D loss: (-14.569)(R -47.442, F 18.305)]  [G loss: -16.964] \n",
      "3334 [D loss: (-14.932)(R -48.702, F 18.838)]  [G loss: -18.592] \n",
      "3335 [D loss: (-13.536)(R -49.841, F 22.769)]  [G loss: -17.840] \n",
      "3335 [D loss: (-14.222)(R -48.891, F 20.447)]  [G loss: -19.310] \n",
      "3336 [D loss: (-16.356)(R -52.411, F 19.698)]  [G loss: -21.345] \n",
      "3336 [D loss: (-14.249)(R -51.668, F 23.169)]  [G loss: -21.225] \n",
      "3337 [D loss: (-13.647)(R -54.655, F 27.362)]  [G loss: -20.781] \n",
      "3337 [D loss: (-18.782)(R -57.647, F 20.082)]  [G loss: -22.132] \n",
      "3338 [D loss: (-16.112)(R -59.836, F 27.611)]  [G loss: -24.395] \n",
      "3338 [D loss: (-17.448)(R -57.461, F 22.565)]  [G loss: -23.899] \n",
      "3339 [D loss: (-18.543)(R -59.961, F 22.875)]  [G loss: -24.417] \n",
      "3339 [D loss: (-15.882)(R -61.773, F 30.009)]  [G loss: -25.977] \n",
      "3340 [D loss: (-18.422)(R -61.892, F 25.048)]  [G loss: -26.273] \n",
      "3340 [D loss: (-20.972)(R -64.491, F 22.546)]  [G loss: -27.382] \n",
      "3341 [D loss: (-16.688)(R -63.588, F 30.212)]  [G loss: -28.471] \n",
      "3341 [D loss: (-16.245)(R -59.711, F 27.222)]  [G loss: -26.033] \n",
      "3342 [D loss: (-18.701)(R -65.314, F 27.913)]  [G loss: -27.357] \n",
      "3342 [D loss: (-16.233)(R -66.174, F 33.708)]  [G loss: -31.309] \n",
      "3343 [D loss: (-15.677)(R -65.904, F 34.550)]  [G loss: -28.959] \n",
      "3343 [D loss: (-18.934)(R -69.267, F 31.399)]  [G loss: -29.828] \n",
      "3344 [D loss: (-22.633)(R -71.685, F 26.419)]  [G loss: -31.665] \n",
      "3344 [D loss: (-20.300)(R -70.954, F 30.354)]  [G loss: -32.298] \n",
      "3345 [D loss: (-21.763)(R -73.688, F 30.163)]  [G loss: -31.652] \n",
      "3345 [D loss: (-17.770)(R -71.514, F 35.973)]  [G loss: -34.956] \n",
      "3346 [D loss: (-20.461)(R -75.592, F 34.669)]  [G loss: -35.189] \n",
      "3346 [D loss: (-20.230)(R -74.845, F 34.385)]  [G loss: -35.009] \n",
      "3347 [D loss: (-24.599)(R -77.335, F 28.137)]  [G loss: -35.526] \n",
      "3347 [D loss: (-20.739)(R -75.268, F 33.790)]  [G loss: -36.760] \n",
      "3348 [D loss: (-20.576)(R -78.212, F 37.060)]  [G loss: -36.945] \n",
      "3348 [D loss: (-19.103)(R -76.623, F 38.417)]  [G loss: -36.930] \n",
      "3349 [D loss: (-17.385)(R -79.614, F 44.843)]  [G loss: -39.046] \n",
      "3349 [D loss: (-19.180)(R -82.926, F 44.565)]  [G loss: -39.524] \n",
      "3350 [D loss: (-20.174)(R -80.635, F 40.288)]  [G loss: -39.911] \n",
      "3350 [D loss: (-20.722)(R -76.658, F 35.214)]  [G loss: -39.533] \n",
      "3351 [D loss: (-19.926)(R -81.732, F 41.880)]  [G loss: -40.254] \n",
      "3351 [D loss: (-21.890)(R -84.079, F 40.298)]  [G loss: -42.460] \n",
      "3352 [D loss: (-20.238)(R -82.290, F 41.813)]  [G loss: -41.430] \n",
      "3352 [D loss: (-17.036)(R -81.455, F 47.383)]  [G loss: -44.127] \n",
      "3353 [D loss: (-17.665)(R -85.722, F 50.393)]  [G loss: -45.233] \n",
      "3353 [D loss: (-21.326)(R -83.213, F 40.560)]  [G loss: -42.867] \n",
      "3354 [D loss: (-21.048)(R -87.312, F 45.216)]  [G loss: -45.660] \n",
      "3354 [D loss: (-17.625)(R -86.598, F 51.348)]  [G loss: -44.002] \n",
      "3355 [D loss: (-14.417)(R -85.888, F 57.054)]  [G loss: -46.559] \n",
      "3355 [D loss: (-21.921)(R -84.389, F 40.547)]  [G loss: -47.607] \n",
      "3356 [D loss: (-21.082)(R -89.592, F 47.428)]  [G loss: -48.502] \n",
      "3356 [D loss: (-19.529)(R -89.014, F 49.956)]  [G loss: -48.906] \n",
      "3357 [D loss: (-22.595)(R -92.758, F 47.567)]  [G loss: -48.830] \n",
      "3357 [D loss: (-20.783)(R -89.160, F 47.594)]  [G loss: -50.493] \n",
      "3358 [D loss: (-21.737)(R -94.309, F 50.835)]  [G loss: -51.315] \n",
      "3358 [D loss: (-23.388)(R -94.512, F 47.735)]  [G loss: -49.606] \n",
      "3359 [D loss: (-23.174)(R -92.412, F 46.065)]  [G loss: -51.076] \n",
      "3359 [D loss: (-15.757)(R -94.715, F 63.202)]  [G loss: -54.161] \n",
      "3360 [D loss: (-24.417)(R -95.707, F 46.874)]  [G loss: -55.405] \n",
      "3360 [D loss: (-23.530)(R -96.457, F 49.397)]  [G loss: -55.095] \n",
      "3361 [D loss: (-20.316)(R -92.152, F 51.521)]  [G loss: -54.682] \n",
      "3361 [D loss: (-21.225)(R -94.516, F 52.065)]  [G loss: -57.219] \n",
      "3362 [D loss: (-20.541)(R -97.414, F 56.331)]  [G loss: -56.991] \n",
      "3362 [D loss: (-18.467)(R -95.408, F 58.475)]  [G loss: -59.317] \n",
      "3363 [D loss: (-17.540)(R -96.195, F 61.115)]  [G loss: -56.790] \n",
      "3363 [D loss: (-13.425)(R -91.772, F 64.922)]  [G loss: -59.090] \n",
      "3364 [D loss: (-17.644)(R -96.110, F 60.822)]  [G loss: -60.762] \n",
      "3364 [D loss: (-18.055)(R -95.642, F 59.531)]  [G loss: -59.552] \n",
      "3365 [D loss: (-19.292)(R -90.805, F 52.221)]  [G loss: -59.923] \n",
      "3365 [D loss: (-10.014)(R -92.403, F 72.376)]  [G loss: -60.923] \n",
      "3366 [D loss: (-19.477)(R -99.100, F 60.147)]  [G loss: -62.007] \n",
      "3366 [D loss: (-15.416)(R -94.886, F 64.053)]  [G loss: -60.703] \n",
      "3367 [D loss: (-21.530)(R -99.880, F 56.821)]  [G loss: -65.935] \n",
      "3367 [D loss: (-21.086)(R -100.157, F 57.985)]  [G loss: -63.773] \n",
      "3368 [D loss: (-17.614)(R -101.402, F 66.174)]  [G loss: -65.540] \n",
      "3368 [D loss: (-14.527)(R -99.183, F 70.128)]  [G loss: -64.210] \n",
      "3369 [D loss: (-14.560)(R -100.333, F 71.213)]  [G loss: -64.710] \n",
      "3369 [D loss: (-14.040)(R -100.925, F 72.844)]  [G loss: -69.156] \n",
      "3370 [D loss: (-22.129)(R -102.852, F 58.594)]  [G loss: -70.717] \n",
      "3370 [D loss: (-19.980)(R -103.296, F 63.336)]  [G loss: -68.112] \n",
      "3371 [D loss: (-18.564)(R -103.962, F 66.835)]  [G loss: -71.707] \n",
      "3371 [D loss: (-15.491)(R -102.582, F 71.599)]  [G loss: -70.027] \n",
      "3372 [D loss: (-16.206)(R -108.665, F 76.253)]  [G loss: -72.965] \n",
      "3372 [D loss: (-15.980)(R -106.425, F 74.465)]  [G loss: -72.569] \n",
      "3373 [D loss: (-17.503)(R -105.907, F 70.901)]  [G loss: -75.703] \n",
      "3373 [D loss: (-14.306)(R -100.018, F 71.406)]  [G loss: -72.662] \n",
      "3374 [D loss: (-14.756)(R -107.533, F 78.021)]  [G loss: -74.441] \n",
      "3374 [D loss: (-15.297)(R -108.093, F 77.500)]  [G loss: -75.732] \n",
      "3375 [D loss: (-18.112)(R -108.854, F 72.630)]  [G loss: -78.100] \n",
      "3375 [D loss: (-19.143)(R -107.039, F 68.753)]  [G loss: -72.648] \n",
      "3376 [D loss: (-17.064)(R -104.175, F 70.046)]  [G loss: -77.716] \n",
      "3376 [D loss: (-14.192)(R -105.998, F 77.613)]  [G loss: -77.581] \n",
      "3377 [D loss: (-16.233)(R -106.659, F 74.192)]  [G loss: -80.069] \n",
      "3377 [D loss: (-11.958)(R -103.484, F 79.567)]  [G loss: -80.727] \n",
      "3378 [D loss: (-17.160)(R -107.118, F 72.797)]  [G loss: -80.886] \n",
      "3378 [D loss: (-10.190)(R -109.647, F 89.267)]  [G loss: -83.618] \n",
      "3379 [D loss: (-11.392)(R -104.210, F 81.427)]  [G loss: -77.718] \n",
      "3379 [D loss: (-11.830)(R -108.094, F 84.434)]  [G loss: -81.231] \n",
      "3380 [D loss: (-16.009)(R -103.685, F 71.666)]  [G loss: -82.802] \n",
      "3380 [D loss: (-14.967)(R -110.648, F 80.715)]  [G loss: -84.315] \n",
      "3381 [D loss: (-8.825)(R -108.816, F 91.166)]  [G loss: -83.768] \n",
      "3381 [D loss: (-8.589)(R -106.770, F 89.591)]  [G loss: -78.417] \n",
      "3382 [D loss: (-12.356)(R -110.736, F 86.024)]  [G loss: -84.070] \n",
      "3382 [D loss: (-12.358)(R -106.739, F 82.022)]  [G loss: -89.689] \n",
      "3383 [D loss: (-10.478)(R -112.558, F 91.602)]  [G loss: -86.218] \n",
      "3383 [D loss: (-10.983)(R -106.164, F 84.197)]  [G loss: -87.870] \n",
      "3384 [D loss: (-15.806)(R -115.594, F 83.982)]  [G loss: -86.963] \n",
      "3384 [D loss: (-18.895)(R -114.222, F 76.432)]  [G loss: -88.643] \n",
      "3385 [D loss: (-13.108)(R -112.602, F 86.386)]  [G loss: -87.843] \n",
      "3385 [D loss: (-13.658)(R -112.717, F 85.400)]  [G loss: -92.854] \n",
      "3386 [D loss: (-17.184)(R -116.506, F 82.138)]  [G loss: -90.858] \n",
      "3386 [D loss: (-13.779)(R -108.244, F 80.686)]  [G loss: -87.621] \n",
      "3387 [D loss: (-5.673)(R -112.248, F 100.902)]  [G loss: -87.000] \n",
      "3387 [D loss: (-12.437)(R -117.659, F 92.785)]  [G loss: -92.273] \n",
      "3388 [D loss: (-10.260)(R -116.561, F 96.041)]  [G loss: -96.484] \n",
      "3388 [D loss: (-11.421)(R -114.342, F 91.499)]  [G loss: -95.036] \n",
      "3389 [D loss: (-11.891)(R -112.809, F 89.026)]  [G loss: -93.581] \n",
      "3389 [D loss: (-9.117)(R -112.594, F 94.360)]  [G loss: -97.164] \n",
      "3390 [D loss: (-16.966)(R -116.705, F 82.772)]  [G loss: -95.855] \n",
      "3390 [D loss: (-13.444)(R -115.046, F 88.157)]  [G loss: -93.231] \n",
      "3391 [D loss: (-5.984)(R -112.289, F 100.322)]  [G loss: -95.984] \n",
      "3391 [D loss: (-11.057)(R -113.496, F 91.382)]  [G loss: -100.555] \n",
      "3392 [D loss: (-8.961)(R -117.975, F 100.053)]  [G loss: -96.972] \n",
      "3392 [D loss: (-5.714)(R -110.859, F 99.432)]  [G loss: -101.936] \n",
      "3393 [D loss: (-12.408)(R -117.582, F 92.767)]  [G loss: -99.685] \n",
      "3393 [D loss: (2.518)(R -112.979, F 118.016)]  [G loss: -101.346] \n",
      "3394 [D loss: (-2.832)(R -118.448, F 112.784)]  [G loss: -98.607] \n",
      "3394 [D loss: (-4.751)(R -114.472, F 104.970)]  [G loss: -101.291] \n",
      "3395 [D loss: (-8.216)(R -119.136, F 102.704)]  [G loss: -96.929] \n",
      "3395 [D loss: (-13.147)(R -115.212, F 88.918)]  [G loss: -98.387] \n",
      "3396 [D loss: (-6.518)(R -111.683, F 98.647)]  [G loss: -105.789] \n",
      "3396 [D loss: (-15.044)(R -116.789, F 86.701)]  [G loss: -109.360] \n",
      "3397 [D loss: (-9.039)(R -117.348, F 99.271)]  [G loss: -104.036] \n",
      "3397 [D loss: (-0.554)(R -112.786, F 111.678)]  [G loss: -102.219] \n",
      "3398 [D loss: (-11.161)(R -121.635, F 99.314)]  [G loss: -102.524] \n",
      "3398 [D loss: (-7.325)(R -117.478, F 102.828)]  [G loss: -101.538] \n",
      "3399 [D loss: (-8.293)(R -114.791, F 98.206)]  [G loss: -102.144] \n",
      "3399 [D loss: (-5.420)(R -123.081, F 112.242)]  [G loss: -107.205] \n",
      "3400 [D loss: (-6.568)(R -118.129, F 104.993)]  [G loss: -110.141] \n",
      "3400 [D loss: (1.981)(R -114.759, F 118.721)]  [G loss: -106.954] \n",
      "3401 [D loss: (-0.114)(R -117.829, F 117.602)]  [G loss: -105.160] \n",
      "3401 [D loss: (-5.434)(R -120.206, F 109.339)]  [G loss: -109.195] \n",
      "3402 [D loss: (-6.387)(R -121.051, F 108.277)]  [G loss: -101.507] \n",
      "3402 [D loss: (-2.389)(R -113.778, F 109.000)]  [G loss: -114.203] \n",
      "3403 [D loss: (-6.680)(R -117.123, F 103.763)]  [G loss: -109.377] \n",
      "3403 [D loss: (-3.004)(R -117.636, F 111.628)]  [G loss: -108.905] \n",
      "3404 [D loss: (-5.679)(R -118.729, F 107.371)]  [G loss: -110.837] \n",
      "3404 [D loss: (-7.934)(R -122.825, F 106.956)]  [G loss: -109.220] \n",
      "3405 [D loss: (2.134)(R -120.830, F 125.097)]  [G loss: -108.732] \n",
      "3405 [D loss: (-3.797)(R -118.207, F 110.612)]  [G loss: -113.739] \n",
      "3406 [D loss: (-5.794)(R -119.234, F 107.646)]  [G loss: -114.754] \n",
      "3406 [D loss: (-9.449)(R -123.638, F 104.740)]  [G loss: -113.342] \n",
      "3407 [D loss: (-7.751)(R -121.228, F 105.727)]  [G loss: -115.030] \n",
      "3407 [D loss: (1.182)(R -115.096, F 117.460)]  [G loss: -116.056] \n",
      "3408 [D loss: (-5.387)(R -123.323, F 112.548)]  [G loss: -116.514] \n",
      "3408 [D loss: (2.121)(R -118.719, F 122.961)]  [G loss: -108.954] \n",
      "3409 [D loss: (-3.662)(R -120.512, F 113.189)]  [G loss: -110.870] \n",
      "3409 [D loss: (-4.482)(R -121.980, F 113.016)]  [G loss: -117.010] \n",
      "3410 [D loss: (0.962)(R -117.706, F 119.630)]  [G loss: -115.251] \n",
      "3410 [D loss: (-8.457)(R -122.621, F 105.706)]  [G loss: -115.082] \n",
      "3411 [D loss: (-1.038)(R -117.558, F 115.482)]  [G loss: -111.834] \n",
      "3411 [D loss: (9.283)(R -119.835, F 138.400)]  [G loss: -117.727] \n",
      "3412 [D loss: (-9.516)(R -121.964, F 102.932)]  [G loss: -119.635] \n",
      "3412 [D loss: (9.511)(R -117.234, F 136.255)]  [G loss: -113.383] \n",
      "3413 [D loss: (-12.806)(R -122.839, F 97.227)]  [G loss: -118.807] \n",
      "3413 [D loss: (-2.773)(R -125.427, F 119.881)]  [G loss: -121.684] \n",
      "3414 [D loss: (9.154)(R -119.383, F 137.692)]  [G loss: -116.830] \n",
      "3414 [D loss: (3.677)(R -115.722, F 123.075)]  [G loss: -116.011] \n",
      "3415 [D loss: (0.113)(R -124.997, F 125.222)]  [G loss: -117.571] \n",
      "3415 [D loss: (4.165)(R -124.282, F 132.612)]  [G loss: -120.855] \n",
      "3416 [D loss: (-2.822)(R -130.218, F 124.574)]  [G loss: -122.533] \n",
      "3416 [D loss: (3.611)(R -119.118, F 126.339)]  [G loss: -120.487] \n",
      "3417 [D loss: (-0.068)(R -125.140, F 125.003)]  [G loss: -116.220] \n",
      "3417 [D loss: (-2.794)(R -122.745, F 117.158)]  [G loss: -121.854] \n",
      "3418 [D loss: (9.106)(R -115.095, F 133.306)]  [G loss: -117.642] \n",
      "3418 [D loss: (-5.478)(R -121.364, F 110.407)]  [G loss: -124.431] \n",
      "3419 [D loss: (-0.743)(R -127.425, F 125.939)]  [G loss: -123.018] \n",
      "3419 [D loss: (3.404)(R -124.255, F 131.063)]  [G loss: -124.919] \n",
      "3420 [D loss: (-1.348)(R -122.215, F 119.519)]  [G loss: -124.129] \n",
      "3420 [D loss: (4.135)(R -122.960, F 131.229)]  [G loss: -124.190] \n",
      "3421 [D loss: (-3.020)(R -123.891, F 117.851)]  [G loss: -124.040] \n",
      "3421 [D loss: (3.184)(R -119.288, F 125.657)]  [G loss: -118.699] \n",
      "3422 [D loss: (3.569)(R -126.983, F 134.121)]  [G loss: -126.025] \n",
      "3422 [D loss: (4.049)(R -124.198, F 132.297)]  [G loss: -122.037] \n",
      "3423 [D loss: (2.491)(R -127.627, F 132.609)]  [G loss: -129.889] \n",
      "3423 [D loss: (4.518)(R -127.195, F 136.231)]  [G loss: -125.348] \n",
      "3424 [D loss: (2.264)(R -126.791, F 131.320)]  [G loss: -125.931] \n",
      "3424 [D loss: (7.224)(R -120.990, F 135.439)]  [G loss: -124.834] \n",
      "3425 [D loss: (-1.283)(R -129.180, F 126.615)]  [G loss: -125.638] \n",
      "3425 [D loss: (-1.065)(R -126.515, F 124.386)]  [G loss: -124.576] \n",
      "3426 [D loss: (3.605)(R -114.990, F 122.199)]  [G loss: -125.825] \n",
      "3426 [D loss: (-3.877)(R -124.759, F 117.006)]  [G loss: -124.328] \n",
      "3427 [D loss: (5.115)(R -125.719, F 135.948)]  [G loss: -126.294] \n",
      "3427 [D loss: (1.235)(R -121.450, F 123.919)]  [G loss: -126.051] \n",
      "3428 [D loss: (0.047)(R -120.683, F 120.778)]  [G loss: -122.675] \n",
      "3428 [D loss: (1.171)(R -122.633, F 124.976)]  [G loss: -123.252] \n",
      "3429 [D loss: (0.887)(R -130.123, F 131.896)]  [G loss: -133.282] \n",
      "3429 [D loss: (-4.710)(R -125.679, F 116.259)]  [G loss: -128.447] \n",
      "3430 [D loss: (4.843)(R -125.405, F 135.091)]  [G loss: -129.486] \n",
      "3430 [D loss: (2.266)(R -129.938, F 134.470)]  [G loss: -128.269] \n",
      "3431 [D loss: (-3.327)(R -125.675, F 119.022)]  [G loss: -131.262] \n",
      "3431 [D loss: (3.091)(R -121.867, F 128.049)]  [G loss: -129.128] \n",
      "3432 [D loss: (3.046)(R -125.629, F 131.721)]  [G loss: -128.406] \n",
      "3432 [D loss: (1.645)(R -123.238, F 126.528)]  [G loss: -133.402] \n",
      "3433 [D loss: (-1.418)(R -128.806, F 125.970)]  [G loss: -126.890] \n",
      "3433 [D loss: (2.824)(R -126.708, F 132.357)]  [G loss: -130.206] \n",
      "3434 [D loss: (-1.234)(R -125.586, F 123.118)]  [G loss: -125.667] \n",
      "3434 [D loss: (3.069)(R -119.860, F 125.998)]  [G loss: -130.438] \n",
      "3435 [D loss: (1.749)(R -123.636, F 127.134)]  [G loss: -130.103] \n",
      "3435 [D loss: (8.593)(R -124.081, F 141.268)]  [G loss: -134.090] \n",
      "3436 [D loss: (16.768)(R -124.203, F 157.739)]  [G loss: -133.274] \n",
      "3436 [D loss: (7.790)(R -122.513, F 138.093)]  [G loss: -130.428] \n",
      "3437 [D loss: (7.745)(R -123.151, F 138.641)]  [G loss: -130.879] \n",
      "3437 [D loss: (-0.357)(R -124.890, F 124.175)]  [G loss: -134.274] \n",
      "3438 [D loss: (4.331)(R -126.951, F 135.613)]  [G loss: -136.351] \n",
      "3438 [D loss: (9.714)(R -128.180, F 147.607)]  [G loss: -135.096] \n",
      "3439 [D loss: (5.480)(R -122.389, F 133.348)]  [G loss: -131.971] \n",
      "3439 [D loss: (11.282)(R -125.984, F 148.548)]  [G loss: -132.629] \n",
      "3440 [D loss: (8.445)(R -124.653, F 141.543)]  [G loss: -133.838] \n",
      "3440 [D loss: (-2.717)(R -125.266, F 119.832)]  [G loss: -133.934] \n",
      "3441 [D loss: (8.363)(R -121.433, F 138.159)]  [G loss: -132.619] \n",
      "3441 [D loss: (13.647)(R -118.268, F 145.561)]  [G loss: -136.297] \n",
      "3442 [D loss: (13.304)(R -118.386, F 144.993)]  [G loss: -131.275] \n",
      "3442 [D loss: (9.803)(R -118.074, F 137.680)]  [G loss: -131.956] \n",
      "3443 [D loss: (5.807)(R -126.212, F 137.826)]  [G loss: -129.414] \n",
      "3443 [D loss: (2.228)(R -122.640, F 127.095)]  [G loss: -135.165] \n",
      "3444 [D loss: (9.709)(R -120.345, F 139.762)]  [G loss: -132.436] \n",
      "3444 [D loss: (5.201)(R -122.365, F 132.768)]  [G loss: -130.286] \n",
      "3445 [D loss: (5.941)(R -118.460, F 130.342)]  [G loss: -135.088] \n",
      "3445 [D loss: (11.014)(R -120.956, F 142.983)]  [G loss: -132.688] \n",
      "3446 [D loss: (10.373)(R -120.322, F 141.067)]  [G loss: -132.996] \n",
      "3446 [D loss: (8.566)(R -114.822, F 131.953)]  [G loss: -135.204] \n",
      "3447 [D loss: (2.671)(R -118.992, F 124.335)]  [G loss: -134.363] \n",
      "3447 [D loss: (14.594)(R -116.445, F 145.634)]  [G loss: -131.421] \n",
      "3448 [D loss: (9.364)(R -119.707, F 138.436)]  [G loss: -133.616] \n",
      "3448 [D loss: (10.999)(R -121.253, F 143.251)]  [G loss: -135.261] \n",
      "3449 [D loss: (2.140)(R -117.914, F 122.194)]  [G loss: -128.344] \n",
      "3449 [D loss: (3.485)(R -123.377, F 130.347)]  [G loss: -137.418] \n",
      "3450 [D loss: (5.390)(R -121.599, F 132.379)]  [G loss: -132.430] \n",
      "3450 [D loss: (7.528)(R -122.195, F 137.251)]  [G loss: -134.766] \n",
      "3451 [D loss: (7.457)(R -121.453, F 136.366)]  [G loss: -135.544] \n",
      "3451 [D loss: (14.803)(R -121.767, F 151.373)]  [G loss: -133.192] \n",
      "3452 [D loss: (4.412)(R -121.966, F 130.790)]  [G loss: -133.664] \n",
      "3452 [D loss: (9.476)(R -117.799, F 136.750)]  [G loss: -132.492] \n",
      "3453 [D loss: (7.914)(R -120.493, F 136.321)]  [G loss: -133.461] \n",
      "3453 [D loss: (3.981)(R -120.349, F 128.310)]  [G loss: -134.926] \n",
      "3454 [D loss: (7.692)(R -113.881, F 129.265)]  [G loss: -139.923] \n",
      "3454 [D loss: (2.947)(R -116.550, F 122.443)]  [G loss: -137.622] \n",
      "3455 [D loss: (9.971)(R -120.480, F 140.422)]  [G loss: -129.722] \n",
      "3455 [D loss: (10.231)(R -111.251, F 131.713)]  [G loss: -136.849] \n",
      "3456 [D loss: (13.652)(R -112.960, F 140.264)]  [G loss: -136.932] \n",
      "3456 [D loss: (3.559)(R -117.586, F 124.704)]  [G loss: -131.735] \n",
      "3457 [D loss: (10.023)(R -119.273, F 139.318)]  [G loss: -132.242] \n",
      "3457 [D loss: (10.185)(R -113.276, F 133.645)]  [G loss: -135.227] \n",
      "3458 [D loss: (8.587)(R -119.200, F 136.374)]  [G loss: -134.055] \n",
      "3458 [D loss: (-1.138)(R -121.410, F 119.134)]  [G loss: -132.510] \n",
      "3459 [D loss: (12.240)(R -119.067, F 143.547)]  [G loss: -134.079] \n",
      "3459 [D loss: (8.146)(R -123.253, F 139.544)]  [G loss: -127.189] \n",
      "3460 [D loss: (9.048)(R -114.852, F 132.948)]  [G loss: -135.733] \n",
      "3460 [D loss: (12.802)(R -114.511, F 140.115)]  [G loss: -129.539] \n",
      "3461 [D loss: (0.255)(R -117.253, F 117.763)]  [G loss: -132.951] \n",
      "3461 [D loss: (8.924)(R -117.622, F 135.471)]  [G loss: -131.107] \n",
      "3462 [D loss: (5.781)(R -114.803, F 126.365)]  [G loss: -130.409] \n",
      "3462 [D loss: (8.290)(R -117.899, F 134.479)]  [G loss: -134.457] \n",
      "3463 [D loss: (3.872)(R -114.962, F 122.705)]  [G loss: -133.708] \n",
      "3463 [D loss: (6.682)(R -117.336, F 130.700)]  [G loss: -130.994] \n",
      "3464 [D loss: (16.437)(R -113.546, F 146.420)]  [G loss: -135.906] \n",
      "3464 [D loss: (12.089)(R -113.503, F 137.681)]  [G loss: -131.646] \n",
      "3465 [D loss: (9.369)(R -117.561, F 136.299)]  [G loss: -127.250] \n",
      "3465 [D loss: (20.912)(R -109.001, F 150.825)]  [G loss: -134.728] \n",
      "3466 [D loss: (5.961)(R -117.618, F 129.541)]  [G loss: -129.765] \n",
      "3466 [D loss: (10.347)(R -117.779, F 138.472)]  [G loss: -129.551] \n",
      "3467 [D loss: (14.615)(R -113.764, F 142.994)]  [G loss: -131.705] \n",
      "3467 [D loss: (10.135)(R -110.409, F 130.680)]  [G loss: -134.088] \n",
      "3468 [D loss: (12.659)(R -111.587, F 136.904)]  [G loss: -127.965] \n",
      "3468 [D loss: (6.928)(R -112.505, F 126.362)]  [G loss: -128.207] \n",
      "3469 [D loss: (4.418)(R -119.107, F 127.943)]  [G loss: -132.909] \n",
      "3469 [D loss: (8.667)(R -115.290, F 132.625)]  [G loss: -133.046] \n",
      "3470 [D loss: (12.343)(R -114.325, F 139.011)]  [G loss: -132.139] \n",
      "3470 [D loss: (0.002)(R -115.443, F 115.447)]  [G loss: -128.719] \n",
      "3471 [D loss: (3.560)(R -119.365, F 126.484)]  [G loss: -134.855] \n",
      "3471 [D loss: (11.767)(R -111.670, F 135.205)]  [G loss: -130.113] \n",
      "3472 [D loss: (2.873)(R -112.943, F 118.688)]  [G loss: -132.956] \n",
      "3472 [D loss: (3.589)(R -112.062, F 119.239)]  [G loss: -134.950] \n",
      "3473 [D loss: (14.275)(R -115.616, F 144.167)]  [G loss: -134.238] \n",
      "3473 [D loss: (10.617)(R -111.834, F 133.069)]  [G loss: -130.796] \n",
      "3474 [D loss: (16.958)(R -111.487, F 145.402)]  [G loss: -131.615] \n",
      "3474 [D loss: (9.662)(R -112.349, F 131.674)]  [G loss: -132.193] \n",
      "3475 [D loss: (18.135)(R -102.128, F 138.397)]  [G loss: -133.584] \n",
      "3475 [D loss: (12.348)(R -113.866, F 138.563)]  [G loss: -131.750] \n",
      "3476 [D loss: (10.053)(R -112.556, F 132.662)]  [G loss: -133.826] \n",
      "3476 [D loss: (10.154)(R -108.654, F 128.961)]  [G loss: -131.721] \n",
      "3477 [D loss: (7.235)(R -111.901, F 126.370)]  [G loss: -131.936] \n",
      "3477 [D loss: (12.911)(R -110.027, F 135.850)]  [G loss: -130.904] \n",
      "3478 [D loss: (14.032)(R -108.947, F 137.011)]  [G loss: -131.938] \n",
      "3478 [D loss: (11.989)(R -111.271, F 135.250)]  [G loss: -131.357] \n",
      "3479 [D loss: (7.440)(R -111.853, F 126.734)]  [G loss: -134.679] \n",
      "3479 [D loss: (13.395)(R -110.726, F 137.516)]  [G loss: -131.881] \n",
      "3480 [D loss: (13.697)(R -111.794, F 139.188)]  [G loss: -130.638] \n",
      "3480 [D loss: (12.409)(R -113.436, F 138.254)]  [G loss: -128.056] \n",
      "3481 [D loss: (9.951)(R -108.468, F 128.369)]  [G loss: -127.546] \n",
      "3481 [D loss: (5.580)(R -108.658, F 119.817)]  [G loss: -124.956] \n",
      "3482 [D loss: (7.356)(R -110.806, F 125.518)]  [G loss: -127.590] \n",
      "3482 [D loss: (10.790)(R -109.516, F 131.096)]  [G loss: -125.627] \n",
      "3483 [D loss: (11.450)(R -109.749, F 132.649)]  [G loss: -129.032] \n",
      "3483 [D loss: (8.012)(R -107.504, F 123.527)]  [G loss: -129.073] \n",
      "3484 [D loss: (13.298)(R -110.224, F 136.820)]  [G loss: -129.397] \n",
      "3484 [D loss: (4.285)(R -109.355, F 117.924)]  [G loss: -129.989] \n",
      "3485 [D loss: (7.583)(R -110.761, F 125.926)]  [G loss: -135.862] \n",
      "3485 [D loss: (10.778)(R -104.903, F 126.459)]  [G loss: -125.373] \n",
      "3486 [D loss: (15.918)(R -102.814, F 134.649)]  [G loss: -132.877] \n",
      "3486 [D loss: (1.287)(R -111.222, F 113.797)]  [G loss: -128.652] \n",
      "3487 [D loss: (14.138)(R -108.048, F 136.325)]  [G loss: -127.822] \n",
      "3487 [D loss: (19.060)(R -105.759, F 143.878)]  [G loss: -129.546] \n",
      "3488 [D loss: (7.354)(R -111.199, F 125.907)]  [G loss: -124.229] \n",
      "3488 [D loss: (6.222)(R -105.185, F 117.630)]  [G loss: -126.838] \n",
      "3489 [D loss: (3.995)(R -107.009, F 114.999)]  [G loss: -127.529] \n",
      "3489 [D loss: (13.066)(R -106.439, F 132.570)]  [G loss: -127.442] \n",
      "3490 [D loss: (10.649)(R -107.181, F 128.478)]  [G loss: -126.030] \n",
      "3490 [D loss: (10.129)(R -103.035, F 123.293)]  [G loss: -127.521] \n",
      "3491 [D loss: (8.320)(R -110.802, F 127.441)]  [G loss: -126.872] \n",
      "3491 [D loss: (13.901)(R -106.574, F 134.375)]  [G loss: -126.094] \n",
      "3492 [D loss: (10.835)(R -103.752, F 125.423)]  [G loss: -125.066] \n",
      "3492 [D loss: (9.760)(R -102.716, F 122.236)]  [G loss: -127.919] \n",
      "3493 [D loss: (6.785)(R -105.781, F 119.352)]  [G loss: -127.572] \n",
      "3493 [D loss: (10.762)(R -107.937, F 129.461)]  [G loss: -122.530] \n",
      "3494 [D loss: (6.741)(R -108.199, F 121.681)]  [G loss: -126.049] \n",
      "3494 [D loss: (5.420)(R -104.754, F 115.594)]  [G loss: -121.472] \n",
      "3495 [D loss: (9.514)(R -100.975, F 120.002)]  [G loss: -123.046] \n",
      "3495 [D loss: (8.828)(R -100.864, F 118.519)]  [G loss: -125.978] \n",
      "3496 [D loss: (10.868)(R -105.754, F 127.489)]  [G loss: -125.581] \n",
      "3496 [D loss: (7.548)(R -102.927, F 118.022)]  [G loss: -127.364] \n",
      "3497 [D loss: (11.758)(R -100.487, F 124.003)]  [G loss: -126.934] \n",
      "3497 [D loss: (10.950)(R -104.390, F 126.289)]  [G loss: -122.580] \n",
      "3498 [D loss: (11.752)(R -101.537, F 125.042)]  [G loss: -130.255] \n",
      "3498 [D loss: (13.871)(R -99.949, F 127.690)]  [G loss: -122.829] \n",
      "3499 [D loss: (9.107)(R -103.667, F 121.881)]  [G loss: -126.996] \n",
      "3499 [D loss: (7.479)(R -98.717, F 113.674)]  [G loss: -123.696] \n",
      "3500 [D loss: (12.763)(R -101.700, F 127.226)]  [G loss: -127.960] \n",
      "3500 [D loss: (8.351)(R -102.624, F 119.327)]  [G loss: -123.352] \n",
      "3501 [D loss: (6.640)(R -101.028, F 114.307)]  [G loss: -125.404] \n",
      "3501 [D loss: (7.033)(R -101.926, F 115.993)]  [G loss: -123.849] \n",
      "3502 [D loss: (12.063)(R -101.565, F 125.691)]  [G loss: -122.189] \n",
      "3502 [D loss: (8.271)(R -102.499, F 119.040)]  [G loss: -125.796] \n",
      "3503 [D loss: (13.217)(R -99.571, F 126.006)]  [G loss: -126.640] \n",
      "3503 [D loss: (12.787)(R -102.597, F 128.170)]  [G loss: -123.345] \n",
      "3504 [D loss: (6.284)(R -102.014, F 114.582)]  [G loss: -122.018] \n",
      "3504 [D loss: (7.435)(R -101.160, F 116.030)]  [G loss: -122.582] \n",
      "3505 [D loss: (9.190)(R -98.281, F 116.662)]  [G loss: -124.155] \n",
      "3505 [D loss: (8.886)(R -102.218, F 119.991)]  [G loss: -120.206] \n",
      "3506 [D loss: (11.410)(R -96.487, F 119.306)]  [G loss: -121.859] \n",
      "3506 [D loss: (6.940)(R -99.726, F 113.607)]  [G loss: -120.364] \n",
      "3507 [D loss: (12.939)(R -99.621, F 125.500)]  [G loss: -122.112] \n",
      "3507 [D loss: (7.935)(R -97.419, F 113.290)]  [G loss: -125.166] \n",
      "3508 [D loss: (8.494)(R -97.001, F 113.990)]  [G loss: -122.178] \n",
      "3508 [D loss: (10.252)(R -100.275, F 120.778)]  [G loss: -124.812] \n",
      "3509 [D loss: (12.346)(R -95.714, F 120.405)]  [G loss: -122.004] \n",
      "3509 [D loss: (7.611)(R -99.725, F 114.947)]  [G loss: -117.352] \n",
      "3510 [D loss: (7.292)(R -103.381, F 117.965)]  [G loss: -119.495] \n",
      "3510 [D loss: (15.252)(R -96.808, F 127.312)]  [G loss: -119.017] \n",
      "3511 [D loss: (8.069)(R -95.272, F 111.411)]  [G loss: -121.316] \n",
      "3511 [D loss: (7.202)(R -99.564, F 113.969)]  [G loss: -119.236] \n",
      "3512 [D loss: (11.020)(R -97.797, F 119.836)]  [G loss: -119.944] \n",
      "3512 [D loss: (7.321)(R -101.874, F 116.517)]  [G loss: -120.933] \n",
      "3513 [D loss: (10.339)(R -99.759, F 120.437)]  [G loss: -119.268] \n",
      "3513 [D loss: (13.418)(R -97.810, F 124.647)]  [G loss: -121.209] \n",
      "3514 [D loss: (13.130)(R -96.650, F 122.910)]  [G loss: -118.327] \n",
      "3514 [D loss: (13.033)(R -94.884, F 120.949)]  [G loss: -122.396] \n",
      "3515 [D loss: (9.926)(R -94.497, F 114.349)]  [G loss: -116.062] \n",
      "3515 [D loss: (12.312)(R -93.204, F 117.829)]  [G loss: -115.973] \n",
      "3516 [D loss: (12.203)(R -95.155, F 119.561)]  [G loss: -117.209] \n",
      "3516 [D loss: (9.853)(R -96.513, F 116.219)]  [G loss: -119.178] \n",
      "3517 [D loss: (11.475)(R -94.513, F 117.463)]  [G loss: -115.512] \n",
      "3517 [D loss: (7.052)(R -99.108, F 113.212)]  [G loss: -119.778] \n",
      "3518 [D loss: (9.174)(R -95.941, F 114.289)]  [G loss: -119.017] \n",
      "3518 [D loss: (9.677)(R -94.451, F 113.806)]  [G loss: -117.350] \n",
      "3519 [D loss: (6.195)(R -96.666, F 109.057)]  [G loss: -114.768] \n",
      "3519 [D loss: (17.839)(R -92.968, F 128.647)]  [G loss: -115.419] \n",
      "3520 [D loss: (8.086)(R -93.287, F 109.460)]  [G loss: -116.870] \n",
      "3520 [D loss: (9.919)(R -97.574, F 117.412)]  [G loss: -117.138] \n",
      "3521 [D loss: (6.002)(R -97.666, F 109.670)]  [G loss: -118.140] \n",
      "3521 [D loss: (13.510)(R -93.279, F 120.300)]  [G loss: -115.595] \n",
      "3522 [D loss: (9.830)(R -92.873, F 112.532)]  [G loss: -114.232] \n",
      "3522 [D loss: (10.273)(R -92.793, F 113.338)]  [G loss: -113.059] \n",
      "3523 [D loss: (8.857)(R -94.244, F 111.958)]  [G loss: -112.883] \n",
      "3523 [D loss: (12.639)(R -91.626, F 116.903)]  [G loss: -115.471] \n",
      "3524 [D loss: (12.540)(R -94.063, F 119.143)]  [G loss: -114.171] \n",
      "3524 [D loss: (15.279)(R -88.876, F 119.434)]  [G loss: -114.163] \n",
      "3525 [D loss: (14.333)(R -90.332, F 118.998)]  [G loss: -113.348] \n",
      "3525 [D loss: (10.283)(R -91.635, F 112.201)]  [G loss: -113.842] \n",
      "3526 [D loss: (14.293)(R -89.853, F 118.439)]  [G loss: -108.826] \n",
      "3526 [D loss: (10.366)(R -85.491, F 106.223)]  [G loss: -112.942] \n",
      "3527 [D loss: (13.550)(R -87.016, F 114.115)]  [G loss: -111.315] \n",
      "3527 [D loss: (12.175)(R -88.169, F 112.520)]  [G loss: -111.721] \n",
      "3528 [D loss: (10.002)(R -92.512, F 112.516)]  [G loss: -115.921] \n",
      "3528 [D loss: (7.904)(R -87.826, F 103.634)]  [G loss: -112.414] \n",
      "3529 [D loss: (13.078)(R -87.999, F 114.156)]  [G loss: -113.619] \n",
      "3529 [D loss: (13.315)(R -89.309, F 115.940)]  [G loss: -109.874] \n",
      "3530 [D loss: (13.912)(R -87.125, F 114.949)]  [G loss: -113.221] \n",
      "3530 [D loss: (10.572)(R -89.867, F 111.012)]  [G loss: -113.047] \n",
      "3531 [D loss: (14.600)(R -87.619, F 116.819)]  [G loss: -112.087] \n",
      "3531 [D loss: (10.036)(R -90.519, F 110.590)]  [G loss: -111.882] \n",
      "3532 [D loss: (14.061)(R -85.539, F 113.661)]  [G loss: -109.946] \n",
      "3532 [D loss: (15.360)(R -84.968, F 115.687)]  [G loss: -113.248] \n",
      "3533 [D loss: (10.496)(R -86.136, F 107.127)]  [G loss: -115.421] \n",
      "3533 [D loss: (12.111)(R -88.961, F 113.182)]  [G loss: -110.807] \n",
      "3534 [D loss: (6.297)(R -90.046, F 102.640)]  [G loss: -110.243] \n",
      "3534 [D loss: (12.333)(R -88.713, F 113.378)]  [G loss: -111.772] \n",
      "3535 [D loss: (5.942)(R -90.607, F 102.492)]  [G loss: -107.534] \n",
      "3535 [D loss: (17.684)(R -84.849, F 120.216)]  [G loss: -111.647] \n",
      "3536 [D loss: (10.003)(R -88.631, F 108.637)]  [G loss: -111.386] \n",
      "3536 [D loss: (15.029)(R -84.740, F 114.798)]  [G loss: -112.509] \n",
      "3537 [D loss: (3.617)(R -88.234, F 95.468)]  [G loss: -108.383] \n",
      "3537 [D loss: (11.315)(R -84.682, F 107.311)]  [G loss: -107.675] \n",
      "3538 [D loss: (13.026)(R -85.696, F 111.747)]  [G loss: -106.209] \n",
      "3538 [D loss: (17.194)(R -83.912, F 118.299)]  [G loss: -110.154] \n",
      "3539 [D loss: (8.774)(R -84.825, F 102.373)]  [G loss: -104.945] \n",
      "3539 [D loss: (20.606)(R -83.776, F 124.988)]  [G loss: -109.552] \n",
      "3540 [D loss: (7.554)(R -85.608, F 100.717)]  [G loss: -107.116] \n",
      "3540 [D loss: (16.354)(R -81.658, F 114.366)]  [G loss: -104.373] \n",
      "3541 [D loss: (11.633)(R -83.941, F 107.208)]  [G loss: -105.329] \n",
      "3541 [D loss: (13.838)(R -83.580, F 111.256)]  [G loss: -107.383] \n",
      "3542 [D loss: (6.935)(R -85.932, F 99.801)]  [G loss: -105.951] \n",
      "3542 [D loss: (12.971)(R -82.222, F 108.163)]  [G loss: -107.606] \n",
      "3543 [D loss: (11.808)(R -84.641, F 108.257)]  [G loss: -107.061] \n",
      "3543 [D loss: (14.190)(R -85.137, F 113.517)]  [G loss: -103.746] \n",
      "3544 [D loss: (11.410)(R -85.976, F 108.795)]  [G loss: -103.764] \n",
      "3544 [D loss: (9.685)(R -82.817, F 102.186)]  [G loss: -104.754] \n",
      "3545 [D loss: (13.800)(R -83.565, F 111.166)]  [G loss: -105.182] \n",
      "3545 [D loss: (13.836)(R -80.395, F 108.066)]  [G loss: -102.946] \n",
      "3546 [D loss: (16.001)(R -77.289, F 109.290)]  [G loss: -104.436] \n",
      "3546 [D loss: (12.756)(R -80.546, F 106.058)]  [G loss: -101.875] \n",
      "3547 [D loss: (11.226)(R -84.487, F 106.939)]  [G loss: -103.388] \n",
      "3547 [D loss: (9.876)(R -82.403, F 102.155)]  [G loss: -102.286] \n",
      "3548 [D loss: (7.030)(R -81.848, F 95.908)]  [G loss: -105.618] \n",
      "3548 [D loss: (9.447)(R -81.971, F 100.865)]  [G loss: -98.597] \n",
      "3549 [D loss: (9.742)(R -78.163, F 97.646)]  [G loss: -102.629] \n",
      "3549 [D loss: (7.173)(R -80.637, F 94.983)]  [G loss: -101.577] \n",
      "3550 [D loss: (14.283)(R -79.284, F 107.850)]  [G loss: -99.997] \n",
      "3550 [D loss: (7.742)(R -79.991, F 95.475)]  [G loss: -102.588] \n",
      "3551 [D loss: (14.584)(R -76.874, F 106.042)]  [G loss: -102.349] \n",
      "3551 [D loss: (13.726)(R -74.777, F 102.228)]  [G loss: -103.162] \n",
      "3552 [D loss: (9.517)(R -79.836, F 98.870)]  [G loss: -100.092] \n",
      "3552 [D loss: (9.399)(R -80.057, F 98.855)]  [G loss: -102.498] \n",
      "3553 [D loss: (13.721)(R -74.308, F 101.749)]  [G loss: -99.972] \n",
      "3553 [D loss: (13.793)(R -76.702, F 104.289)]  [G loss: -101.387] \n",
      "3554 [D loss: (11.339)(R -77.048, F 99.725)]  [G loss: -96.281] \n",
      "3554 [D loss: (6.892)(R -77.737, F 91.520)]  [G loss: -100.514] \n",
      "3555 [D loss: (10.720)(R -78.079, F 99.519)]  [G loss: -99.117] \n",
      "3555 [D loss: (11.097)(R -78.315, F 100.508)]  [G loss: -98.529] \n",
      "3556 [D loss: (11.440)(R -74.894, F 97.774)]  [G loss: -97.386] \n",
      "3556 [D loss: (9.789)(R -76.499, F 96.077)]  [G loss: -94.953] \n",
      "3557 [D loss: (8.762)(R -76.934, F 94.457)]  [G loss: -97.140] \n",
      "3557 [D loss: (11.164)(R -76.192, F 98.521)]  [G loss: -97.305] \n",
      "3558 [D loss: (9.993)(R -71.538, F 91.524)]  [G loss: -96.166] \n",
      "3558 [D loss: (8.296)(R -75.357, F 91.950)]  [G loss: -97.265] \n",
      "3559 [D loss: (8.212)(R -75.668, F 92.092)]  [G loss: -99.105] \n",
      "3559 [D loss: (9.008)(R -77.562, F 95.578)]  [G loss: -94.401] \n",
      "3560 [D loss: (10.890)(R -73.358, F 95.138)]  [G loss: -97.731] \n",
      "3560 [D loss: (11.360)(R -72.499, F 95.220)]  [G loss: -94.929] \n",
      "3561 [D loss: (10.208)(R -76.044, F 96.460)]  [G loss: -97.095] \n",
      "3561 [D loss: (11.594)(R -74.165, F 97.353)]  [G loss: -95.902] \n",
      "3562 [D loss: (9.666)(R -72.221, F 91.554)]  [G loss: -96.558] \n",
      "3562 [D loss: (4.758)(R -73.088, F 82.604)]  [G loss: -93.714] \n",
      "3563 [D loss: (13.297)(R -72.074, F 98.669)]  [G loss: -91.007] \n",
      "3563 [D loss: (10.918)(R -73.550, F 95.385)]  [G loss: -94.563] \n",
      "3564 [D loss: (4.167)(R -75.196, F 83.531)]  [G loss: -93.711] \n",
      "3564 [D loss: (10.201)(R -74.360, F 94.762)]  [G loss: -93.984] \n",
      "3565 [D loss: (7.107)(R -70.777, F 84.991)]  [G loss: -95.023] \n",
      "3565 [D loss: (9.386)(R -73.118, F 91.890)]  [G loss: -93.685] \n",
      "3566 [D loss: (10.758)(R -72.396, F 93.911)]  [G loss: -96.375] \n",
      "3566 [D loss: (11.738)(R -72.332, F 95.808)]  [G loss: -90.122] \n",
      "3567 [D loss: (8.704)(R -71.526, F 88.934)]  [G loss: -93.444] \n",
      "3567 [D loss: (12.604)(R -70.923, F 96.131)]  [G loss: -89.822] \n",
      "3568 [D loss: (11.846)(R -70.806, F 94.498)]  [G loss: -92.718] \n",
      "3568 [D loss: (11.079)(R -71.920, F 94.079)]  [G loss: -91.601] \n",
      "3569 [D loss: (13.274)(R -70.640, F 97.188)]  [G loss: -92.237] \n",
      "3569 [D loss: (8.038)(R -72.138, F 88.214)]  [G loss: -91.797] \n",
      "3570 [D loss: (12.200)(R -70.405, F 94.804)]  [G loss: -91.000] \n",
      "3570 [D loss: (9.977)(R -70.504, F 90.457)]  [G loss: -90.524] \n",
      "3571 [D loss: (10.317)(R -70.882, F 91.515)]  [G loss: -89.987] \n",
      "3571 [D loss: (8.229)(R -70.263, F 86.721)]  [G loss: -90.875] \n",
      "3572 [D loss: (7.298)(R -69.775, F 84.372)]  [G loss: -90.485] \n",
      "3572 [D loss: (10.178)(R -70.050, F 90.405)]  [G loss: -89.469] \n",
      "3573 [D loss: (9.830)(R -69.673, F 89.332)]  [G loss: -88.064] \n",
      "3573 [D loss: (12.264)(R -67.225, F 91.753)]  [G loss: -89.637] \n",
      "3574 [D loss: (6.467)(R -71.506, F 84.440)]  [G loss: -86.417] \n",
      "3574 [D loss: (12.820)(R -67.902, F 93.543)]  [G loss: -87.120] \n",
      "3575 [D loss: (12.696)(R -68.312, F 93.704)]  [G loss: -86.799] \n",
      "3575 [D loss: (11.251)(R -68.230, F 90.731)]  [G loss: -88.134] \n",
      "3576 [D loss: (13.309)(R -65.506, F 92.124)]  [G loss: -85.204] \n",
      "3576 [D loss: (7.413)(R -67.531, F 82.357)]  [G loss: -87.195] \n",
      "3577 [D loss: (8.218)(R -67.130, F 83.565)]  [G loss: -89.839] \n",
      "3577 [D loss: (9.854)(R -68.935, F 88.643)]  [G loss: -87.985] \n",
      "3578 [D loss: (10.737)(R -66.790, F 88.265)]  [G loss: -83.649] \n",
      "3578 [D loss: (6.592)(R -66.061, F 79.245)]  [G loss: -86.554] \n",
      "3579 [D loss: (7.542)(R -67.972, F 83.057)]  [G loss: -87.670] \n",
      "3579 [D loss: (7.900)(R -67.426, F 83.226)]  [G loss: -88.874] \n",
      "3580 [D loss: (12.107)(R -64.631, F 88.846)]  [G loss: -86.954] \n",
      "3580 [D loss: (11.154)(R -65.868, F 88.176)]  [G loss: -87.948] \n",
      "3581 [D loss: (11.088)(R -66.341, F 88.517)]  [G loss: -85.396] \n",
      "3581 [D loss: (7.699)(R -66.631, F 82.029)]  [G loss: -82.984] \n",
      "3582 [D loss: (6.763)(R -66.895, F 80.421)]  [G loss: -84.962] \n",
      "3582 [D loss: (7.381)(R -63.806, F 78.568)]  [G loss: -87.495] \n",
      "3583 [D loss: (8.521)(R -65.107, F 82.149)]  [G loss: -83.432] \n",
      "3583 [D loss: (7.509)(R -64.299, F 79.316)]  [G loss: -86.530] \n",
      "3584 [D loss: (10.586)(R -63.033, F 84.206)]  [G loss: -83.438] \n",
      "3584 [D loss: (4.452)(R -66.788, F 75.691)]  [G loss: -84.022] \n",
      "3585 [D loss: (8.921)(R -65.339, F 83.181)]  [G loss: -85.787] \n",
      "3585 [D loss: (9.764)(R -65.261, F 84.790)]  [G loss: -84.495] \n",
      "3586 [D loss: (9.531)(R -65.033, F 84.095)]  [G loss: -84.771] \n",
      "3586 [D loss: (8.660)(R -62.067, F 79.388)]  [G loss: -85.860] \n",
      "3587 [D loss: (9.721)(R -61.193, F 80.634)]  [G loss: -84.109] \n",
      "3587 [D loss: (9.868)(R -64.076, F 83.813)]  [G loss: -79.774] \n",
      "3588 [D loss: (12.698)(R -61.331, F 86.727)]  [G loss: -83.912] \n",
      "3588 [D loss: (10.533)(R -62.842, F 83.909)]  [G loss: -82.508] \n",
      "3589 [D loss: (7.310)(R -62.795, F 77.416)]  [G loss: -81.708] \n",
      "3589 [D loss: (7.270)(R -61.794, F 76.334)]  [G loss: -82.205] \n",
      "3590 [D loss: (7.943)(R -63.864, F 79.750)]  [G loss: -81.686] \n",
      "3590 [D loss: (8.898)(R -63.043, F 80.839)]  [G loss: -80.810] \n",
      "3591 [D loss: (6.834)(R -60.795, F 74.464)]  [G loss: -81.468] \n",
      "3591 [D loss: (9.872)(R -60.401, F 80.145)]  [G loss: -81.985] \n",
      "3592 [D loss: (8.118)(R -60.916, F 77.152)]  [G loss: -81.717] \n",
      "3592 [D loss: (10.797)(R -60.589, F 82.182)]  [G loss: -80.567] \n",
      "3593 [D loss: (9.296)(R -60.992, F 79.584)]  [G loss: -79.786] \n",
      "3593 [D loss: (8.175)(R -61.503, F 77.853)]  [G loss: -80.962] \n",
      "3594 [D loss: (6.576)(R -58.366, F 71.519)]  [G loss: -77.564] \n",
      "3594 [D loss: (10.254)(R -59.591, F 80.099)]  [G loss: -81.666] \n",
      "3595 [D loss: (10.606)(R -59.860, F 81.072)]  [G loss: -80.619] \n",
      "3595 [D loss: (9.760)(R -59.298, F 78.817)]  [G loss: -79.276] \n",
      "3596 [D loss: (8.470)(R -60.318, F 77.258)]  [G loss: -79.005] \n",
      "3596 [D loss: (10.038)(R -59.326, F 79.402)]  [G loss: -78.184] \n",
      "3597 [D loss: (10.302)(R -58.249, F 78.853)]  [G loss: -77.974] \n",
      "3597 [D loss: (8.790)(R -59.458, F 77.039)]  [G loss: -77.028] \n",
      "3598 [D loss: (8.656)(R -58.434, F 75.746)]  [G loss: -77.663] \n",
      "3598 [D loss: (7.131)(R -59.428, F 73.690)]  [G loss: -76.889] \n",
      "3599 [D loss: (8.969)(R -58.653, F 76.592)]  [G loss: -76.201] \n",
      "3599 [D loss: (11.322)(R -58.906, F 81.550)]  [G loss: -79.156] \n",
      "3600 [D loss: (8.472)(R -58.570, F 75.513)]  [G loss: -76.095] \n",
      "3600 [D loss: (9.216)(R -57.775, F 76.206)]  [G loss: -78.337] \n",
      "3601 [D loss: (8.089)(R -57.037, F 73.214)]  [G loss: -76.792] \n",
      "3601 [D loss: (12.046)(R -56.423, F 80.516)]  [G loss: -75.068] \n",
      "3602 [D loss: (10.292)(R -57.276, F 77.860)]  [G loss: -75.444] \n",
      "3602 [D loss: (12.670)(R -57.289, F 82.629)]  [G loss: -77.245] \n",
      "3603 [D loss: (9.163)(R -58.194, F 76.520)]  [G loss: -75.542] \n",
      "3603 [D loss: (8.591)(R -58.163, F 75.345)]  [G loss: -76.165] \n",
      "3604 [D loss: (13.742)(R -58.693, F 86.178)]  [G loss: -75.617] \n",
      "3604 [D loss: (11.233)(R -55.765, F 78.231)]  [G loss: -75.485] \n",
      "3605 [D loss: (10.132)(R -57.605, F 77.869)]  [G loss: -74.516] \n",
      "3605 [D loss: (9.639)(R -56.732, F 76.011)]  [G loss: -74.267] \n",
      "3606 [D loss: (7.940)(R -55.731, F 71.611)]  [G loss: -74.114] \n",
      "3606 [D loss: (6.916)(R -56.579, F 70.411)]  [G loss: -75.223] \n",
      "3607 [D loss: (12.280)(R -54.800, F 79.361)]  [G loss: -72.791] \n",
      "3607 [D loss: (11.715)(R -56.360, F 79.790)]  [G loss: -74.836] \n",
      "3608 [D loss: (7.668)(R -55.546, F 70.883)]  [G loss: -72.900] \n",
      "3608 [D loss: (8.896)(R -56.265, F 74.056)]  [G loss: -73.571] \n",
      "3609 [D loss: (6.794)(R -55.910, F 69.498)]  [G loss: -70.877] \n",
      "3609 [D loss: (7.530)(R -56.164, F 71.224)]  [G loss: -72.925] \n",
      "3610 [D loss: (10.497)(R -54.111, F 75.105)]  [G loss: -73.908] \n",
      "3610 [D loss: (9.069)(R -53.529, F 71.666)]  [G loss: -72.542] \n",
      "3611 [D loss: (6.750)(R -54.005, F 67.506)]  [G loss: -70.145] \n",
      "3611 [D loss: (7.254)(R -56.830, F 71.338)]  [G loss: -71.805] \n",
      "3612 [D loss: (13.028)(R -53.052, F 79.108)]  [G loss: -71.822] \n",
      "3612 [D loss: (9.151)(R -54.205, F 72.507)]  [G loss: -70.921] \n",
      "3613 [D loss: (10.304)(R -52.969, F 73.577)]  [G loss: -69.710] \n",
      "3613 [D loss: (10.995)(R -54.451, F 76.440)]  [G loss: -72.657] \n",
      "3614 [D loss: (9.440)(R -52.346, F 71.227)]  [G loss: -70.997] \n",
      "3614 [D loss: (8.443)(R -53.656, F 70.542)]  [G loss: -71.917] \n",
      "3615 [D loss: (8.145)(R -53.839, F 70.129)]  [G loss: -69.084] \n",
      "3615 [D loss: (11.814)(R -51.775, F 75.402)]  [G loss: -68.768] \n",
      "3616 [D loss: (9.523)(R -52.544, F 71.589)]  [G loss: -69.442] \n",
      "3616 [D loss: (8.126)(R -54.497, F 70.749)]  [G loss: -69.455] \n",
      "3617 [D loss: (6.867)(R -51.797, F 65.531)]  [G loss: -70.597] \n",
      "3617 [D loss: (9.307)(R -51.187, F 69.800)]  [G loss: -70.482] \n",
      "3618 [D loss: (9.618)(R -50.084, F 69.321)]  [G loss: -69.597] \n",
      "3618 [D loss: (10.110)(R -51.373, F 71.592)]  [G loss: -67.697] \n",
      "3619 [D loss: (7.440)(R -53.241, F 68.122)]  [G loss: -67.928] \n",
      "3619 [D loss: (9.867)(R -52.044, F 71.778)]  [G loss: -66.580] \n",
      "3620 [D loss: (11.881)(R -49.953, F 73.715)]  [G loss: -67.322] \n",
      "3620 [D loss: (7.643)(R -51.095, F 66.381)]  [G loss: -68.519] \n",
      "3621 [D loss: (7.956)(R -49.820, F 65.732)]  [G loss: -66.908] \n",
      "3621 [D loss: (9.433)(R -50.428, F 69.294)]  [G loss: -65.974] \n",
      "3622 [D loss: (8.376)(R -50.935, F 67.687)]  [G loss: -67.130] \n",
      "3622 [D loss: (9.460)(R -49.576, F 68.495)]  [G loss: -64.988] \n",
      "3623 [D loss: (9.508)(R -48.732, F 67.749)]  [G loss: -66.661] \n",
      "3623 [D loss: (11.786)(R -48.771, F 72.344)]  [G loss: -65.358] \n",
      "3624 [D loss: (9.971)(R -49.784, F 69.727)]  [G loss: -65.278] \n",
      "3624 [D loss: (6.744)(R -48.618, F 62.105)]  [G loss: -64.654] \n",
      "3625 [D loss: (9.562)(R -47.443, F 66.567)]  [G loss: -65.334] \n",
      "3625 [D loss: (7.219)(R -48.492, F 62.930)]  [G loss: -65.610] \n",
      "3626 [D loss: (6.728)(R -47.837, F 61.293)]  [G loss: -67.425] \n",
      "3626 [D loss: (10.104)(R -48.182, F 68.389)]  [G loss: -65.077] \n",
      "3627 [D loss: (8.155)(R -49.060, F 65.370)]  [G loss: -65.944] \n",
      "3627 [D loss: (10.140)(R -47.645, F 67.926)]  [G loss: -65.517] \n",
      "3628 [D loss: (7.722)(R -47.166, F 62.610)]  [G loss: -63.572] \n",
      "3628 [D loss: (8.442)(R -47.703, F 64.588)]  [G loss: -64.868] \n",
      "3629 [D loss: (6.868)(R -48.014, F 61.750)]  [G loss: -62.238] \n",
      "3629 [D loss: (8.940)(R -47.388, F 65.268)]  [G loss: -64.531] \n",
      "3630 [D loss: (8.675)(R -48.298, F 65.649)]  [G loss: -63.251] \n",
      "3630 [D loss: (8.831)(R -45.442, F 63.105)]  [G loss: -63.230] \n",
      "3631 [D loss: (8.817)(R -45.431, F 63.065)]  [G loss: -62.308] \n",
      "3631 [D loss: (10.043)(R -46.793, F 66.879)]  [G loss: -62.398] \n",
      "3632 [D loss: (8.504)(R -47.758, F 64.767)]  [G loss: -61.217] \n",
      "3632 [D loss: (4.577)(R -46.932, F 56.087)]  [G loss: -61.444] \n",
      "3633 [D loss: (8.522)(R -46.170, F 63.213)]  [G loss: -62.058] \n",
      "3633 [D loss: (7.241)(R -45.505, F 59.987)]  [G loss: -61.116] \n",
      "3634 [D loss: (6.038)(R -46.555, F 58.631)]  [G loss: -61.223] \n",
      "3634 [D loss: (6.986)(R -47.497, F 61.469)]  [G loss: -59.939] \n",
      "3635 [D loss: (6.332)(R -46.959, F 59.623)]  [G loss: -63.080] \n",
      "3635 [D loss: (8.149)(R -45.498, F 61.796)]  [G loss: -61.668] \n",
      "3636 [D loss: (8.804)(R -45.941, F 63.549)]  [G loss: -61.126] \n",
      "3636 [D loss: (9.014)(R -46.036, F 64.063)]  [G loss: -60.763] \n",
      "3637 [D loss: (9.357)(R -44.811, F 63.526)]  [G loss: -61.140] \n",
      "3637 [D loss: (9.558)(R -44.015, F 63.130)]  [G loss: -61.216] \n",
      "3638 [D loss: (8.689)(R -44.037, F 61.414)]  [G loss: -59.680] \n",
      "3638 [D loss: (7.881)(R -44.768, F 60.531)]  [G loss: -60.672] \n",
      "3639 [D loss: (6.998)(R -45.539, F 59.536)]  [G loss: -59.340] \n",
      "3639 [D loss: (8.919)(R -43.945, F 61.784)]  [G loss: -59.973] \n",
      "3640 [D loss: (8.029)(R -45.097, F 61.155)]  [G loss: -58.762] \n",
      "3640 [D loss: (6.250)(R -42.908, F 55.408)]  [G loss: -58.537] \n",
      "3641 [D loss: (6.359)(R -43.218, F 55.935)]  [G loss: -60.500] \n",
      "3641 [D loss: (6.028)(R -45.363, F 57.419)]  [G loss: -60.577] \n",
      "3642 [D loss: (8.101)(R -44.013, F 60.215)]  [G loss: -58.152] \n",
      "3642 [D loss: (5.912)(R -42.146, F 53.970)]  [G loss: -57.798] \n",
      "3643 [D loss: (7.567)(R -43.090, F 58.224)]  [G loss: -57.250] \n",
      "3643 [D loss: (6.151)(R -41.256, F 53.558)]  [G loss: -56.878] \n",
      "3644 [D loss: (7.545)(R -42.884, F 57.975)]  [G loss: -57.819] \n",
      "3644 [D loss: (8.777)(R -43.093, F 60.647)]  [G loss: -58.237] \n",
      "3645 [D loss: (7.081)(R -42.499, F 56.661)]  [G loss: -56.845] \n",
      "3645 [D loss: (5.363)(R -42.064, F 52.791)]  [G loss: -56.482] \n",
      "3646 [D loss: (8.519)(R -42.116, F 59.153)]  [G loss: -56.166] \n",
      "3646 [D loss: (8.894)(R -40.134, F 57.922)]  [G loss: -55.915] \n",
      "3647 [D loss: (8.041)(R -41.464, F 57.546)]  [G loss: -55.475] \n",
      "3647 [D loss: (8.349)(R -41.717, F 58.415)]  [G loss: -56.015] \n",
      "3648 [D loss: (8.136)(R -41.934, F 58.206)]  [G loss: -57.677] \n",
      "3648 [D loss: (8.890)(R -42.116, F 59.897)]  [G loss: -54.827] \n",
      "3649 [D loss: (7.570)(R -41.722, F 56.863)]  [G loss: -55.817] \n",
      "3649 [D loss: (7.420)(R -41.659, F 56.500)]  [G loss: -55.727] \n",
      "3650 [D loss: (6.721)(R -40.206, F 53.649)]  [G loss: -55.927] \n",
      "3650 [D loss: (6.956)(R -41.516, F 55.428)]  [G loss: -55.950] \n",
      "3651 [D loss: (7.079)(R -40.283, F 54.442)]  [G loss: -54.439] \n",
      "3651 [D loss: (7.398)(R -40.118, F 54.915)]  [G loss: -55.026] \n",
      "3652 [D loss: (8.210)(R -38.270, F 54.691)]  [G loss: -55.909] \n",
      "3652 [D loss: (8.004)(R -39.737, F 55.744)]  [G loss: -55.286] \n",
      "3653 [D loss: (8.168)(R -39.459, F 55.795)]  [G loss: -53.558] \n",
      "3653 [D loss: (6.202)(R -40.480, F 52.884)]  [G loss: -53.173] \n",
      "3654 [D loss: (6.407)(R -40.313, F 53.127)]  [G loss: -54.003] \n",
      "3654 [D loss: (7.170)(R -39.679, F 54.018)]  [G loss: -52.928] \n",
      "3655 [D loss: (8.894)(R -39.901, F 57.688)]  [G loss: -53.639] \n",
      "3655 [D loss: (6.279)(R -38.515, F 51.073)]  [G loss: -53.471] \n",
      "3656 [D loss: (7.953)(R -39.606, F 55.513)]  [G loss: -54.152] \n",
      "3656 [D loss: (7.734)(R -39.010, F 54.479)]  [G loss: -53.137] \n",
      "3657 [D loss: (7.970)(R -38.069, F 54.010)]  [G loss: -52.023] \n",
      "3657 [D loss: (11.096)(R -36.489, F 58.680)]  [G loss: -52.547] \n",
      "3658 [D loss: (8.650)(R -37.802, F 55.103)]  [G loss: -52.207] \n",
      "3658 [D loss: (7.820)(R -37.481, F 53.120)]  [G loss: -51.829] \n",
      "3659 [D loss: (7.112)(R -38.794, F 53.017)]  [G loss: -51.583] \n",
      "3659 [D loss: (5.726)(R -38.415, F 49.868)]  [G loss: -52.338] \n",
      "3660 [D loss: (7.963)(R -37.606, F 53.531)]  [G loss: -50.662] \n",
      "3660 [D loss: (7.561)(R -38.250, F 53.372)]  [G loss: -51.854] \n",
      "3661 [D loss: (5.313)(R -37.964, F 48.590)]  [G loss: -52.155] \n",
      "3661 [D loss: (6.751)(R -36.899, F 50.402)]  [G loss: -50.120] \n",
      "3662 [D loss: (4.250)(R -37.493, F 45.993)]  [G loss: -51.039] \n",
      "3662 [D loss: (7.381)(R -37.308, F 52.069)]  [G loss: -49.721] \n",
      "3663 [D loss: (8.614)(R -36.233, F 53.461)]  [G loss: -51.114] \n",
      "3663 [D loss: (5.889)(R -37.995, F 49.772)]  [G loss: -51.116] \n",
      "3664 [D loss: (5.573)(R -36.315, F 47.462)]  [G loss: -50.614] \n",
      "3664 [D loss: (5.457)(R -37.026, F 47.939)]  [G loss: -49.541] \n",
      "3665 [D loss: (7.996)(R -35.153, F 51.145)]  [G loss: -50.373] \n",
      "3665 [D loss: (7.187)(R -36.919, F 51.292)]  [G loss: -49.490] \n",
      "3666 [D loss: (6.882)(R -35.901, F 49.665)]  [G loss: -51.449] \n",
      "3666 [D loss: (8.113)(R -36.273, F 52.498)]  [G loss: -48.913] \n",
      "3667 [D loss: (8.170)(R -35.449, F 51.790)]  [G loss: -49.413] \n",
      "3667 [D loss: (7.077)(R -35.278, F 49.431)]  [G loss: -48.925] \n",
      "3668 [D loss: (7.552)(R -34.758, F 49.861)]  [G loss: -49.250] \n",
      "3668 [D loss: (6.827)(R -36.516, F 50.170)]  [G loss: -47.956] \n",
      "3669 [D loss: (5.959)(R -35.580, F 47.498)]  [G loss: -48.180] \n",
      "3669 [D loss: (3.825)(R -36.331, F 43.981)]  [G loss: -47.531] \n",
      "3670 [D loss: (5.974)(R -35.257, F 47.206)]  [G loss: -48.265] \n",
      "3670 [D loss: (7.789)(R -34.010, F 49.588)]  [G loss: -47.625] \n",
      "3671 [D loss: (6.493)(R -35.180, F 48.166)]  [G loss: -48.285] \n",
      "3671 [D loss: (5.746)(R -35.310, F 46.802)]  [G loss: -46.482] \n",
      "3672 [D loss: (6.212)(R -34.665, F 47.089)]  [G loss: -46.680] \n",
      "3672 [D loss: (6.411)(R -34.711, F 47.532)]  [G loss: -46.390] \n",
      "3673 [D loss: (7.025)(R -34.101, F 48.150)]  [G loss: -47.341] \n",
      "3673 [D loss: (5.895)(R -34.050, F 45.840)]  [G loss: -46.679] \n",
      "3674 [D loss: (5.622)(R -33.618, F 44.862)]  [G loss: -46.221] \n",
      "3674 [D loss: (7.070)(R -34.751, F 48.892)]  [G loss: -46.860] \n",
      "3675 [D loss: (6.587)(R -32.947, F 46.120)]  [G loss: -46.185] \n",
      "3675 [D loss: (7.009)(R -33.203, F 47.221)]  [G loss: -45.704] \n",
      "3676 [D loss: (8.054)(R -34.331, F 50.440)]  [G loss: -45.402] \n",
      "3676 [D loss: (5.632)(R -33.885, F 45.148)]  [G loss: -45.345] \n",
      "3677 [D loss: (6.477)(R -32.399, F 45.353)]  [G loss: -44.957] \n",
      "3677 [D loss: (6.759)(R -33.226, F 46.743)]  [G loss: -45.216] \n",
      "3678 [D loss: (5.559)(R -33.159, F 44.277)]  [G loss: -45.586] \n",
      "3678 [D loss: (7.088)(R -31.837, F 46.013)]  [G loss: -44.184] \n",
      "3679 [D loss: (3.927)(R -31.987, F 39.840)]  [G loss: -43.065] \n",
      "3679 [D loss: (5.324)(R -33.452, F 44.100)]  [G loss: -44.335] \n",
      "3680 [D loss: (4.574)(R -33.326, F 42.474)]  [G loss: -44.585] \n",
      "3680 [D loss: (5.887)(R -32.053, F 43.828)]  [G loss: -44.808] \n",
      "3681 [D loss: (5.422)(R -32.888, F 43.732)]  [G loss: -44.192] \n",
      "3681 [D loss: (6.612)(R -31.526, F 44.750)]  [G loss: -44.277] \n",
      "3682 [D loss: (6.522)(R -33.297, F 46.341)]  [G loss: -44.469] \n",
      "3682 [D loss: (5.759)(R -30.905, F 42.423)]  [G loss: -43.389] \n",
      "3683 [D loss: (4.337)(R -32.321, F 40.995)]  [G loss: -42.910] \n",
      "3683 [D loss: (5.747)(R -31.090, F 42.585)]  [G loss: -43.719] \n",
      "3684 [D loss: (6.923)(R -30.575, F 44.421)]  [G loss: -42.610] \n",
      "3684 [D loss: (5.606)(R -30.720, F 41.933)]  [G loss: -42.656] \n",
      "3685 [D loss: (5.380)(R -31.557, F 42.317)]  [G loss: -42.890] \n",
      "3685 [D loss: (6.715)(R -31.542, F 44.973)]  [G loss: -42.442] \n",
      "3686 [D loss: (6.845)(R -31.440, F 45.130)]  [G loss: -41.912] \n",
      "3686 [D loss: (6.503)(R -30.703, F 43.709)]  [G loss: -42.692] \n",
      "3687 [D loss: (6.316)(R -29.233, F 41.865)]  [G loss: -42.300] \n",
      "3687 [D loss: (4.924)(R -31.300, F 41.148)]  [G loss: -42.202] \n",
      "3688 [D loss: (5.297)(R -31.041, F 41.634)]  [G loss: -41.636] \n",
      "3688 [D loss: (6.805)(R -30.017, F 43.626)]  [G loss: -42.354] \n",
      "3689 [D loss: (6.036)(R -30.092, F 42.164)]  [G loss: -41.613] \n",
      "3689 [D loss: (5.047)(R -29.362, F 39.455)]  [G loss: -41.625] \n",
      "3690 [D loss: (5.174)(R -30.392, F 40.741)]  [G loss: -41.968] \n",
      "3690 [D loss: (5.788)(R -29.951, F 41.526)]  [G loss: -40.039] \n",
      "3691 [D loss: (6.197)(R -29.794, F 42.189)]  [G loss: -41.975] \n",
      "3691 [D loss: (4.533)(R -30.169, F 39.234)]  [G loss: -41.755] \n",
      "3692 [D loss: (4.344)(R -29.667, F 38.355)]  [G loss: -41.121] \n",
      "3692 [D loss: (5.117)(R -30.358, F 40.593)]  [G loss: -40.067] \n",
      "3693 [D loss: (5.590)(R -30.092, F 41.272)]  [G loss: -40.759] \n",
      "3693 [D loss: (5.619)(R -30.692, F 41.931)]  [G loss: -40.731] \n",
      "3694 [D loss: (5.235)(R -29.411, F 39.882)]  [G loss: -40.744] \n",
      "3694 [D loss: (6.192)(R -30.287, F 42.671)]  [G loss: -39.641] \n",
      "3695 [D loss: (6.058)(R -30.771, F 42.886)]  [G loss: -38.682] \n",
      "3695 [D loss: (5.811)(R -27.473, F 39.095)]  [G loss: -39.663] \n",
      "3696 [D loss: (5.257)(R -28.927, F 39.442)]  [G loss: -39.891] \n",
      "3696 [D loss: (6.319)(R -27.764, F 40.402)]  [G loss: -39.576] \n",
      "3697 [D loss: (6.082)(R -27.953, F 40.118)]  [G loss: -38.209] \n",
      "3697 [D loss: (5.886)(R -27.149, F 38.920)]  [G loss: -38.458] \n",
      "3698 [D loss: (3.631)(R -28.913, F 36.175)]  [G loss: -39.330] \n",
      "3698 [D loss: (4.547)(R -27.267, F 36.360)]  [G loss: -37.899] \n",
      "3699 [D loss: (5.862)(R -27.269, F 38.993)]  [G loss: -38.644] \n",
      "3699 [D loss: (4.589)(R -27.828, F 37.006)]  [G loss: -37.884] \n",
      "3700 [D loss: (5.361)(R -27.731, F 38.452)]  [G loss: -38.233] \n",
      "3700 [D loss: (7.514)(R -27.693, F 42.722)]  [G loss: -38.202] \n",
      "3701 [D loss: (5.928)(R -25.943, F 37.799)]  [G loss: -38.581] \n",
      "3701 [D loss: (4.249)(R -28.703, F 37.201)]  [G loss: -37.592] \n",
      "3702 [D loss: (6.562)(R -27.373, F 40.496)]  [G loss: -37.963] \n",
      "3702 [D loss: (4.347)(R -27.291, F 35.984)]  [G loss: -37.524] \n",
      "3703 [D loss: (5.379)(R -27.540, F 38.299)]  [G loss: -36.566] \n",
      "3703 [D loss: (5.673)(R -26.509, F 37.854)]  [G loss: -37.093] \n",
      "3704 [D loss: (4.933)(R -27.058, F 36.924)]  [G loss: -35.720] \n",
      "3704 [D loss: (4.791)(R -26.177, F 35.760)]  [G loss: -36.856] \n",
      "3705 [D loss: (4.582)(R -26.685, F 35.849)]  [G loss: -36.927] \n",
      "3705 [D loss: (4.562)(R -27.208, F 36.332)]  [G loss: -36.362] \n",
      "3706 [D loss: (5.770)(R -26.121, F 37.662)]  [G loss: -36.206] \n",
      "3706 [D loss: (5.141)(R -26.847, F 37.129)]  [G loss: -36.175] \n",
      "3707 [D loss: (4.449)(R -25.652, F 34.551)]  [G loss: -35.537] \n",
      "3707 [D loss: (4.816)(R -26.053, F 35.685)]  [G loss: -35.591] \n",
      "3708 [D loss: (5.671)(R -25.566, F 36.909)]  [G loss: -36.022] \n",
      "3708 [D loss: (4.393)(R -25.600, F 34.386)]  [G loss: -35.406] \n",
      "3709 [D loss: (5.314)(R -25.907, F 36.535)]  [G loss: -35.305] \n",
      "3709 [D loss: (3.812)(R -26.262, F 33.887)]  [G loss: -34.965] \n",
      "3710 [D loss: (3.707)(R -25.663, F 33.077)]  [G loss: -34.197] \n",
      "3710 [D loss: (4.199)(R -25.489, F 33.888)]  [G loss: -35.157] \n",
      "3711 [D loss: (5.129)(R -24.758, F 35.015)]  [G loss: -34.664] \n",
      "3711 [D loss: (5.013)(R -25.310, F 35.337)]  [G loss: -35.324] \n",
      "3712 [D loss: (4.414)(R -25.510, F 34.337)]  [G loss: -34.799] \n",
      "3712 [D loss: (4.632)(R -25.030, F 34.294)]  [G loss: -34.644] \n",
      "3713 [D loss: (5.971)(R -24.987, F 36.929)]  [G loss: -35.202] \n",
      "3713 [D loss: (4.384)(R -25.049, F 33.817)]  [G loss: -34.036] \n",
      "3714 [D loss: (4.754)(R -25.438, F 34.946)]  [G loss: -34.365] \n",
      "3714 [D loss: (4.520)(R -24.318, F 33.357)]  [G loss: -34.081] \n",
      "3715 [D loss: (4.039)(R -24.335, F 32.413)]  [G loss: -33.218] \n",
      "3715 [D loss: (5.485)(R -23.486, F 34.456)]  [G loss: -33.467] \n",
      "3716 [D loss: (4.180)(R -24.777, F 33.137)]  [G loss: -34.646] \n",
      "3716 [D loss: (3.957)(R -24.621, F 32.535)]  [G loss: -33.464] \n",
      "3717 [D loss: (5.040)(R -23.807, F 33.887)]  [G loss: -32.839] \n",
      "3717 [D loss: (4.356)(R -24.127, F 32.838)]  [G loss: -33.316] \n",
      "3718 [D loss: (5.799)(R -22.897, F 34.496)]  [G loss: -34.109] \n",
      "3718 [D loss: (4.646)(R -23.207, F 32.500)]  [G loss: -32.794] \n",
      "3719 [D loss: (5.175)(R -23.408, F 33.759)]  [G loss: -33.214] \n",
      "3719 [D loss: (4.403)(R -24.351, F 33.156)]  [G loss: -32.722] \n",
      "3720 [D loss: (3.671)(R -24.444, F 31.787)]  [G loss: -32.221] \n",
      "3720 [D loss: (4.021)(R -22.686, F 30.728)]  [G loss: -32.382] \n",
      "3721 [D loss: (4.263)(R -23.541, F 32.068)]  [G loss: -31.557] \n",
      "3721 [D loss: (4.920)(R -24.345, F 34.186)]  [G loss: -32.387] \n",
      "3722 [D loss: (4.401)(R -23.412, F 32.214)]  [G loss: -32.429] \n",
      "3722 [D loss: (4.822)(R -23.060, F 32.704)]  [G loss: -31.899] \n",
      "3723 [D loss: (4.426)(R -22.664, F 31.516)]  [G loss: -31.710] \n",
      "3723 [D loss: (3.870)(R -23.288, F 31.028)]  [G loss: -31.922] \n",
      "3724 [D loss: (4.384)(R -22.668, F 31.437)]  [G loss: -31.298] \n",
      "3724 [D loss: (4.192)(R -23.024, F 31.409)]  [G loss: -31.581] \n",
      "3725 [D loss: (4.899)(R -22.069, F 31.867)]  [G loss: -31.683] \n",
      "3725 [D loss: (4.225)(R -22.554, F 31.003)]  [G loss: -31.945] \n",
      "3726 [D loss: (4.319)(R -22.531, F 31.169)]  [G loss: -31.672] \n",
      "3726 [D loss: (5.147)(R -22.385, F 32.679)]  [G loss: -31.040] \n",
      "3727 [D loss: (5.434)(R -22.442, F 33.309)]  [G loss: -32.050] \n",
      "3727 [D loss: (4.573)(R -22.117, F 31.264)]  [G loss: -31.398] \n",
      "3728 [D loss: (4.181)(R -22.136, F 30.498)]  [G loss: -31.148] \n",
      "3728 [D loss: (4.539)(R -21.513, F 30.592)]  [G loss: -30.654] \n",
      "3729 [D loss: (5.102)(R -21.213, F 31.417)]  [G loss: -30.273] \n",
      "3729 [D loss: (4.379)(R -22.265, F 31.023)]  [G loss: -30.351] \n",
      "3730 [D loss: (4.053)(R -22.035, F 30.140)]  [G loss: -30.083] \n",
      "3730 [D loss: (4.896)(R -21.842, F 31.634)]  [G loss: -30.264] \n",
      "3731 [D loss: (4.988)(R -21.813, F 31.789)]  [G loss: -29.886] \n",
      "3731 [D loss: (4.164)(R -21.338, F 29.666)]  [G loss: -29.868] \n",
      "3732 [D loss: (3.975)(R -21.569, F 29.520)]  [G loss: -29.330] \n",
      "3732 [D loss: (3.849)(R -21.258, F 28.956)]  [G loss: -29.238] \n",
      "3733 [D loss: (4.693)(R -20.599, F 29.986)]  [G loss: -29.415] \n",
      "3733 [D loss: (6.003)(R -20.433, F 32.440)]  [G loss: -29.622] \n",
      "3734 [D loss: (3.795)(R -20.764, F 28.354)]  [G loss: -29.886] \n",
      "3734 [D loss: (4.269)(R -20.458, F 28.995)]  [G loss: -28.780] \n",
      "3735 [D loss: (4.039)(R -20.682, F 28.761)]  [G loss: -29.121] \n",
      "3735 [D loss: (3.685)(R -19.825, F 27.196)]  [G loss: -28.759] \n",
      "3736 [D loss: (4.415)(R -21.096, F 29.927)]  [G loss: -28.344] \n",
      "3736 [D loss: (4.152)(R -20.270, F 28.575)]  [G loss: -28.619] \n",
      "3737 [D loss: (5.999)(R -20.299, F 32.298)]  [G loss: -28.249] \n",
      "3737 [D loss: (3.677)(R -20.200, F 27.554)]  [G loss: -28.232] \n",
      "3738 [D loss: (3.156)(R -20.287, F 26.598)]  [G loss: -28.144] \n",
      "3738 [D loss: (4.196)(R -20.955, F 29.346)]  [G loss: -27.926] \n",
      "3739 [D loss: (3.820)(R -20.052, F 27.691)]  [G loss: -27.417] \n",
      "3739 [D loss: (4.995)(R -19.844, F 29.833)]  [G loss: -27.539] \n",
      "3740 [D loss: (4.074)(R -19.563, F 27.710)]  [G loss: -27.926] \n",
      "3740 [D loss: (3.642)(R -19.541, F 26.826)]  [G loss: -27.427] \n",
      "3741 [D loss: (3.487)(R -19.911, F 26.885)]  [G loss: -27.367] \n",
      "3741 [D loss: (3.683)(R -19.401, F 26.766)]  [G loss: -27.186] \n",
      "3742 [D loss: (2.904)(R -19.114, F 24.922)]  [G loss: -26.968] \n",
      "3742 [D loss: (3.373)(R -19.233, F 25.980)]  [G loss: -26.819] \n",
      "3743 [D loss: (4.017)(R -19.539, F 27.574)]  [G loss: -26.762] \n",
      "3743 [D loss: (3.909)(R -18.469, F 26.286)]  [G loss: -27.445] \n",
      "3744 [D loss: (2.917)(R -19.775, F 25.609)]  [G loss: -26.936] \n",
      "3744 [D loss: (4.306)(R -18.791, F 27.403)]  [G loss: -26.902] \n",
      "3745 [D loss: (3.455)(R -19.179, F 26.089)]  [G loss: -26.604] \n",
      "3745 [D loss: (4.143)(R -18.195, F 26.482)]  [G loss: -26.859] \n",
      "3746 [D loss: (4.253)(R -18.531, F 27.037)]  [G loss: -26.213] \n",
      "3746 [D loss: (4.143)(R -19.074, F 27.359)]  [G loss: -25.998] \n",
      "3747 [D loss: (3.192)(R -19.076, F 25.460)]  [G loss: -26.343] \n",
      "3747 [D loss: (4.800)(R -18.459, F 28.059)]  [G loss: -26.226] \n",
      "3748 [D loss: (5.495)(R -18.342, F 29.332)]  [G loss: -25.906] \n",
      "3748 [D loss: (2.589)(R -18.607, F 23.786)]  [G loss: -25.968] \n",
      "3749 [D loss: (4.271)(R -18.412, F 26.954)]  [G loss: -25.939] \n",
      "3749 [D loss: (3.608)(R -18.470, F 25.686)]  [G loss: -26.159] \n",
      "3750 [D loss: (4.137)(R -17.708, F 25.981)]  [G loss: -25.776] \n",
      "3750 [D loss: (3.629)(R -17.386, F 24.643)]  [G loss: -25.505] \n",
      "3751 [D loss: (3.720)(R -17.834, F 25.274)]  [G loss: -25.211] \n",
      "3751 [D loss: (3.281)(R -17.688, F 24.250)]  [G loss: -24.977] \n",
      "3752 [D loss: (3.659)(R -17.203, F 24.521)]  [G loss: -25.053] \n",
      "3752 [D loss: (3.000)(R -18.382, F 24.381)]  [G loss: -25.653] \n",
      "3753 [D loss: (4.263)(R -17.563, F 26.089)]  [G loss: -25.385] \n",
      "3753 [D loss: (4.004)(R -17.106, F 25.113)]  [G loss: -25.054] \n",
      "3754 [D loss: (4.063)(R -17.547, F 25.673)]  [G loss: -24.169] \n",
      "3754 [D loss: (3.776)(R -18.233, F 25.785)]  [G loss: -24.391] \n",
      "3755 [D loss: (3.390)(R -17.081, F 23.862)]  [G loss: -24.651] \n",
      "3755 [D loss: (3.329)(R -18.040, F 24.699)]  [G loss: -24.212] \n",
      "3756 [D loss: (3.816)(R -17.071, F 24.703)]  [G loss: -24.723] \n",
      "3756 [D loss: (3.684)(R -17.380, F 24.747)]  [G loss: -23.995] \n",
      "3757 [D loss: (3.757)(R -16.963, F 24.477)]  [G loss: -24.167] \n",
      "3757 [D loss: (3.817)(R -17.429, F 25.063)]  [G loss: -23.537] \n",
      "3758 [D loss: (3.049)(R -16.834, F 22.931)]  [G loss: -23.963] \n",
      "3758 [D loss: (3.549)(R -16.680, F 23.778)]  [G loss: -23.750] \n",
      "3759 [D loss: (3.659)(R -16.784, F 24.103)]  [G loss: -23.479] \n",
      "3759 [D loss: (3.075)(R -17.046, F 23.196)]  [G loss: -23.509] \n",
      "3760 [D loss: (4.032)(R -16.808, F 24.872)]  [G loss: -23.578] \n",
      "3760 [D loss: (4.019)(R -16.750, F 24.789)]  [G loss: -23.274] \n",
      "3761 [D loss: (3.614)(R -16.490, F 23.718)]  [G loss: -23.814] \n",
      "3761 [D loss: (3.878)(R -16.358, F 24.114)]  [G loss: -23.408] \n",
      "3762 [D loss: (4.570)(R -15.736, F 24.876)]  [G loss: -23.627] \n",
      "3762 [D loss: (2.635)(R -16.450, F 21.719)]  [G loss: -23.170] \n",
      "3763 [D loss: (3.645)(R -16.139, F 23.430)]  [G loss: -23.099] \n",
      "3763 [D loss: (3.377)(R -16.553, F 23.306)]  [G loss: -22.731] \n",
      "3764 [D loss: (3.728)(R -16.138, F 23.594)]  [G loss: -22.926] \n",
      "3764 [D loss: (2.537)(R -16.156, F 21.229)]  [G loss: -22.934] \n",
      "3765 [D loss: (2.360)(R -16.819, F 21.539)]  [G loss: -22.785] \n",
      "3765 [D loss: (3.323)(R -15.317, F 21.963)]  [G loss: -22.378] \n",
      "3766 [D loss: (3.446)(R -15.998, F 22.890)]  [G loss: -22.545] \n",
      "3766 [D loss: (3.094)(R -16.094, F 22.281)]  [G loss: -22.559] \n",
      "3767 [D loss: (3.456)(R -15.943, F 22.856)]  [G loss: -22.123] \n",
      "3767 [D loss: (3.246)(R -15.516, F 22.008)]  [G loss: -22.056] \n",
      "3768 [D loss: (3.167)(R -15.538, F 21.872)]  [G loss: -22.043] \n",
      "3768 [D loss: (3.526)(R -15.144, F 22.196)]  [G loss: -22.429] \n",
      "3769 [D loss: (3.205)(R -15.776, F 22.187)]  [G loss: -21.531] \n",
      "3769 [D loss: (3.944)(R -15.215, F 23.103)]  [G loss: -21.680] \n",
      "3770 [D loss: (2.849)(R -15.573, F 21.272)]  [G loss: -21.459] \n",
      "3770 [D loss: (3.595)(R -15.209, F 22.399)]  [G loss: -21.527] \n",
      "3771 [D loss: (3.381)(R -15.370, F 22.133)]  [G loss: -21.549] \n",
      "3771 [D loss: (3.736)(R -14.909, F 22.380)]  [G loss: -21.107] \n",
      "3772 [D loss: (3.943)(R -15.049, F 22.935)]  [G loss: -21.524] \n",
      "3772 [D loss: (3.319)(R -15.419, F 22.057)]  [G loss: -21.161] \n",
      "3773 [D loss: (2.716)(R -15.178, F 20.611)]  [G loss: -21.194] \n",
      "3773 [D loss: (2.450)(R -15.376, F 20.275)]  [G loss: -21.191] \n",
      "3774 [D loss: (3.705)(R -14.443, F 21.852)]  [G loss: -20.692] \n",
      "3774 [D loss: (2.240)(R -14.699, F 19.180)]  [G loss: -20.812] \n",
      "3775 [D loss: (3.316)(R -14.672, F 21.304)]  [G loss: -21.085] \n",
      "3775 [D loss: (2.521)(R -14.559, F 19.601)]  [G loss: -20.348] \n",
      "3776 [D loss: (3.776)(R -14.803, F 22.355)]  [G loss: -20.588] \n",
      "3776 [D loss: (3.337)(R -14.572, F 21.247)]  [G loss: -20.401] \n",
      "3777 [D loss: (3.002)(R -14.414, F 20.419)]  [G loss: -20.516] \n",
      "3777 [D loss: (3.329)(R -13.983, F 20.640)]  [G loss: -20.607] \n",
      "3778 [D loss: (2.779)(R -14.563, F 20.121)]  [G loss: -20.207] \n",
      "3778 [D loss: (2.825)(R -14.620, F 20.270)]  [G loss: -20.222] \n",
      "3779 [D loss: (2.956)(R -14.319, F 20.232)]  [G loss: -19.971] \n",
      "3779 [D loss: (3.158)(R -14.249, F 20.564)]  [G loss: -20.497] \n",
      "3780 [D loss: (2.874)(R -13.626, F 19.374)]  [G loss: -19.657] \n",
      "3780 [D loss: (2.550)(R -14.666, F 19.766)]  [G loss: -19.799] \n",
      "3781 [D loss: (2.502)(R -14.412, F 19.416)]  [G loss: -19.962] \n",
      "3781 [D loss: (2.573)(R -13.892, F 19.037)]  [G loss: -19.808] \n",
      "3782 [D loss: (2.294)(R -13.665, F 18.253)]  [G loss: -19.437] \n",
      "3782 [D loss: (3.189)(R -13.653, F 20.032)]  [G loss: -19.984] \n",
      "3783 [D loss: (3.157)(R -14.096, F 20.410)]  [G loss: -19.543] \n",
      "3783 [D loss: (3.025)(R -13.652, F 19.702)]  [G loss: -19.389] \n",
      "3784 [D loss: (2.754)(R -13.519, F 19.027)]  [G loss: -19.320] \n",
      "3784 [D loss: (2.705)(R -13.153, F 18.562)]  [G loss: -18.933] \n",
      "3785 [D loss: (3.198)(R -13.036, F 19.432)]  [G loss: -19.239] \n",
      "3785 [D loss: (2.830)(R -13.859, F 19.519)]  [G loss: -19.001] \n",
      "3786 [D loss: (2.308)(R -13.762, F 18.378)]  [G loss: -19.151] \n",
      "3786 [D loss: (2.220)(R -13.383, F 17.823)]  [G loss: -18.793] \n",
      "3787 [D loss: (2.497)(R -13.349, F 18.343)]  [G loss: -19.141] \n",
      "3787 [D loss: (2.737)(R -13.084, F 18.557)]  [G loss: -18.714] \n",
      "3788 [D loss: (2.808)(R -13.292, F 18.907)]  [G loss: -18.378] \n",
      "3788 [D loss: (3.267)(R -13.144, F 19.679)]  [G loss: -18.648] \n",
      "3789 [D loss: (3.132)(R -12.678, F 18.943)]  [G loss: -18.816] \n",
      "3789 [D loss: (2.877)(R -12.923, F 18.677)]  [G loss: -18.076] \n",
      "3790 [D loss: (2.244)(R -12.554, F 17.043)]  [G loss: -18.206] \n",
      "3790 [D loss: (2.613)(R -12.829, F 18.055)]  [G loss: -18.754] \n",
      "3791 [D loss: (3.386)(R -12.534, F 19.306)]  [G loss: -18.211] \n",
      "3791 [D loss: (2.271)(R -12.850, F 17.392)]  [G loss: -17.891] \n",
      "3792 [D loss: (2.382)(R -12.217, F 16.980)]  [G loss: -18.170] \n",
      "3792 [D loss: (3.484)(R -12.223, F 19.191)]  [G loss: -18.030] \n",
      "3793 [D loss: (2.178)(R -12.770, F 17.127)]  [G loss: -17.864] \n",
      "3793 [D loss: (1.997)(R -12.953, F 16.947)]  [G loss: -17.946] \n",
      "3794 [D loss: (2.604)(R -12.610, F 17.818)]  [G loss: -17.445] \n",
      "3794 [D loss: (2.236)(R -12.219, F 16.690)]  [G loss: -17.690] \n",
      "3795 [D loss: (2.559)(R -12.241, F 17.360)]  [G loss: -17.462] \n",
      "3795 [D loss: (2.707)(R -12.426, F 17.840)]  [G loss: -17.436] \n",
      "3796 [D loss: (2.633)(R -12.420, F 17.686)]  [G loss: -17.780] \n",
      "3796 [D loss: (2.563)(R -12.051, F 17.177)]  [G loss: -17.240] \n",
      "3797 [D loss: (2.513)(R -12.105, F 17.131)]  [G loss: -17.169] \n",
      "3797 [D loss: (2.520)(R -11.673, F 16.712)]  [G loss: -17.080] \n",
      "3798 [D loss: (2.466)(R -12.042, F 16.974)]  [G loss: -16.961] \n",
      "3798 [D loss: (1.892)(R -11.964, F 15.749)]  [G loss: -17.499] \n",
      "3799 [D loss: (2.237)(R -12.066, F 16.539)]  [G loss: -16.905] \n",
      "3799 [D loss: (2.029)(R -12.027, F 16.084)]  [G loss: -17.019] \n",
      "3800 [D loss: (2.293)(R -11.971, F 16.558)]  [G loss: -17.165] \n",
      "3800 [D loss: (3.006)(R -11.870, F 17.883)]  [G loss: -16.842] \n",
      "3801 [D loss: (2.296)(R -11.874, F 16.467)]  [G loss: -16.555] \n",
      "3801 [D loss: (2.335)(R -11.663, F 16.332)]  [G loss: -16.636] \n",
      "3802 [D loss: (2.268)(R -11.627, F 16.162)]  [G loss: -16.605] \n",
      "3802 [D loss: (2.439)(R -11.556, F 16.434)]  [G loss: -16.533] \n",
      "3803 [D loss: (2.707)(R -11.708, F 17.122)]  [G loss: -16.755] \n",
      "3803 [D loss: (2.163)(R -11.714, F 16.039)]  [G loss: -16.296] \n",
      "3804 [D loss: (2.207)(R -11.131, F 15.544)]  [G loss: -16.272] \n",
      "3804 [D loss: (2.050)(R -11.399, F 15.500)]  [G loss: -16.218] \n",
      "3805 [D loss: (2.782)(R -10.879, F 16.443)]  [G loss: -16.278] \n",
      "3805 [D loss: (2.091)(R -11.474, F 15.656)]  [G loss: -16.018] \n",
      "3806 [D loss: (3.045)(R -10.939, F 17.030)]  [G loss: -15.999] \n",
      "3806 [D loss: (2.822)(R -10.800, F 16.445)]  [G loss: -16.121] \n",
      "3807 [D loss: (2.544)(R -10.774, F 15.862)]  [G loss: -15.881] \n",
      "3807 [D loss: (2.194)(R -11.453, F 15.841)]  [G loss: -15.922] \n",
      "3808 [D loss: (2.611)(R -11.089, F 16.311)]  [G loss: -16.128] \n",
      "3808 [D loss: (2.137)(R -11.291, F 15.564)]  [G loss: -15.593] \n",
      "3809 [D loss: (2.348)(R -10.879, F 15.575)]  [G loss: -15.663] \n",
      "3809 [D loss: (2.056)(R -10.950, F 15.062)]  [G loss: -15.533] \n",
      "3810 [D loss: (2.371)(R -10.866, F 15.608)]  [G loss: -15.283] \n",
      "3810 [D loss: (2.478)(R -10.722, F 15.678)]  [G loss: -15.507] \n",
      "3811 [D loss: (2.108)(R -10.831, F 15.047)]  [G loss: -15.391] \n",
      "3811 [D loss: (1.882)(R -11.018, F 14.781)]  [G loss: -15.095] \n",
      "3812 [D loss: (2.412)(R -10.665, F 15.490)]  [G loss: -15.159] \n",
      "3812 [D loss: (2.103)(R -11.118, F 15.323)]  [G loss: -15.190] \n",
      "3813 [D loss: (1.639)(R -10.814, F 14.091)]  [G loss: -14.895] \n",
      "3813 [D loss: (2.081)(R -10.621, F 14.783)]  [G loss: -15.123] \n",
      "3814 [D loss: (2.548)(R -10.384, F 15.481)]  [G loss: -15.018] \n",
      "3814 [D loss: (2.446)(R -10.384, F 15.275)]  [G loss: -14.749] \n",
      "3815 [D loss: (2.297)(R -10.329, F 14.922)]  [G loss: -15.000] \n",
      "3815 [D loss: (1.907)(R -10.440, F 14.255)]  [G loss: -14.881] \n",
      "3816 [D loss: (2.044)(R -10.424, F 14.512)]  [G loss: -14.513] \n",
      "3816 [D loss: (2.166)(R -10.350, F 14.682)]  [G loss: -14.474] \n",
      "3817 [D loss: (2.508)(R -9.895, F 14.912)]  [G loss: -14.431] \n",
      "3817 [D loss: (2.225)(R -10.175, F 14.625)]  [G loss: -14.596] \n",
      "3818 [D loss: (2.525)(R -9.957, F 15.007)]  [G loss: -14.116] \n",
      "3818 [D loss: (2.047)(R -9.857, F 13.951)]  [G loss: -14.205] \n",
      "3819 [D loss: (2.050)(R -9.788, F 13.888)]  [G loss: -14.531] \n",
      "3819 [D loss: (2.113)(R -9.815, F 14.042)]  [G loss: -14.038] \n",
      "3820 [D loss: (2.178)(R -9.894, F 14.250)]  [G loss: -14.296] \n",
      "3820 [D loss: (2.380)(R -9.997, F 14.756)]  [G loss: -14.049] \n",
      "3821 [D loss: (1.975)(R -9.974, F 13.924)]  [G loss: -14.281] \n",
      "3821 [D loss: (1.564)(R -9.869, F 12.997)]  [G loss: -14.083] \n",
      "3822 [D loss: (2.105)(R -9.832, F 14.042)]  [G loss: -13.895] \n",
      "3822 [D loss: (2.175)(R -9.722, F 14.073)]  [G loss: -13.926] \n",
      "3823 [D loss: (2.130)(R -9.093, F 13.353)]  [G loss: -13.693] \n",
      "3823 [D loss: (1.525)(R -10.174, F 13.224)]  [G loss: -14.065] \n",
      "3824 [D loss: (1.965)(R -9.601, F 13.531)]  [G loss: -13.759] \n",
      "3824 [D loss: (1.921)(R -9.212, F 13.054)]  [G loss: -13.752] \n",
      "3825 [D loss: (2.110)(R -9.694, F 13.915)]  [G loss: -13.202] \n",
      "3825 [D loss: (2.147)(R -9.527, F 13.822)]  [G loss: -13.616] \n",
      "3826 [D loss: (1.961)(R -9.070, F 12.991)]  [G loss: -13.503] \n",
      "3826 [D loss: (2.014)(R -9.203, F 13.232)]  [G loss: -13.509] \n",
      "3827 [D loss: (1.571)(R -9.434, F 12.575)]  [G loss: -13.127] \n",
      "3827 [D loss: (1.876)(R -9.521, F 13.273)]  [G loss: -13.471] \n",
      "3828 [D loss: (1.882)(R -9.335, F 13.098)]  [G loss: -13.196] \n",
      "3828 [D loss: (1.840)(R -9.097, F 12.777)]  [G loss: -13.393] \n",
      "3829 [D loss: (2.077)(R -9.166, F 13.319)]  [G loss: -13.291] \n",
      "3829 [D loss: (2.208)(R -9.052, F 13.468)]  [G loss: -12.893] \n",
      "3830 [D loss: (2.105)(R -8.854, F 13.065)]  [G loss: -13.110] \n",
      "3830 [D loss: (1.924)(R -8.967, F 12.814)]  [G loss: -13.128] \n",
      "3831 [D loss: (1.957)(R -9.070, F 12.984)]  [G loss: -12.871] \n",
      "3831 [D loss: (1.871)(R -9.252, F 12.994)]  [G loss: -12.788] \n",
      "3832 [D loss: (1.739)(R -8.791, F 12.268)]  [G loss: -12.954] \n",
      "3832 [D loss: (2.009)(R -9.032, F 13.051)]  [G loss: -12.715] \n",
      "3833 [D loss: (2.207)(R -8.355, F 12.769)]  [G loss: -12.572] \n",
      "3833 [D loss: (1.732)(R -8.887, F 12.351)]  [G loss: -12.584] \n",
      "3834 [D loss: (1.755)(R -8.765, F 12.275)]  [G loss: -12.585] \n",
      "3834 [D loss: (1.915)(R -8.803, F 12.634)]  [G loss: -12.196] \n",
      "3835 [D loss: (1.813)(R -8.900, F 12.527)]  [G loss: -12.194] \n",
      "3835 [D loss: (1.795)(R -8.396, F 11.985)]  [G loss: -12.223] \n",
      "3836 [D loss: (1.818)(R -8.673, F 12.310)]  [G loss: -12.305] \n",
      "3836 [D loss: (1.976)(R -8.847, F 12.799)]  [G loss: -12.229] \n",
      "3837 [D loss: (2.507)(R -8.260, F 13.275)]  [G loss: -12.136] \n",
      "3837 [D loss: (1.790)(R -8.210, F 11.790)]  [G loss: -12.022] \n",
      "3838 [D loss: (1.860)(R -8.605, F 12.325)]  [G loss: -11.863] \n",
      "3838 [D loss: (1.935)(R -8.444, F 12.315)]  [G loss: -11.947] \n",
      "3839 [D loss: (1.830)(R -8.604, F 12.263)]  [G loss: -11.912] \n",
      "3839 [D loss: (1.894)(R -8.117, F 11.904)]  [G loss: -12.043] \n",
      "3840 [D loss: (1.539)(R -8.259, F 11.337)]  [G loss: -11.733] \n",
      "3840 [D loss: (1.651)(R -8.321, F 11.623)]  [G loss: -11.847] \n",
      "3841 [D loss: (2.197)(R -8.157, F 12.551)]  [G loss: -11.884] \n",
      "3841 [D loss: (1.852)(R -8.209, F 11.914)]  [G loss: -11.676] \n",
      "3842 [D loss: (1.947)(R -8.385, F 12.280)]  [G loss: -11.655] \n",
      "3842 [D loss: (1.438)(R -8.265, F 11.140)]  [G loss: -11.427] \n",
      "3843 [D loss: (1.556)(R -7.908, F 11.019)]  [G loss: -11.392] \n",
      "3843 [D loss: (1.720)(R -7.983, F 11.424)]  [G loss: -11.479] \n",
      "3844 [D loss: (1.759)(R -7.747, F 11.265)]  [G loss: -11.432] \n",
      "3844 [D loss: (1.872)(R -7.969, F 11.712)]  [G loss: -11.513] \n",
      "3845 [D loss: (1.693)(R -7.702, F 11.089)]  [G loss: -11.253] \n",
      "3845 [D loss: (1.642)(R -8.007, F 11.290)]  [G loss: -11.223] \n",
      "3846 [D loss: (1.837)(R -7.871, F 11.544)]  [G loss: -11.170] \n",
      "3846 [D loss: (1.697)(R -7.859, F 11.253)]  [G loss: -11.321] \n",
      "3847 [D loss: (1.923)(R -7.734, F 11.579)]  [G loss: -11.144] \n",
      "3847 [D loss: (1.618)(R -7.888, F 11.124)]  [G loss: -11.033] \n",
      "3848 [D loss: (1.855)(R -7.697, F 11.408)]  [G loss: -10.903] \n",
      "3848 [D loss: (1.429)(R -7.614, F 10.471)]  [G loss: -11.030] \n",
      "3849 [D loss: (1.811)(R -7.704, F 11.326)]  [G loss: -11.145] \n",
      "3849 [D loss: (1.634)(R -7.678, F 10.945)]  [G loss: -10.712] \n",
      "3850 [D loss: (1.557)(R -7.520, F 10.634)]  [G loss: -10.820] \n",
      "3850 [D loss: (1.676)(R -7.471, F 10.823)]  [G loss: -10.855] \n",
      "3851 [D loss: (1.730)(R -7.651, F 11.111)]  [G loss: -10.833] \n",
      "3851 [D loss: (1.872)(R -7.224, F 10.968)]  [G loss: -10.573] \n",
      "3852 [D loss: (1.411)(R -7.492, F 10.314)]  [G loss: -10.506] \n",
      "3852 [D loss: (1.378)(R -7.625, F 10.381)]  [G loss: -10.621] \n",
      "3853 [D loss: (1.488)(R -7.335, F 10.312)]  [G loss: -10.487] \n",
      "3853 [D loss: (1.467)(R -7.427, F 10.362)]  [G loss: -10.562] \n",
      "3854 [D loss: (1.509)(R -7.203, F 10.221)]  [G loss: -10.522] \n",
      "3854 [D loss: (1.592)(R -7.292, F 10.476)]  [G loss: -10.396] \n",
      "3855 [D loss: (1.252)(R -7.096, F 9.599)]  [G loss: -10.226] \n",
      "3855 [D loss: (1.070)(R -7.111, F 9.250)]  [G loss: -10.261] \n",
      "3856 [D loss: (1.556)(R -7.275, F 10.388)]  [G loss: -10.225] \n",
      "3856 [D loss: (1.322)(R -7.083, F 9.727)]  [G loss: -10.234] \n",
      "3857 [D loss: (1.543)(R -7.216, F 10.301)]  [G loss: -10.203] \n",
      "3857 [D loss: (1.452)(R -6.927, F 9.831)]  [G loss: -9.999] \n",
      "3858 [D loss: (1.509)(R -7.156, F 10.174)]  [G loss: -10.262] \n",
      "3858 [D loss: (1.442)(R -7.216, F 10.100)]  [G loss: -10.183] \n",
      "3859 [D loss: (1.541)(R -7.183, F 10.265)]  [G loss: -10.160] \n",
      "3859 [D loss: (1.657)(R -6.890, F 10.204)]  [G loss: -9.957] \n",
      "3860 [D loss: (1.406)(R -7.058, F 9.869)]  [G loss: -9.706] \n",
      "3860 [D loss: (1.461)(R -6.695, F 9.616)]  [G loss: -9.808] \n",
      "3861 [D loss: (1.523)(R -6.726, F 9.772)]  [G loss: -9.934] \n",
      "3861 [D loss: (1.629)(R -6.875, F 10.133)]  [G loss: -9.618] \n",
      "3862 [D loss: (1.607)(R -6.704, F 9.918)]  [G loss: -9.697] \n",
      "3862 [D loss: (1.590)(R -6.815, F 9.996)]  [G loss: -9.854] \n",
      "3863 [D loss: (1.700)(R -6.897, F 10.297)]  [G loss: -9.464] \n",
      "3863 [D loss: (1.480)(R -6.799, F 9.760)]  [G loss: -9.653] \n",
      "3864 [D loss: (1.399)(R -6.624, F 9.423)]  [G loss: -9.512] \n",
      "3864 [D loss: (1.219)(R -7.029, F 9.466)]  [G loss: -9.700] \n",
      "3865 [D loss: (1.575)(R -6.616, F 9.765)]  [G loss: -9.452] \n",
      "3865 [D loss: (1.413)(R -6.621, F 9.448)]  [G loss: -9.338] \n",
      "3866 [D loss: (1.461)(R -6.517, F 9.438)]  [G loss: -9.431] \n",
      "3866 [D loss: (1.570)(R -6.512, F 9.652)]  [G loss: -9.451] \n",
      "3867 [D loss: (1.362)(R -6.604, F 9.329)]  [G loss: -9.267] \n",
      "3867 [D loss: (1.385)(R -6.501, F 9.270)]  [G loss: -9.135] \n",
      "3868 [D loss: (1.623)(R -6.381, F 9.626)]  [G loss: -9.396] \n",
      "3868 [D loss: (1.563)(R -6.233, F 9.360)]  [G loss: -9.203] \n",
      "3869 [D loss: (1.218)(R -6.320, F 8.757)]  [G loss: -9.051] \n",
      "3869 [D loss: (1.348)(R -6.442, F 9.138)]  [G loss: -9.261] \n",
      "3870 [D loss: (1.249)(R -6.303, F 8.800)]  [G loss: -8.966] \n",
      "3870 [D loss: (1.355)(R -6.279, F 8.990)]  [G loss: -8.982] \n",
      "3871 [D loss: (1.064)(R -6.052, F 8.180)]  [G loss: -8.892] \n",
      "3871 [D loss: (1.187)(R -6.291, F 8.665)]  [G loss: -8.909] \n",
      "3872 [D loss: (1.449)(R -6.342, F 9.240)]  [G loss: -8.888] \n",
      "3872 [D loss: (1.376)(R -6.276, F 9.029)]  [G loss: -8.920] \n",
      "3873 [D loss: (1.636)(R -6.077, F 9.348)]  [G loss: -8.786] \n",
      "3873 [D loss: (1.217)(R -6.236, F 8.671)]  [G loss: -8.807] \n",
      "3874 [D loss: (1.335)(R -6.193, F 8.863)]  [G loss: -8.631] \n",
      "3874 [D loss: (1.155)(R -6.334, F 8.643)]  [G loss: -8.720] \n",
      "3875 [D loss: (1.268)(R -6.038, F 8.575)]  [G loss: -8.707] \n",
      "3875 [D loss: (1.430)(R -6.002, F 8.863)]  [G loss: -8.612] \n",
      "3876 [D loss: (1.194)(R -6.052, F 8.440)]  [G loss: -8.781] \n",
      "3876 [D loss: (1.293)(R -6.116, F 8.703)]  [G loss: -8.572] \n",
      "3877 [D loss: (1.324)(R -5.796, F 8.444)]  [G loss: -8.585] \n",
      "3877 [D loss: (1.056)(R -5.878, F 7.991)]  [G loss: -8.499] \n",
      "3878 [D loss: (1.414)(R -6.022, F 8.851)]  [G loss: -8.281] \n",
      "3878 [D loss: (1.334)(R -5.618, F 8.286)]  [G loss: -8.354] \n",
      "3879 [D loss: (1.502)(R -5.708, F 8.712)]  [G loss: -8.327] \n",
      "3879 [D loss: (1.225)(R -5.775, F 8.226)]  [G loss: -8.180] \n",
      "3880 [D loss: (1.191)(R -5.786, F 8.167)]  [G loss: -8.131] \n",
      "3880 [D loss: (1.327)(R -5.607, F 8.261)]  [G loss: -8.329] \n",
      "3881 [D loss: (1.082)(R -5.758, F 7.922)]  [G loss: -8.079] \n",
      "3881 [D loss: (1.347)(R -5.683, F 8.378)]  [G loss: -8.188] \n",
      "3882 [D loss: (1.254)(R -5.761, F 8.268)]  [G loss: -8.130] \n",
      "3882 [D loss: (1.120)(R -5.670, F 7.910)]  [G loss: -7.973] \n",
      "3883 [D loss: (1.222)(R -5.519, F 7.963)]  [G loss: -8.008] \n",
      "3883 [D loss: (1.037)(R -5.739, F 7.814)]  [G loss: -7.770] \n",
      "3884 [D loss: (1.338)(R -5.642, F 8.317)]  [G loss: -8.011] \n",
      "3884 [D loss: (1.152)(R -5.631, F 7.934)]  [G loss: -7.984] \n",
      "3885 [D loss: (1.162)(R -5.470, F 7.793)]  [G loss: -7.991] \n",
      "3885 [D loss: (1.328)(R -5.500, F 8.157)]  [G loss: -7.703] \n",
      "3886 [D loss: (1.168)(R -5.634, F 7.970)]  [G loss: -7.735] \n",
      "3886 [D loss: (1.199)(R -5.366, F 7.764)]  [G loss: -7.774] \n",
      "3887 [D loss: (1.124)(R -5.333, F 7.582)]  [G loss: -7.744] \n",
      "3887 [D loss: (0.857)(R -5.599, F 7.314)]  [G loss: -7.669] \n",
      "3888 [D loss: (1.251)(R -5.342, F 7.844)]  [G loss: -7.752] \n",
      "3888 [D loss: (1.217)(R -5.270, F 7.704)]  [G loss: -7.631] \n",
      "3889 [D loss: (0.866)(R -5.478, F 7.210)]  [G loss: -7.636] \n",
      "3889 [D loss: (1.069)(R -5.178, F 7.316)]  [G loss: -7.436] \n",
      "3890 [D loss: (1.096)(R -5.361, F 7.553)]  [G loss: -7.531] \n",
      "3890 [D loss: (1.075)(R -5.112, F 7.261)]  [G loss: -7.412] \n",
      "3891 [D loss: (1.037)(R -5.110, F 7.184)]  [G loss: -7.578] \n",
      "3891 [D loss: (0.868)(R -5.163, F 6.898)]  [G loss: -7.484] \n",
      "3892 [D loss: (1.017)(R -5.318, F 7.351)]  [G loss: -7.388] \n",
      "3892 [D loss: (1.199)(R -5.123, F 7.521)]  [G loss: -7.187] \n",
      "3893 [D loss: (1.061)(R -5.184, F 7.306)]  [G loss: -7.287] \n",
      "3893 [D loss: (1.157)(R -5.028, F 7.342)]  [G loss: -7.515] \n",
      "3894 [D loss: (1.233)(R -5.068, F 7.534)]  [G loss: -7.313] \n",
      "3894 [D loss: (1.223)(R -5.061, F 7.508)]  [G loss: -7.338] \n",
      "3895 [D loss: (1.066)(R -5.113, F 7.245)]  [G loss: -7.119] \n",
      "3895 [D loss: (1.191)(R -4.975, F 7.357)]  [G loss: -7.178] \n",
      "3896 [D loss: (1.077)(R -5.019, F 7.174)]  [G loss: -7.131] \n",
      "3896 [D loss: (1.020)(R -5.025, F 7.066)]  [G loss: -7.099] \n",
      "3897 [D loss: (1.110)(R -4.969, F 7.188)]  [G loss: -7.140] \n",
      "3897 [D loss: (1.366)(R -4.770, F 7.501)]  [G loss: -7.192] \n",
      "3898 [D loss: (1.146)(R -4.897, F 7.189)]  [G loss: -7.091] \n",
      "3898 [D loss: (1.125)(R -4.841, F 7.091)]  [G loss: -6.869] \n",
      "3899 [D loss: (1.001)(R -4.782, F 6.784)]  [G loss: -6.963] \n",
      "3899 [D loss: (0.905)(R -4.814, F 6.625)]  [G loss: -6.865] \n",
      "3900 [D loss: (1.107)(R -4.653, F 6.867)]  [G loss: -6.998] \n",
      "3900 [D loss: (0.905)(R -4.759, F 6.568)]  [G loss: -6.761] \n",
      "3901 [D loss: (1.212)(R -4.843, F 7.268)]  [G loss: -6.744] \n",
      "3901 [D loss: (0.791)(R -4.906, F 6.487)]  [G loss: -6.814] \n",
      "3902 [D loss: (0.998)(R -4.673, F 6.670)]  [G loss: -6.730] \n",
      "3902 [D loss: (0.919)(R -4.702, F 6.540)]  [G loss: -6.747] \n",
      "3903 [D loss: (1.029)(R -4.709, F 6.768)]  [G loss: -6.699] \n",
      "3903 [D loss: (0.953)(R -4.690, F 6.596)]  [G loss: -6.782] \n",
      "3904 [D loss: (0.968)(R -4.663, F 6.598)]  [G loss: -6.730] \n",
      "3904 [D loss: (0.962)(R -4.810, F 6.733)]  [G loss: -6.522] \n",
      "3905 [D loss: (0.934)(R -4.600, F 6.467)]  [G loss: -6.578] \n",
      "3905 [D loss: (0.943)(R -4.673, F 6.559)]  [G loss: -6.501] \n",
      "3906 [D loss: (0.981)(R -4.674, F 6.635)]  [G loss: -6.582] \n",
      "3906 [D loss: (1.099)(R -4.526, F 6.723)]  [G loss: -6.534] \n",
      "3907 [D loss: (1.006)(R -4.525, F 6.537)]  [G loss: -6.557] \n",
      "3907 [D loss: (1.053)(R -4.515, F 6.622)]  [G loss: -6.370] \n",
      "3908 [D loss: (0.989)(R -4.402, F 6.381)]  [G loss: -6.388] \n",
      "3908 [D loss: (0.870)(R -4.560, F 6.300)]  [G loss: -6.379] \n",
      "3909 [D loss: (1.062)(R -4.328, F 6.451)]  [G loss: -6.360] \n",
      "3909 [D loss: (0.987)(R -4.231, F 6.205)]  [G loss: -6.309] \n",
      "3910 [D loss: (0.893)(R -4.440, F 6.225)]  [G loss: -6.279] \n",
      "3910 [D loss: (0.720)(R -4.445, F 5.885)]  [G loss: -6.199] \n",
      "3911 [D loss: (0.994)(R -4.433, F 6.420)]  [G loss: -6.251] \n",
      "3911 [D loss: (0.949)(R -4.214, F 6.113)]  [G loss: -6.148] \n",
      "3912 [D loss: (0.826)(R -4.421, F 6.073)]  [G loss: -6.218] \n",
      "3912 [D loss: (0.779)(R -4.478, F 6.036)]  [G loss: -6.140] \n",
      "3913 [D loss: (0.774)(R -4.372, F 5.919)]  [G loss: -6.107] \n",
      "3913 [D loss: (0.956)(R -4.299, F 6.210)]  [G loss: -6.113] \n",
      "3914 [D loss: (0.756)(R -4.412, F 5.924)]  [G loss: -6.070] \n",
      "3914 [D loss: (0.867)(R -4.138, F 5.872)]  [G loss: -5.997] \n",
      "3915 [D loss: (0.901)(R -4.286, F 6.088)]  [G loss: -5.976] \n",
      "3915 [D loss: (0.950)(R -4.235, F 6.136)]  [G loss: -5.989] \n",
      "3916 [D loss: (0.745)(R -4.215, F 5.706)]  [G loss: -5.927] \n",
      "3916 [D loss: (0.955)(R -4.230, F 6.141)]  [G loss: -5.967] \n",
      "3917 [D loss: (0.894)(R -4.089, F 5.876)]  [G loss: -5.869] \n",
      "3917 [D loss: (0.902)(R -4.188, F 5.992)]  [G loss: -5.849] \n",
      "3918 [D loss: (0.642)(R -4.148, F 5.432)]  [G loss: -5.809] \n",
      "3918 [D loss: (0.853)(R -4.045, F 5.752)]  [G loss: -5.961] \n",
      "3919 [D loss: (0.920)(R -3.960, F 5.799)]  [G loss: -5.808] \n",
      "3919 [D loss: (0.740)(R -4.051, F 5.531)]  [G loss: -5.670] \n",
      "3920 [D loss: (0.818)(R -4.092, F 5.728)]  [G loss: -5.614] \n",
      "3920 [D loss: (0.646)(R -4.058, F 5.351)]  [G loss: -5.703] \n",
      "3921 [D loss: (0.658)(R -4.062, F 5.378)]  [G loss: -5.708] \n",
      "3921 [D loss: (0.836)(R -3.975, F 5.647)]  [G loss: -5.573] \n",
      "3922 [D loss: (0.697)(R -3.980, F 5.374)]  [G loss: -5.642] \n",
      "3922 [D loss: (0.678)(R -3.957, F 5.314)]  [G loss: -5.639] \n",
      "3923 [D loss: (0.657)(R -3.997, F 5.312)]  [G loss: -5.654] \n",
      "3923 [D loss: (0.999)(R -3.884, F 5.883)]  [G loss: -5.571] \n",
      "3924 [D loss: (0.743)(R -4.002, F 5.488)]  [G loss: -5.506] \n",
      "3924 [D loss: (0.867)(R -3.862, F 5.596)]  [G loss: -5.548] \n",
      "3925 [D loss: (0.711)(R -3.901, F 5.322)]  [G loss: -5.435] \n",
      "3925 [D loss: (0.873)(R -3.666, F 5.411)]  [G loss: -5.471] \n",
      "3926 [D loss: (0.887)(R -3.687, F 5.461)]  [G loss: -5.505] \n",
      "3926 [D loss: (0.793)(R -3.813, F 5.399)]  [G loss: -5.496] \n",
      "3927 [D loss: (0.863)(R -3.831, F 5.558)]  [G loss: -5.381] \n",
      "3927 [D loss: (0.822)(R -3.713, F 5.358)]  [G loss: -5.282] \n",
      "3928 [D loss: (0.822)(R -3.736, F 5.380)]  [G loss: -5.475] \n",
      "3928 [D loss: (0.781)(R -3.728, F 5.291)]  [G loss: -5.277] \n",
      "3929 [D loss: (0.803)(R -3.752, F 5.358)]  [G loss: -5.287] \n",
      "3929 [D loss: (0.864)(R -3.639, F 5.366)]  [G loss: -5.304] \n",
      "3930 [D loss: (0.742)(R -3.660, F 5.145)]  [G loss: -5.284] \n",
      "3930 [D loss: (0.718)(R -3.687, F 5.123)]  [G loss: -5.240] \n",
      "3931 [D loss: (0.657)(R -3.580, F 4.894)]  [G loss: -5.152] \n",
      "3931 [D loss: (0.713)(R -3.668, F 5.094)]  [G loss: -5.094] \n",
      "3932 [D loss: (0.887)(R -3.423, F 5.196)]  [G loss: -5.260] \n",
      "3932 [D loss: (0.619)(R -3.706, F 4.944)]  [G loss: -5.056] \n",
      "3933 [D loss: (0.713)(R -3.578, F 5.003)]  [G loss: -5.115] \n",
      "3933 [D loss: (0.733)(R -3.589, F 5.055)]  [G loss: -5.066] \n",
      "3934 [D loss: (0.735)(R -3.518, F 4.988)]  [G loss: -5.092] \n",
      "3934 [D loss: (0.682)(R -3.501, F 4.864)]  [G loss: -5.030] \n",
      "3935 [D loss: (0.699)(R -3.642, F 5.039)]  [G loss: -4.918] \n",
      "3935 [D loss: (0.889)(R -3.457, F 5.236)]  [G loss: -4.921] \n",
      "3936 [D loss: (0.521)(R -3.508, F 4.550)]  [G loss: -5.053] \n",
      "3936 [D loss: (0.754)(R -3.480, F 4.987)]  [G loss: -4.972] \n",
      "3937 [D loss: (0.540)(R -3.512, F 4.592)]  [G loss: -4.893] \n",
      "3937 [D loss: (0.804)(R -3.450, F 5.059)]  [G loss: -4.862] \n",
      "3938 [D loss: (0.637)(R -3.437, F 4.710)]  [G loss: -4.912] \n",
      "3938 [D loss: (0.758)(R -3.178, F 4.695)]  [G loss: -4.892] \n",
      "3939 [D loss: (0.800)(R -3.391, F 4.991)]  [G loss: -4.765] \n",
      "3939 [D loss: (0.625)(R -3.445, F 4.694)]  [G loss: -4.797] \n",
      "3940 [D loss: (0.637)(R -3.380, F 4.654)]  [G loss: -4.724] \n",
      "3940 [D loss: (0.635)(R -3.505, F 4.774)]  [G loss: -4.794] \n",
      "3941 [D loss: (0.695)(R -3.386, F 4.777)]  [G loss: -4.692] \n",
      "3941 [D loss: (0.676)(R -3.241, F 4.592)]  [G loss: -4.743] \n",
      "3942 [D loss: (0.602)(R -3.350, F 4.554)]  [G loss: -4.630] \n",
      "3942 [D loss: (0.574)(R -3.330, F 4.477)]  [G loss: -4.663] \n",
      "3943 [D loss: (0.596)(R -3.323, F 4.515)]  [G loss: -4.662] \n",
      "3943 [D loss: (0.708)(R -3.228, F 4.644)]  [G loss: -4.565] \n",
      "3944 [D loss: (0.622)(R -3.251, F 4.496)]  [G loss: -4.689] \n",
      "3944 [D loss: (0.686)(R -3.221, F 4.593)]  [G loss: -4.586] \n",
      "3945 [D loss: (0.514)(R -3.234, F 4.263)]  [G loss: -4.540] \n",
      "3945 [D loss: (0.662)(R -3.173, F 4.497)]  [G loss: -4.494] \n",
      "3946 [D loss: (0.579)(R -3.239, F 4.398)]  [G loss: -4.515] \n",
      "3946 [D loss: (0.669)(R -3.206, F 4.544)]  [G loss: -4.475] \n",
      "3947 [D loss: (0.634)(R -3.178, F 4.447)]  [G loss: -4.473] \n",
      "3947 [D loss: (0.743)(R -2.971, F 4.458)]  [G loss: -4.493] \n",
      "3948 [D loss: (0.767)(R -3.091, F 4.624)]  [G loss: -4.503] \n",
      "3948 [D loss: (0.723)(R -3.112, F 4.558)]  [G loss: -4.450] \n",
      "3949 [D loss: (0.673)(R -3.087, F 4.433)]  [G loss: -4.468] \n",
      "3949 [D loss: (0.726)(R -3.103, F 4.555)]  [G loss: -4.311] \n",
      "3950 [D loss: (0.673)(R -3.081, F 4.428)]  [G loss: -4.354] \n",
      "3950 [D loss: (0.554)(R -3.145, F 4.253)]  [G loss: -4.327] \n",
      "3951 [D loss: (0.654)(R -2.927, F 4.234)]  [G loss: -4.306] \n",
      "3951 [D loss: (0.552)(R -3.103, F 4.208)]  [G loss: -4.276] \n",
      "3952 [D loss: (0.644)(R -3.052, F 4.339)]  [G loss: -4.257] \n",
      "3952 [D loss: (0.621)(R -2.959, F 4.201)]  [G loss: -4.302] \n",
      "3953 [D loss: (0.676)(R -3.021, F 4.372)]  [G loss: -4.268] \n",
      "3953 [D loss: (0.587)(R -2.963, F 4.137)]  [G loss: -4.180] \n",
      "3954 [D loss: (0.653)(R -3.042, F 4.348)]  [G loss: -4.188] \n",
      "3954 [D loss: (0.494)(R -3.048, F 4.036)]  [G loss: -4.187] \n",
      "3955 [D loss: (0.626)(R -2.921, F 4.173)]  [G loss: -4.217] \n",
      "3955 [D loss: (0.559)(R -2.936, F 4.054)]  [G loss: -4.182] \n",
      "3956 [D loss: (0.661)(R -2.939, F 4.262)]  [G loss: -4.082] \n",
      "3956 [D loss: (0.566)(R -2.906, F 4.038)]  [G loss: -4.126] \n",
      "3957 [D loss: (0.619)(R -2.780, F 4.017)]  [G loss: -4.071] \n",
      "3957 [D loss: (0.497)(R -2.861, F 3.854)]  [G loss: -4.073] \n",
      "3958 [D loss: (0.592)(R -2.798, F 3.981)]  [G loss: -4.046] \n",
      "3958 [D loss: (0.505)(R -2.860, F 3.869)]  [G loss: -4.017] \n",
      "3959 [D loss: (0.704)(R -2.701, F 4.108)]  [G loss: -4.009] \n",
      "3959 [D loss: (0.605)(R -2.882, F 4.091)]  [G loss: -3.928] \n",
      "3960 [D loss: (0.611)(R -2.840, F 4.062)]  [G loss: -3.976] \n",
      "3960 [D loss: (0.563)(R -2.753, F 3.878)]  [G loss: -3.947] \n",
      "3961 [D loss: (0.552)(R -2.800, F 3.903)]  [G loss: -3.920] \n",
      "3961 [D loss: (0.541)(R -2.771, F 3.853)]  [G loss: -3.947] \n",
      "3962 [D loss: (0.599)(R -2.716, F 3.915)]  [G loss: -3.879] \n",
      "3962 [D loss: (0.495)(R -2.665, F 3.654)]  [G loss: -3.810] \n",
      "3963 [D loss: (0.503)(R -2.732, F 3.739)]  [G loss: -3.820] \n",
      "3963 [D loss: (0.657)(R -2.670, F 3.984)]  [G loss: -3.831] \n",
      "3964 [D loss: (0.448)(R -2.704, F 3.600)]  [G loss: -3.800] \n",
      "3964 [D loss: (0.586)(R -2.647, F 3.818)]  [G loss: -3.826] \n",
      "3965 [D loss: (0.487)(R -2.736, F 3.709)]  [G loss: -3.807] \n",
      "3965 [D loss: (0.599)(R -2.543, F 3.740)]  [G loss: -3.727] \n",
      "3966 [D loss: (0.572)(R -2.608, F 3.753)]  [G loss: -3.737] \n",
      "3966 [D loss: (0.454)(R -2.645, F 3.553)]  [G loss: -3.727] \n",
      "3967 [D loss: (0.618)(R -2.567, F 3.803)]  [G loss: -3.738] \n",
      "3967 [D loss: (0.601)(R -2.658, F 3.860)]  [G loss: -3.568] \n",
      "3968 [D loss: (0.487)(R -2.582, F 3.556)]  [G loss: -3.679] \n",
      "3968 [D loss: (0.521)(R -2.552, F 3.594)]  [G loss: -3.692] \n",
      "3969 [D loss: (0.512)(R -2.537, F 3.561)]  [G loss: -3.660] \n",
      "3969 [D loss: (0.456)(R -2.613, F 3.525)]  [G loss: -3.628] \n",
      "3970 [D loss: (0.449)(R -2.653, F 3.552)]  [G loss: -3.568] \n",
      "3970 [D loss: (0.453)(R -2.631, F 3.537)]  [G loss: -3.595] \n",
      "3971 [D loss: (0.501)(R -2.580, F 3.581)]  [G loss: -3.547] \n",
      "3971 [D loss: (0.575)(R -2.516, F 3.666)]  [G loss: -3.527] \n",
      "3972 [D loss: (0.474)(R -2.477, F 3.425)]  [G loss: -3.456] \n",
      "3972 [D loss: (0.550)(R -2.471, F 3.571)]  [G loss: -3.573] \n",
      "3973 [D loss: (0.427)(R -2.492, F 3.346)]  [G loss: -3.535] \n",
      "3973 [D loss: (0.478)(R -2.464, F 3.419)]  [G loss: -3.529] \n",
      "3974 [D loss: (0.527)(R -2.460, F 3.515)]  [G loss: -3.448] \n",
      "3974 [D loss: (0.444)(R -2.448, F 3.336)]  [G loss: -3.459] \n",
      "3975 [D loss: (0.478)(R -2.525, F 3.481)]  [G loss: -3.480] \n",
      "3975 [D loss: (0.519)(R -2.500, F 3.537)]  [G loss: -3.386] \n",
      "3976 [D loss: (0.412)(R -2.459, F 3.282)]  [G loss: -3.368] \n",
      "3976 [D loss: (0.529)(R -2.346, F 3.405)]  [G loss: -3.419] \n",
      "3977 [D loss: (0.588)(R -2.345, F 3.522)]  [G loss: -3.378] \n",
      "3977 [D loss: (0.538)(R -2.306, F 3.382)]  [G loss: -3.396] \n",
      "3978 [D loss: (0.564)(R -2.416, F 3.544)]  [G loss: -3.339] \n",
      "3978 [D loss: (0.431)(R -2.316, F 3.178)]  [G loss: -3.301] \n",
      "3979 [D loss: (0.471)(R -2.335, F 3.277)]  [G loss: -3.279] \n",
      "3979 [D loss: (0.401)(R -2.388, F 3.190)]  [G loss: -3.216] \n",
      "3980 [D loss: (0.433)(R -2.347, F 3.212)]  [G loss: -3.308] \n",
      "3980 [D loss: (0.522)(R -2.301, F 3.344)]  [G loss: -3.230] \n",
      "3981 [D loss: (0.465)(R -2.285, F 3.215)]  [G loss: -3.249] \n",
      "3981 [D loss: (0.406)(R -2.305, F 3.118)]  [G loss: -3.220] \n",
      "3982 [D loss: (0.539)(R -2.225, F 3.303)]  [G loss: -3.211] \n",
      "3982 [D loss: (0.391)(R -2.301, F 3.084)]  [G loss: -3.201] \n",
      "3983 [D loss: (0.516)(R -2.281, F 3.314)]  [G loss: -3.219] \n",
      "3983 [D loss: (0.503)(R -2.218, F 3.225)]  [G loss: -3.206] \n",
      "3984 [D loss: (0.463)(R -2.297, F 3.222)]  [G loss: -3.123] \n",
      "3984 [D loss: (0.565)(R -2.243, F 3.373)]  [G loss: -3.150] \n",
      "3985 [D loss: (0.473)(R -2.210, F 3.155)]  [G loss: -3.159] \n",
      "3985 [D loss: (0.500)(R -2.235, F 3.236)]  [G loss: -3.154] \n",
      "3986 [D loss: (0.388)(R -2.267, F 3.042)]  [G loss: -3.097] \n",
      "3986 [D loss: (0.552)(R -2.195, F 3.298)]  [G loss: -3.023] \n",
      "3987 [D loss: (0.468)(R -2.193, F 3.128)]  [G loss: -3.121] \n",
      "3987 [D loss: (0.413)(R -2.145, F 2.971)]  [G loss: -3.032] \n",
      "3988 [D loss: (0.485)(R -2.233, F 3.204)]  [G loss: -3.048] \n",
      "3988 [D loss: (0.519)(R -2.062, F 3.099)]  [G loss: -3.030] \n",
      "3989 [D loss: (0.486)(R -2.061, F 3.033)]  [G loss: -2.991] \n",
      "3989 [D loss: (0.446)(R -2.117, F 3.010)]  [G loss: -3.026] \n",
      "3990 [D loss: (0.387)(R -2.126, F 2.900)]  [G loss: -2.959] \n",
      "3990 [D loss: (0.453)(R -2.046, F 2.952)]  [G loss: -2.947] \n",
      "3991 [D loss: (0.412)(R -2.066, F 2.889)]  [G loss: -2.932] \n",
      "3991 [D loss: (0.372)(R -2.100, F 2.844)]  [G loss: -2.921] \n",
      "3992 [D loss: (0.363)(R -2.075, F 2.801)]  [G loss: -2.914] \n",
      "3992 [D loss: (0.441)(R -1.972, F 2.853)]  [G loss: -2.859] \n",
      "3993 [D loss: (0.430)(R -2.088, F 2.948)]  [G loss: -2.886] \n",
      "3993 [D loss: (0.374)(R -2.103, F 2.851)]  [G loss: -2.895] \n",
      "3994 [D loss: (0.435)(R -2.004, F 2.873)]  [G loss: -2.873] \n",
      "3994 [D loss: (0.392)(R -2.013, F 2.797)]  [G loss: -2.900] \n",
      "3995 [D loss: (0.462)(R -2.032, F 2.956)]  [G loss: -2.873] \n",
      "3995 [D loss: (0.460)(R -1.989, F 2.908)]  [G loss: -2.799] \n",
      "3996 [D loss: (0.430)(R -2.035, F 2.896)]  [G loss: -2.787] \n",
      "3996 [D loss: (0.494)(R -2.011, F 3.000)]  [G loss: -2.808] \n",
      "3997 [D loss: (0.374)(R -2.025, F 2.772)]  [G loss: -2.748] \n",
      "3997 [D loss: (0.336)(R -1.953, F 2.625)]  [G loss: -2.713] \n",
      "3998 [D loss: (0.355)(R -1.913, F 2.623)]  [G loss: -2.722] \n",
      "3998 [D loss: (0.391)(R -1.908, F 2.691)]  [G loss: -2.725] \n",
      "3999 [D loss: (0.377)(R -1.879, F 2.633)]  [G loss: -2.736] \n",
      "3999 [D loss: (0.408)(R -1.909, F 2.725)]  [G loss: -2.690] \n",
      "4000 [D loss: (0.436)(R -1.973, F 2.845)]  [G loss: -2.708] \n",
      "4000 [D loss: (0.459)(R -1.899, F 2.818)]  [G loss: -2.637] \n",
      "4001 [D loss: (0.413)(R -1.884, F 2.710)]  [G loss: -2.668] \n",
      "4001 [D loss: (0.429)(R -1.857, F 2.714)]  [G loss: -2.632] \n",
      "4002 [D loss: (0.443)(R -1.883, F 2.769)]  [G loss: -2.671] \n",
      "4002 [D loss: (0.386)(R -1.886, F 2.657)]  [G loss: -2.612] \n",
      "4003 [D loss: (0.379)(R -1.945, F 2.703)]  [G loss: -2.565] \n",
      "4003 [D loss: (0.376)(R -1.835, F 2.587)]  [G loss: -2.622] \n",
      "4004 [D loss: (0.328)(R -1.806, F 2.462)]  [G loss: -2.606] \n",
      "4004 [D loss: (0.355)(R -1.828, F 2.538)]  [G loss: -2.583] \n",
      "4005 [D loss: (0.392)(R -1.829, F 2.612)]  [G loss: -2.549] \n",
      "4005 [D loss: (0.370)(R -1.836, F 2.576)]  [G loss: -2.543] \n",
      "4006 [D loss: (0.399)(R -1.833, F 2.631)]  [G loss: -2.560] \n",
      "4006 [D loss: (0.373)(R -1.779, F 2.524)]  [G loss: -2.496] \n",
      "4007 [D loss: (0.430)(R -1.750, F 2.610)]  [G loss: -2.519] \n",
      "4007 [D loss: (0.340)(R -1.830, F 2.509)]  [G loss: -2.525] \n",
      "4008 [D loss: (0.405)(R -1.805, F 2.615)]  [G loss: -2.457] \n",
      "4008 [D loss: (0.369)(R -1.770, F 2.507)]  [G loss: -2.479] \n",
      "4009 [D loss: (0.361)(R -1.771, F 2.493)]  [G loss: -2.472] \n",
      "4009 [D loss: (0.348)(R -1.766, F 2.461)]  [G loss: -2.430] \n",
      "4010 [D loss: (0.320)(R -1.783, F 2.422)]  [G loss: -2.407] \n",
      "4010 [D loss: (0.337)(R -1.755, F 2.429)]  [G loss: -2.448] \n",
      "4011 [D loss: (0.312)(R -1.762, F 2.385)]  [G loss: -2.439] \n",
      "4011 [D loss: (0.326)(R -1.746, F 2.397)]  [G loss: -2.440] \n",
      "4012 [D loss: (0.352)(R -1.742, F 2.446)]  [G loss: -2.372] \n",
      "4012 [D loss: (0.446)(R -1.728, F 2.620)]  [G loss: -2.343] \n",
      "4013 [D loss: (0.354)(R -1.667, F 2.376)]  [G loss: -2.357] \n",
      "4013 [D loss: (0.362)(R -1.692, F 2.416)]  [G loss: -2.380] \n",
      "4014 [D loss: (0.306)(R -1.689, F 2.301)]  [G loss: -2.324] \n",
      "4014 [D loss: (0.395)(R -1.668, F 2.458)]  [G loss: -2.348] \n",
      "4015 [D loss: (0.306)(R -1.685, F 2.297)]  [G loss: -2.318] \n",
      "4015 [D loss: (0.318)(R -1.657, F 2.293)]  [G loss: -2.302] \n",
      "4016 [D loss: (0.259)(R -1.675, F 2.194)]  [G loss: -2.310] \n",
      "4016 [D loss: (0.303)(R -1.680, F 2.286)]  [G loss: -2.330] \n",
      "4017 [D loss: (0.309)(R -1.639, F 2.256)]  [G loss: -2.316] \n",
      "4017 [D loss: (0.293)(R -1.663, F 2.250)]  [G loss: -2.228] \n",
      "4018 [D loss: (0.321)(R -1.603, F 2.246)]  [G loss: -2.247] \n",
      "4018 [D loss: (0.301)(R -1.601, F 2.204)]  [G loss: -2.223] \n",
      "4019 [D loss: (0.334)(R -1.599, F 2.268)]  [G loss: -2.228] \n",
      "4019 [D loss: (0.274)(R -1.615, F 2.163)]  [G loss: -2.217] \n",
      "4020 [D loss: (0.276)(R -1.628, F 2.180)]  [G loss: -2.202] \n",
      "4020 [D loss: (0.323)(R -1.593, F 2.239)]  [G loss: -2.170] \n",
      "4021 [D loss: (0.346)(R -1.539, F 2.230)]  [G loss: -2.194] \n",
      "4021 [D loss: (0.284)(R -1.568, F 2.136)]  [G loss: -2.152] \n",
      "4022 [D loss: (0.277)(R -1.529, F 2.084)]  [G loss: -2.169] \n",
      "4022 [D loss: (0.302)(R -1.559, F 2.164)]  [G loss: -2.153] \n",
      "4023 [D loss: (0.287)(R -1.512, F 2.086)]  [G loss: -2.130] \n",
      "4023 [D loss: (0.299)(R -1.539, F 2.138)]  [G loss: -2.101] \n",
      "4024 [D loss: (0.318)(R -1.529, F 2.166)]  [G loss: -2.114] \n",
      "4024 [D loss: (0.299)(R -1.512, F 2.110)]  [G loss: -2.115] \n",
      "4025 [D loss: (0.298)(R -1.498, F 2.094)]  [G loss: -2.127] \n",
      "4025 [D loss: (0.220)(R -1.566, F 2.005)]  [G loss: -2.082] \n",
      "4026 [D loss: (0.273)(R -1.542, F 2.088)]  [G loss: -2.066] \n",
      "4026 [D loss: (0.289)(R -1.515, F 2.093)]  [G loss: -2.041] \n",
      "4027 [D loss: (0.329)(R -1.493, F 2.152)]  [G loss: -2.085] \n",
      "4027 [D loss: (0.308)(R -1.487, F 2.103)]  [G loss: -2.042] \n",
      "4028 [D loss: (0.313)(R -1.471, F 2.097)]  [G loss: -2.019] \n",
      "4028 [D loss: (0.235)(R -1.477, F 1.947)]  [G loss: -2.084] \n",
      "4029 [D loss: (0.231)(R -1.489, F 1.951)]  [G loss: -2.039] \n",
      "4029 [D loss: (0.295)(R -1.435, F 2.026)]  [G loss: -2.022] \n",
      "4030 [D loss: (0.280)(R -1.492, F 2.051)]  [G loss: -1.993] \n",
      "4030 [D loss: (0.298)(R -1.439, F 2.035)]  [G loss: -2.000] \n",
      "4031 [D loss: (0.327)(R -1.397, F 2.050)]  [G loss: -1.969] \n",
      "4031 [D loss: (0.254)(R -1.465, F 1.973)]  [G loss: -1.995] \n",
      "4032 [D loss: (0.343)(R -1.433, F 2.118)]  [G loss: -1.954] \n",
      "4032 [D loss: (0.261)(R -1.441, F 1.964)]  [G loss: -1.959] \n",
      "4033 [D loss: (0.264)(R -1.413, F 1.940)]  [G loss: -1.948] \n",
      "4033 [D loss: (0.219)(R -1.431, F 1.869)]  [G loss: -1.950] \n",
      "4034 [D loss: (0.218)(R -1.403, F 1.839)]  [G loss: -1.906] \n",
      "4034 [D loss: (0.218)(R -1.390, F 1.826)]  [G loss: -1.919] \n",
      "4035 [D loss: (0.216)(R -1.397, F 1.828)]  [G loss: -1.913] \n",
      "4035 [D loss: (0.237)(R -1.370, F 1.843)]  [G loss: -1.884] \n",
      "4036 [D loss: (0.252)(R -1.345, F 1.849)]  [G loss: -1.895] \n",
      "4036 [D loss: (0.257)(R -1.389, F 1.903)]  [G loss: -1.859] \n",
      "4037 [D loss: (0.224)(R -1.410, F 1.858)]  [G loss: -1.839] \n",
      "4037 [D loss: (0.207)(R -1.374, F 1.788)]  [G loss: -1.849] \n",
      "4038 [D loss: (0.229)(R -1.355, F 1.813)]  [G loss: -1.828] \n",
      "4038 [D loss: (0.254)(R -1.345, F 1.852)]  [G loss: -1.852] \n",
      "4039 [D loss: (0.212)(R -1.314, F 1.738)]  [G loss: -1.846] \n",
      "4039 [D loss: (0.288)(R -1.338, F 1.913)]  [G loss: -1.790] \n",
      "4040 [D loss: (0.241)(R -1.309, F 1.791)]  [G loss: -1.798] \n",
      "4040 [D loss: (0.219)(R -1.307, F 1.744)]  [G loss: -1.817] \n",
      "4041 [D loss: (0.240)(R -1.296, F 1.777)]  [G loss: -1.772] \n",
      "4041 [D loss: (0.273)(R -1.334, F 1.880)]  [G loss: -1.800] \n",
      "4042 [D loss: (0.271)(R -1.315, F 1.857)]  [G loss: -1.778] \n",
      "4042 [D loss: (0.276)(R -1.279, F 1.830)]  [G loss: -1.804] \n",
      "4043 [D loss: (0.213)(R -1.306, F 1.732)]  [G loss: -1.749] \n",
      "4043 [D loss: (0.229)(R -1.277, F 1.735)]  [G loss: -1.759] \n",
      "4044 [D loss: (0.274)(R -1.248, F 1.797)]  [G loss: -1.759] \n",
      "4044 [D loss: (0.233)(R -1.227, F 1.693)]  [G loss: -1.735] \n",
      "4045 [D loss: (0.226)(R -1.287, F 1.739)]  [G loss: -1.748] \n",
      "4045 [D loss: (0.200)(R -1.254, F 1.655)]  [G loss: -1.702] \n",
      "4046 [D loss: (0.213)(R -1.255, F 1.682)]  [G loss: -1.704] \n",
      "4046 [D loss: (0.269)(R -1.204, F 1.742)]  [G loss: -1.696] \n",
      "4047 [D loss: (0.240)(R -1.182, F 1.662)]  [G loss: -1.696] \n",
      "4047 [D loss: (0.250)(R -1.230, F 1.730)]  [G loss: -1.697] \n",
      "4048 [D loss: (0.237)(R -1.215, F 1.689)]  [G loss: -1.681] \n",
      "4048 [D loss: (0.261)(R -1.217, F 1.738)]  [G loss: -1.678] \n",
      "4049 [D loss: (0.248)(R -1.225, F 1.721)]  [G loss: -1.643] \n",
      "4049 [D loss: (0.274)(R -1.205, F 1.752)]  [G loss: -1.652] \n",
      "4050 [D loss: (0.224)(R -1.194, F 1.643)]  [G loss: -1.649] \n",
      "4050 [D loss: (0.209)(R -1.212, F 1.630)]  [G loss: -1.653] \n",
      "4051 [D loss: (0.214)(R -1.191, F 1.619)]  [G loss: -1.651] \n",
      "4051 [D loss: (0.216)(R -1.174, F 1.605)]  [G loss: -1.631] \n",
      "4052 [D loss: (0.217)(R -1.180, F 1.615)]  [G loss: -1.577] \n",
      "4052 [D loss: (0.202)(R -1.174, F 1.578)]  [G loss: -1.584] \n",
      "4053 [D loss: (0.188)(R -1.201, F 1.577)]  [G loss: -1.563] \n",
      "4053 [D loss: (0.220)(R -1.166, F 1.607)]  [G loss: -1.608] \n",
      "4054 [D loss: (0.213)(R -1.172, F 1.598)]  [G loss: -1.564] \n",
      "4054 [D loss: (0.211)(R -1.165, F 1.587)]  [G loss: -1.556] \n",
      "4055 [D loss: (0.191)(R -1.135, F 1.516)]  [G loss: -1.546] \n",
      "4055 [D loss: (0.173)(R -1.114, F 1.461)]  [G loss: -1.556] \n",
      "4056 [D loss: (0.216)(R -1.096, F 1.527)]  [G loss: -1.531] \n",
      "4056 [D loss: (0.219)(R -1.120, F 1.557)]  [G loss: -1.545] \n",
      "4057 [D loss: (0.168)(R -1.129, F 1.466)]  [G loss: -1.516] \n",
      "4057 [D loss: (0.203)(R -1.124, F 1.530)]  [G loss: -1.511] \n",
      "4058 [D loss: (0.194)(R -1.118, F 1.507)]  [G loss: -1.508] \n",
      "4058 [D loss: (0.227)(R -1.084, F 1.538)]  [G loss: -1.500] \n",
      "4059 [D loss: (0.220)(R -1.134, F 1.573)]  [G loss: -1.522] \n",
      "4059 [D loss: (0.209)(R -1.064, F 1.482)]  [G loss: -1.479] \n",
      "4060 [D loss: (0.190)(R -1.101, F 1.481)]  [G loss: -1.509] \n",
      "4060 [D loss: (0.194)(R -1.089, F 1.478)]  [G loss: -1.479] \n",
      "4061 [D loss: (0.225)(R -1.086, F 1.536)]  [G loss: -1.489] \n",
      "4061 [D loss: (0.190)(R -1.082, F 1.462)]  [G loss: -1.467] \n",
      "4062 [D loss: (0.192)(R -1.044, F 1.427)]  [G loss: -1.451] \n",
      "4062 [D loss: (0.157)(R -1.070, F 1.384)]  [G loss: -1.465] \n",
      "4063 [D loss: (0.191)(R -1.025, F 1.408)]  [G loss: -1.438] \n",
      "4063 [D loss: (0.211)(R -1.060, F 1.481)]  [G loss: -1.429] \n",
      "4064 [D loss: (0.159)(R -1.030, F 1.347)]  [G loss: -1.438] \n",
      "4064 [D loss: (0.170)(R -1.037, F 1.377)]  [G loss: -1.419] \n",
      "4065 [D loss: (0.190)(R -1.017, F 1.396)]  [G loss: -1.421] \n",
      "4065 [D loss: (0.207)(R -1.023, F 1.436)]  [G loss: -1.393] \n",
      "4066 [D loss: (0.190)(R -1.007, F 1.387)]  [G loss: -1.398] \n",
      "4066 [D loss: (0.188)(R -1.022, F 1.399)]  [G loss: -1.387] \n",
      "4067 [D loss: (0.150)(R -1.013, F 1.314)]  [G loss: -1.401] \n",
      "4067 [D loss: (0.203)(R -0.998, F 1.404)]  [G loss: -1.396] \n",
      "4068 [D loss: (0.173)(R -0.994, F 1.340)]  [G loss: -1.378] \n",
      "4068 [D loss: (0.210)(R -1.010, F 1.429)]  [G loss: -1.366] \n",
      "4069 [D loss: (0.179)(R -0.996, F 1.354)]  [G loss: -1.375] \n",
      "4069 [D loss: (0.189)(R -0.979, F 1.357)]  [G loss: -1.356] \n",
      "4070 [D loss: (0.187)(R -0.999, F 1.372)]  [G loss: -1.362] \n",
      "4070 [D loss: (0.189)(R -0.968, F 1.345)]  [G loss: -1.343] \n",
      "4071 [D loss: (0.154)(R -0.994, F 1.303)]  [G loss: -1.333] \n",
      "4071 [D loss: (0.164)(R -0.997, F 1.324)]  [G loss: -1.300] \n",
      "4072 [D loss: (0.156)(R -0.966, F 1.278)]  [G loss: -1.324] \n",
      "4072 [D loss: (0.220)(R -0.934, F 1.374)]  [G loss: -1.295] \n",
      "4073 [D loss: (0.149)(R -0.959, F 1.256)]  [G loss: -1.306] \n",
      "4073 [D loss: (0.149)(R -0.945, F 1.243)]  [G loss: -1.272] \n",
      "4074 [D loss: (0.148)(R -0.946, F 1.242)]  [G loss: -1.266] \n",
      "4074 [D loss: (0.146)(R -0.947, F 1.240)]  [G loss: -1.281] \n",
      "4075 [D loss: (0.150)(R -0.947, F 1.248)]  [G loss: -1.287] \n",
      "4075 [D loss: (0.145)(R -0.958, F 1.247)]  [G loss: -1.285] \n",
      "4076 [D loss: (0.164)(R -0.934, F 1.262)]  [G loss: -1.243] \n",
      "4076 [D loss: (0.159)(R -0.945, F 1.264)]  [G loss: -1.283] \n",
      "4077 [D loss: (0.137)(R -0.936, F 1.210)]  [G loss: -1.226] \n",
      "4077 [D loss: (0.174)(R -0.911, F 1.259)]  [G loss: -1.230] \n",
      "4078 [D loss: (0.155)(R -0.921, F 1.231)]  [G loss: -1.223] \n",
      "4078 [D loss: (0.144)(R -0.898, F 1.185)]  [G loss: -1.245] \n",
      "4079 [D loss: (0.155)(R -0.902, F 1.211)]  [G loss: -1.218] \n",
      "4079 [D loss: (0.148)(R -0.912, F 1.208)]  [G loss: -1.190] \n",
      "4080 [D loss: (0.175)(R -0.899, F 1.249)]  [G loss: -1.207] \n",
      "4080 [D loss: (0.189)(R -0.870, F 1.249)]  [G loss: -1.175] \n",
      "4081 [D loss: (0.128)(R -0.887, F 1.142)]  [G loss: -1.205] \n",
      "4081 [D loss: (0.139)(R -0.883, F 1.160)]  [G loss: -1.193] \n",
      "4082 [D loss: (0.157)(R -0.891, F 1.206)]  [G loss: -1.199] \n",
      "4082 [D loss: (0.160)(R -0.884, F 1.205)]  [G loss: -1.176] \n",
      "4083 [D loss: (0.176)(R -0.861, F 1.212)]  [G loss: -1.158] \n",
      "4083 [D loss: (0.126)(R -0.883, F 1.135)]  [G loss: -1.180] \n",
      "4084 [D loss: (0.148)(R -0.861, F 1.157)]  [G loss: -1.158] \n",
      "4084 [D loss: (0.180)(R -0.860, F 1.221)]  [G loss: -1.153] \n",
      "4085 [D loss: (0.127)(R -0.873, F 1.126)]  [G loss: -1.136] \n",
      "4085 [D loss: (0.131)(R -0.878, F 1.140)]  [G loss: -1.147] \n",
      "4086 [D loss: (0.147)(R -0.860, F 1.154)]  [G loss: -1.136] \n",
      "4086 [D loss: (0.170)(R -0.817, F 1.156)]  [G loss: -1.141] \n",
      "4087 [D loss: (0.166)(R -0.842, F 1.174)]  [G loss: -1.117] \n",
      "4087 [D loss: (0.183)(R -0.812, F 1.178)]  [G loss: -1.115] \n",
      "4088 [D loss: (0.152)(R -0.828, F 1.133)]  [G loss: -1.112] \n",
      "4088 [D loss: (0.137)(R -0.843, F 1.117)]  [G loss: -1.109] \n",
      "4089 [D loss: (0.112)(R -0.828, F 1.052)]  [G loss: -1.078] \n",
      "4089 [D loss: (0.117)(R -0.811, F 1.045)]  [G loss: -1.088] \n",
      "4090 [D loss: (0.162)(R -0.829, F 1.154)]  [G loss: -1.092] \n",
      "4090 [D loss: (0.126)(R -0.822, F 1.075)]  [G loss: -1.080] \n",
      "4091 [D loss: (0.110)(R -0.830, F 1.050)]  [G loss: -1.079] \n",
      "4091 [D loss: (0.150)(R -0.820, F 1.121)]  [G loss: -1.054] \n",
      "4092 [D loss: (0.143)(R -0.811, F 1.097)]  [G loss: -1.064] \n",
      "4092 [D loss: (0.130)(R -0.816, F 1.077)]  [G loss: -1.061] \n",
      "4093 [D loss: (0.128)(R -0.792, F 1.048)]  [G loss: -1.049] \n",
      "4093 [D loss: (0.125)(R -0.789, F 1.039)]  [G loss: -1.036] \n",
      "4094 [D loss: (0.112)(R -0.808, F 1.031)]  [G loss: -1.032] \n",
      "4094 [D loss: (0.128)(R -0.797, F 1.053)]  [G loss: -1.030] \n",
      "4095 [D loss: (0.136)(R -0.766, F 1.037)]  [G loss: -1.028] \n",
      "4095 [D loss: (0.116)(R -0.800, F 1.032)]  [G loss: -1.025] \n",
      "4096 [D loss: (0.125)(R -0.773, F 1.023)]  [G loss: -1.005] \n",
      "4096 [D loss: (0.136)(R -0.774, F 1.045)]  [G loss: -1.024] \n",
      "4097 [D loss: (0.114)(R -0.770, F 0.999)]  [G loss: -1.021] \n",
      "4097 [D loss: (0.113)(R -0.789, F 1.014)]  [G loss: -1.017] \n",
      "4098 [D loss: (0.081)(R -0.762, F 0.924)]  [G loss: -0.988] \n",
      "4098 [D loss: (0.129)(R -0.724, F 0.982)]  [G loss: -0.994] \n",
      "4099 [D loss: (0.109)(R -0.775, F 0.994)]  [G loss: -0.991] \n",
      "4099 [D loss: (0.112)(R -0.770, F 0.994)]  [G loss: -0.982] \n",
      "4100 [D loss: (0.104)(R -0.742, F 0.950)]  [G loss: -0.975] \n",
      "4100 [D loss: (0.083)(R -0.760, F 0.925)]  [G loss: -0.981] \n",
      "4101 [D loss: (0.099)(R -0.733, F 0.931)]  [G loss: -0.972] \n",
      "4101 [D loss: (0.095)(R -0.757, F 0.946)]  [G loss: -0.958] \n",
      "4102 [D loss: (0.136)(R -0.717, F 0.988)]  [G loss: -0.947] \n",
      "4102 [D loss: (0.116)(R -0.722, F 0.954)]  [G loss: -0.949] \n",
      "4103 [D loss: (0.090)(R -0.720, F 0.901)]  [G loss: -0.937] \n",
      "4103 [D loss: (0.111)(R -0.719, F 0.940)]  [G loss: -0.938] \n",
      "4104 [D loss: (0.099)(R -0.731, F 0.928)]  [G loss: -0.944] \n",
      "4104 [D loss: (0.131)(R -0.713, F 0.975)]  [G loss: -0.947] \n",
      "4105 [D loss: (0.111)(R -0.693, F 0.914)]  [G loss: -0.935] \n",
      "4105 [D loss: (0.096)(R -0.729, F 0.922)]  [G loss: -0.923] \n",
      "4106 [D loss: (0.100)(R -0.729, F 0.929)]  [G loss: -0.920] \n",
      "4106 [D loss: (0.085)(R -0.712, F 0.883)]  [G loss: -0.908] \n",
      "4107 [D loss: (0.113)(R -0.695, F 0.920)]  [G loss: -0.923] \n",
      "4107 [D loss: (0.098)(R -0.696, F 0.891)]  [G loss: -0.913] \n",
      "4108 [D loss: (0.117)(R -0.704, F 0.938)]  [G loss: -0.905] \n",
      "4108 [D loss: (0.102)(R -0.711, F 0.916)]  [G loss: -0.896] \n",
      "4109 [D loss: (0.098)(R -0.687, F 0.884)]  [G loss: -0.898] \n",
      "4109 [D loss: (0.112)(R -0.673, F 0.898)]  [G loss: -0.896] \n",
      "4110 [D loss: (0.107)(R -0.667, F 0.881)]  [G loss: -0.881] \n",
      "4110 [D loss: (0.122)(R -0.669, F 0.913)]  [G loss: -0.875] \n",
      "4111 [D loss: (0.122)(R -0.672, F 0.915)]  [G loss: -0.876] \n",
      "4111 [D loss: (0.087)(R -0.667, F 0.841)]  [G loss: -0.860] \n",
      "4112 [D loss: (0.097)(R -0.656, F 0.850)]  [G loss: -0.854] \n",
      "4112 [D loss: (0.092)(R -0.678, F 0.862)]  [G loss: -0.844] \n",
      "4113 [D loss: (0.100)(R -0.670, F 0.869)]  [G loss: -0.856] \n",
      "4113 [D loss: (0.075)(R -0.656, F 0.806)]  [G loss: -0.839] \n",
      "4114 [D loss: (0.103)(R -0.643, F 0.848)]  [G loss: -0.831] \n",
      "4114 [D loss: (0.094)(R -0.650, F 0.838)]  [G loss: -0.828] \n",
      "4115 [D loss: (0.090)(R -0.665, F 0.845)]  [G loss: -0.824] \n",
      "4115 [D loss: (0.095)(R -0.642, F 0.831)]  [G loss: -0.826] \n",
      "4116 [D loss: (0.102)(R -0.647, F 0.851)]  [G loss: -0.823] \n",
      "4116 [D loss: (0.102)(R -0.631, F 0.836)]  [G loss: -0.831] \n",
      "4117 [D loss: (0.071)(R -0.647, F 0.789)]  [G loss: -0.817] \n",
      "4117 [D loss: (0.094)(R -0.637, F 0.825)]  [G loss: -0.805] \n",
      "4118 [D loss: (0.091)(R -0.636, F 0.819)]  [G loss: -0.788] \n",
      "4118 [D loss: (0.084)(R -0.619, F 0.786)]  [G loss: -0.803] \n",
      "4119 [D loss: (0.104)(R -0.624, F 0.832)]  [G loss: -0.773] \n",
      "4119 [D loss: (0.095)(R -0.619, F 0.808)]  [G loss: -0.785] \n",
      "4120 [D loss: (0.074)(R -0.609, F 0.757)]  [G loss: -0.779] \n",
      "4120 [D loss: (0.098)(R -0.609, F 0.805)]  [G loss: -0.783] \n",
      "4121 [D loss: (0.087)(R -0.602, F 0.777)]  [G loss: -0.778] \n",
      "4121 [D loss: (0.102)(R -0.609, F 0.813)]  [G loss: -0.775] \n",
      "4122 [D loss: (0.094)(R -0.609, F 0.797)]  [G loss: -0.773] \n",
      "4122 [D loss: (0.092)(R -0.604, F 0.789)]  [G loss: -0.767] \n",
      "4123 [D loss: (0.082)(R -0.591, F 0.754)]  [G loss: -0.765] \n",
      "4123 [D loss: (0.080)(R -0.598, F 0.757)]  [G loss: -0.756] \n",
      "4124 [D loss: (0.113)(R -0.577, F 0.803)]  [G loss: -0.754] \n",
      "4124 [D loss: (0.074)(R -0.593, F 0.740)]  [G loss: -0.756] \n",
      "4125 [D loss: (0.073)(R -0.592, F 0.738)]  [G loss: -0.734] \n",
      "4125 [D loss: (0.074)(R -0.606, F 0.754)]  [G loss: -0.743] \n",
      "4126 [D loss: (0.067)(R -0.566, F 0.700)]  [G loss: -0.737] \n",
      "4126 [D loss: (0.083)(R -0.582, F 0.748)]  [G loss: -0.722] \n",
      "4127 [D loss: (0.069)(R -0.577, F 0.714)]  [G loss: -0.730] \n",
      "4127 [D loss: (0.070)(R -0.567, F 0.707)]  [G loss: -0.723] \n",
      "4128 [D loss: (0.066)(R -0.577, F 0.708)]  [G loss: -0.718] \n",
      "4128 [D loss: (0.077)(R -0.550, F 0.703)]  [G loss: -0.727] \n",
      "4129 [D loss: (0.073)(R -0.570, F 0.716)]  [G loss: -0.705] \n",
      "4129 [D loss: (0.075)(R -0.574, F 0.725)]  [G loss: -0.705] \n",
      "4130 [D loss: (0.067)(R -0.563, F 0.696)]  [G loss: -0.701] \n",
      "4130 [D loss: (0.082)(R -0.548, F 0.711)]  [G loss: -0.701] \n",
      "4131 [D loss: (0.088)(R -0.556, F 0.732)]  [G loss: -0.691] \n",
      "4131 [D loss: (0.074)(R -0.542, F 0.691)]  [G loss: -0.695] \n",
      "4132 [D loss: (0.069)(R -0.542, F 0.681)]  [G loss: -0.687] \n",
      "4132 [D loss: (0.067)(R -0.551, F 0.684)]  [G loss: -0.678] \n",
      "4133 [D loss: (0.069)(R -0.542, F 0.680)]  [G loss: -0.674] \n",
      "4133 [D loss: (0.060)(R -0.547, F 0.668)]  [G loss: -0.657] \n",
      "4134 [D loss: (0.064)(R -0.543, F 0.672)]  [G loss: -0.676] \n",
      "4134 [D loss: (0.067)(R -0.527, F 0.660)]  [G loss: -0.665] \n",
      "4135 [D loss: (0.057)(R -0.533, F 0.647)]  [G loss: -0.668] \n",
      "4135 [D loss: (0.073)(R -0.519, F 0.664)]  [G loss: -0.657] \n",
      "4136 [D loss: (0.060)(R -0.529, F 0.648)]  [G loss: -0.655] \n",
      "4136 [D loss: (0.064)(R -0.516, F 0.645)]  [G loss: -0.651] \n",
      "4137 [D loss: (0.062)(R -0.522, F 0.647)]  [G loss: -0.645] \n",
      "4137 [D loss: (0.069)(R -0.516, F 0.654)]  [G loss: -0.640] \n",
      "4138 [D loss: (0.073)(R -0.511, F 0.656)]  [G loss: -0.622] \n",
      "4138 [D loss: (0.060)(R -0.508, F 0.627)]  [G loss: -0.634] \n",
      "4139 [D loss: (0.060)(R -0.515, F 0.636)]  [G loss: -0.628] \n",
      "4139 [D loss: (0.062)(R -0.499, F 0.623)]  [G loss: -0.638] \n",
      "4140 [D loss: (0.057)(R -0.502, F 0.616)]  [G loss: -0.620] \n",
      "4140 [D loss: (0.061)(R -0.506, F 0.627)]  [G loss: -0.621] \n",
      "4141 [D loss: (0.070)(R -0.492, F 0.632)]  [G loss: -0.617] \n",
      "4141 [D loss: (0.044)(R -0.506, F 0.594)]  [G loss: -0.610] \n",
      "4142 [D loss: (0.045)(R -0.492, F 0.582)]  [G loss: -0.611] \n",
      "4142 [D loss: (0.053)(R -0.510, F 0.616)]  [G loss: -0.600] \n",
      "4143 [D loss: (0.061)(R -0.487, F 0.608)]  [G loss: -0.601] \n",
      "4143 [D loss: (0.052)(R -0.489, F 0.594)]  [G loss: -0.594] \n",
      "4144 [D loss: (0.054)(R -0.496, F 0.605)]  [G loss: -0.598] \n",
      "4144 [D loss: (0.055)(R -0.475, F 0.585)]  [G loss: -0.582] \n",
      "4145 [D loss: (0.049)(R -0.480, F 0.578)]  [G loss: -0.580] \n",
      "4145 [D loss: (0.044)(R -0.475, F 0.563)]  [G loss: -0.582] \n",
      "4146 [D loss: (0.043)(R -0.475, F 0.560)]  [G loss: -0.580] \n",
      "4146 [D loss: (0.053)(R -0.472, F 0.577)]  [G loss: -0.573] \n",
      "4147 [D loss: (0.061)(R -0.466, F 0.588)]  [G loss: -0.563] \n",
      "4147 [D loss: (0.049)(R -0.471, F 0.570)]  [G loss: -0.563] \n",
      "4148 [D loss: (0.051)(R -0.465, F 0.566)]  [G loss: -0.566] \n",
      "4148 [D loss: (0.062)(R -0.454, F 0.579)]  [G loss: -0.561] \n",
      "4149 [D loss: (0.052)(R -0.455, F 0.559)]  [G loss: -0.564] \n",
      "4149 [D loss: (0.053)(R -0.461, F 0.567)]  [G loss: -0.553] \n",
      "4150 [D loss: (0.050)(R -0.441, F 0.541)]  [G loss: -0.552] \n",
      "4150 [D loss: (0.042)(R -0.461, F 0.546)]  [G loss: -0.547] \n",
      "4151 [D loss: (0.048)(R -0.449, F 0.544)]  [G loss: -0.543] \n",
      "4151 [D loss: (0.050)(R -0.442, F 0.541)]  [G loss: -0.538] \n",
      "4152 [D loss: (0.052)(R -0.444, F 0.548)]  [G loss: -0.544] \n",
      "4152 [D loss: (0.056)(R -0.432, F 0.545)]  [G loss: -0.532] \n",
      "4153 [D loss: (0.041)(R -0.441, F 0.523)]  [G loss: -0.535] \n",
      "4153 [D loss: (0.046)(R -0.446, F 0.538)]  [G loss: -0.526] \n",
      "4154 [D loss: (0.045)(R -0.437, F 0.526)]  [G loss: -0.527] \n",
      "4154 [D loss: (0.037)(R -0.435, F 0.510)]  [G loss: -0.515] \n",
      "4155 [D loss: (0.038)(R -0.440, F 0.517)]  [G loss: -0.512] \n",
      "4155 [D loss: (0.054)(R -0.426, F 0.533)]  [G loss: -0.512] \n",
      "4156 [D loss: (0.039)(R -0.433, F 0.510)]  [G loss: -0.508] \n",
      "4156 [D loss: (0.036)(R -0.431, F 0.503)]  [G loss: -0.505] \n",
      "4157 [D loss: (0.037)(R -0.421, F 0.494)]  [G loss: -0.505] \n",
      "4157 [D loss: (0.047)(R -0.421, F 0.516)]  [G loss: -0.497] \n",
      "4158 [D loss: (0.043)(R -0.419, F 0.506)]  [G loss: -0.495] \n",
      "4158 [D loss: (0.030)(R -0.428, F 0.489)]  [G loss: -0.490] \n",
      "4159 [D loss: (0.029)(R -0.418, F 0.477)]  [G loss: -0.487] \n",
      "4159 [D loss: (0.034)(R -0.412, F 0.480)]  [G loss: -0.479] \n",
      "4160 [D loss: (0.040)(R -0.407, F 0.487)]  [G loss: -0.485] \n",
      "4160 [D loss: (0.044)(R -0.402, F 0.490)]  [G loss: -0.479] \n",
      "4161 [D loss: (0.041)(R -0.397, F 0.479)]  [G loss: -0.478] \n",
      "4161 [D loss: (0.033)(R -0.407, F 0.472)]  [G loss: -0.480] \n",
      "4162 [D loss: (0.049)(R -0.391, F 0.489)]  [G loss: -0.471] \n",
      "4162 [D loss: (0.039)(R -0.394, F 0.471)]  [G loss: -0.462] \n",
      "4163 [D loss: (0.032)(R -0.400, F 0.463)]  [G loss: -0.469] \n",
      "4163 [D loss: (0.033)(R -0.400, F 0.466)]  [G loss: -0.458] \n",
      "4164 [D loss: (0.041)(R -0.379, F 0.460)]  [G loss: -0.456] \n",
      "4164 [D loss: (0.038)(R -0.390, F 0.465)]  [G loss: -0.448] \n",
      "4165 [D loss: (0.018)(R -0.392, F 0.428)]  [G loss: -0.453] \n",
      "4165 [D loss: (0.029)(R -0.384, F 0.442)]  [G loss: -0.449] \n",
      "4166 [D loss: (0.022)(R -0.387, F 0.432)]  [G loss: -0.440] \n",
      "4166 [D loss: (0.040)(R -0.385, F 0.464)]  [G loss: -0.443] \n",
      "4167 [D loss: (0.037)(R -0.374, F 0.449)]  [G loss: -0.436] \n",
      "4167 [D loss: (0.040)(R -0.373, F 0.452)]  [G loss: -0.434] \n",
      "4168 [D loss: (0.028)(R -0.371, F 0.427)]  [G loss: -0.434] \n",
      "4168 [D loss: (0.029)(R -0.371, F 0.429)]  [G loss: -0.425] \n",
      "4169 [D loss: (0.030)(R -0.372, F 0.431)]  [G loss: -0.421] \n",
      "4169 [D loss: (0.023)(R -0.372, F 0.417)]  [G loss: -0.423] \n",
      "4170 [D loss: (0.027)(R -0.368, F 0.421)]  [G loss: -0.414] \n",
      "4170 [D loss: (0.028)(R -0.366, F 0.421)]  [G loss: -0.420] \n",
      "4171 [D loss: (0.021)(R -0.372, F 0.413)]  [G loss: -0.412] \n",
      "4171 [D loss: (0.027)(R -0.356, F 0.411)]  [G loss: -0.409] \n",
      "4172 [D loss: (0.026)(R -0.354, F 0.407)]  [G loss: -0.410] \n",
      "4172 [D loss: (0.023)(R -0.358, F 0.405)]  [G loss: -0.408] \n",
      "4173 [D loss: (0.012)(R -0.363, F 0.387)]  [G loss: -0.397] \n",
      "4173 [D loss: (0.015)(R -0.360, F 0.390)]  [G loss: -0.395] \n",
      "4174 [D loss: (0.025)(R -0.352, F 0.402)]  [G loss: -0.398] \n",
      "4174 [D loss: (0.019)(R -0.360, F 0.398)]  [G loss: -0.397] \n",
      "4175 [D loss: (0.021)(R -0.347, F 0.388)]  [G loss: -0.392] \n",
      "4175 [D loss: (0.021)(R -0.339, F 0.381)]  [G loss: -0.384] \n",
      "4176 [D loss: (0.024)(R -0.343, F 0.391)]  [G loss: -0.384] \n",
      "4176 [D loss: (0.026)(R -0.335, F 0.388)]  [G loss: -0.382] \n",
      "4177 [D loss: (0.015)(R -0.342, F 0.373)]  [G loss: -0.382] \n",
      "4177 [D loss: (0.024)(R -0.339, F 0.387)]  [G loss: -0.376] \n",
      "4178 [D loss: (0.018)(R -0.347, F 0.383)]  [G loss: -0.372] \n",
      "4178 [D loss: (0.019)(R -0.333, F 0.370)]  [G loss: -0.374] \n",
      "4179 [D loss: (0.017)(R -0.339, F 0.373)]  [G loss: -0.362] \n",
      "4179 [D loss: (0.013)(R -0.331, F 0.357)]  [G loss: -0.358] \n",
      "4180 [D loss: (0.015)(R -0.331, F 0.361)]  [G loss: -0.361] \n",
      "4180 [D loss: (0.021)(R -0.330, F 0.372)]  [G loss: -0.357] \n",
      "4181 [D loss: (0.018)(R -0.322, F 0.358)]  [G loss: -0.357] \n",
      "4181 [D loss: (0.008)(R -0.330, F 0.347)]  [G loss: -0.356] \n",
      "4182 [D loss: (0.013)(R -0.326, F 0.352)]  [G loss: -0.352] \n",
      "4182 [D loss: (0.017)(R -0.313, F 0.346)]  [G loss: -0.350] \n",
      "4183 [D loss: (0.016)(R -0.321, F 0.354)]  [G loss: -0.345] \n",
      "4183 [D loss: (0.012)(R -0.316, F 0.340)]  [G loss: -0.341] \n",
      "4184 [D loss: (0.009)(R -0.320, F 0.339)]  [G loss: -0.342] \n",
      "4184 [D loss: (0.012)(R -0.314, F 0.337)]  [G loss: -0.330] \n",
      "4185 [D loss: (0.006)(R -0.305, F 0.317)]  [G loss: -0.331] \n",
      "4185 [D loss: (0.012)(R -0.311, F 0.334)]  [G loss: -0.330] \n",
      "4186 [D loss: (0.011)(R -0.305, F 0.326)]  [G loss: -0.329] \n",
      "4186 [D loss: (0.005)(R -0.307, F 0.318)]  [G loss: -0.327] \n",
      "4187 [D loss: (0.013)(R -0.299, F 0.324)]  [G loss: -0.321] \n",
      "4187 [D loss: (0.006)(R -0.301, F 0.312)]  [G loss: -0.321] \n",
      "4188 [D loss: (0.005)(R -0.305, F 0.314)]  [G loss: -0.314] \n",
      "4188 [D loss: (0.008)(R -0.305, F 0.320)]  [G loss: -0.314] \n",
      "4189 [D loss: (0.003)(R -0.302, F 0.309)]  [G loss: -0.311] \n",
      "4189 [D loss: (0.013)(R -0.283, F 0.309)]  [G loss: -0.311] \n",
      "4190 [D loss: (0.006)(R -0.293, F 0.306)]  [G loss: -0.310] \n",
      "4190 [D loss: (0.005)(R -0.294, F 0.305)]  [G loss: -0.301] \n",
      "4191 [D loss: (0.004)(R -0.297, F 0.305)]  [G loss: -0.306] \n",
      "4191 [D loss: (0.004)(R -0.289, F 0.296)]  [G loss: -0.298] \n",
      "4192 [D loss: (0.002)(R -0.284, F 0.289)]  [G loss: -0.294] \n",
      "4192 [D loss: (0.004)(R -0.283, F 0.291)]  [G loss: -0.291] \n",
      "4193 [D loss: (-0.001)(R -0.288, F 0.285)]  [G loss: -0.291] \n",
      "4193 [D loss: (0.006)(R -0.282, F 0.294)]  [G loss: -0.288] \n",
      "4194 [D loss: (0.010)(R -0.277, F 0.296)]  [G loss: -0.285] \n",
      "4194 [D loss: (0.005)(R -0.277, F 0.286)]  [G loss: -0.282] \n",
      "4195 [D loss: (-0.000)(R -0.283, F 0.283)]  [G loss: -0.279] \n",
      "4195 [D loss: (0.004)(R -0.268, F 0.276)]  [G loss: -0.280] \n",
      "4196 [D loss: (0.000)(R -0.280, F 0.280)]  [G loss: -0.273] \n",
      "4196 [D loss: (0.005)(R -0.271, F 0.281)]  [G loss: -0.275] \n",
      "4197 [D loss: (0.002)(R -0.276, F 0.281)]  [G loss: -0.271] \n",
      "4197 [D loss: (-0.003)(R -0.276, F 0.270)]  [G loss: -0.265] \n",
      "4198 [D loss: (0.001)(R -0.265, F 0.267)]  [G loss: -0.264] \n",
      "4198 [D loss: (-0.005)(R -0.267, F 0.257)]  [G loss: -0.259] \n",
      "4199 [D loss: (0.004)(R -0.262, F 0.270)]  [G loss: -0.253] \n",
      "4199 [D loss: (-0.007)(R -0.260, F 0.246)]  [G loss: -0.255] \n",
      "4200 [D loss: (-0.015)(R -0.267, F 0.238)]  [G loss: -0.254] \n",
      "4200 [D loss: (-0.012)(R -0.263, F 0.239)]  [G loss: -0.250] \n",
      "4201 [D loss: (-0.007)(R -0.256, F 0.243)]  [G loss: -0.245] \n",
      "4201 [D loss: (-0.010)(R -0.255, F 0.235)]  [G loss: -0.243] \n",
      "4202 [D loss: (-0.008)(R -0.249, F 0.232)]  [G loss: -0.240] \n",
      "4202 [D loss: (-0.009)(R -0.250, F 0.233)]  [G loss: -0.236] \n",
      "4203 [D loss: (-0.004)(R -0.245, F 0.238)]  [G loss: -0.235] \n",
      "4203 [D loss: (-0.008)(R -0.249, F 0.232)]  [G loss: -0.227] \n",
      "4204 [D loss: (-0.005)(R -0.238, F 0.229)]  [G loss: -0.228] \n",
      "4204 [D loss: (-0.009)(R -0.243, F 0.225)]  [G loss: -0.228] \n",
      "4205 [D loss: (-0.013)(R -0.242, F 0.217)]  [G loss: -0.221] \n",
      "4205 [D loss: (-0.014)(R -0.241, F 0.213)]  [G loss: -0.220] \n",
      "4206 [D loss: (-0.015)(R -0.241, F 0.211)]  [G loss: -0.220] \n",
      "4206 [D loss: (-0.014)(R -0.239, F 0.211)]  [G loss: -0.217] \n",
      "4207 [D loss: (-0.013)(R -0.239, F 0.212)]  [G loss: -0.208] \n",
      "4207 [D loss: (-0.012)(R -0.232, F 0.209)]  [G loss: -0.210] \n",
      "4208 [D loss: (-0.015)(R -0.234, F 0.204)]  [G loss: -0.203] \n",
      "4208 [D loss: (-0.019)(R -0.233, F 0.194)]  [G loss: -0.201] \n",
      "4209 [D loss: (-0.011)(R -0.228, F 0.206)]  [G loss: -0.200] \n",
      "4209 [D loss: (-0.016)(R -0.221, F 0.189)]  [G loss: -0.200] \n",
      "4210 [D loss: (-0.018)(R -0.226, F 0.189)]  [G loss: -0.198] \n",
      "4210 [D loss: (-0.014)(R -0.224, F 0.197)]  [G loss: -0.196] \n",
      "4211 [D loss: (-0.012)(R -0.220, F 0.197)]  [G loss: -0.190] \n",
      "4211 [D loss: (-0.011)(R -0.221, F 0.198)]  [G loss: -0.189] \n",
      "4212 [D loss: (-0.017)(R -0.218, F 0.184)]  [G loss: -0.183] \n",
      "4212 [D loss: (-0.018)(R -0.216, F 0.180)]  [G loss: -0.178] \n",
      "4213 [D loss: (-0.019)(R -0.211, F 0.173)]  [G loss: -0.179] \n",
      "4213 [D loss: (-0.019)(R -0.216, F 0.178)]  [G loss: -0.174] \n",
      "4214 [D loss: (-0.013)(R -0.210, F 0.184)]  [G loss: -0.176] \n",
      "4214 [D loss: (-0.017)(R -0.212, F 0.178)]  [G loss: -0.177] \n",
      "4215 [D loss: (-0.025)(R -0.211, F 0.161)]  [G loss: -0.168] \n",
      "4215 [D loss: (-0.017)(R -0.209, F 0.175)]  [G loss: -0.168] \n",
      "4216 [D loss: (-0.018)(R -0.206, F 0.169)]  [G loss: -0.169] \n",
      "4216 [D loss: (-0.021)(R -0.202, F 0.160)]  [G loss: -0.162] \n",
      "4217 [D loss: (-0.019)(R -0.201, F 0.163)]  [G loss: -0.157] \n",
      "4217 [D loss: (-0.020)(R -0.199, F 0.159)]  [G loss: -0.157] \n",
      "4218 [D loss: (-0.025)(R -0.201, F 0.151)]  [G loss: -0.156] \n",
      "4218 [D loss: (-0.024)(R -0.200, F 0.151)]  [G loss: -0.147] \n",
      "4219 [D loss: (-0.028)(R -0.198, F 0.141)]  [G loss: -0.143] \n",
      "4219 [D loss: (-0.022)(R -0.190, F 0.145)]  [G loss: -0.142] \n",
      "4220 [D loss: (-0.025)(R -0.192, F 0.141)]  [G loss: -0.148] \n",
      "4220 [D loss: (-0.026)(R -0.189, F 0.136)]  [G loss: -0.140] \n",
      "4221 [D loss: (-0.028)(R -0.192, F 0.136)]  [G loss: -0.135] \n",
      "4221 [D loss: (-0.027)(R -0.185, F 0.131)]  [G loss: -0.141] \n",
      "4222 [D loss: (-0.032)(R -0.192, F 0.127)]  [G loss: -0.135] \n",
      "4222 [D loss: (-0.025)(R -0.183, F 0.132)]  [G loss: -0.125] \n",
      "4223 [D loss: (-0.029)(R -0.180, F 0.121)]  [G loss: -0.127] \n",
      "4223 [D loss: (-0.036)(R -0.190, F 0.118)]  [G loss: -0.119] \n",
      "4224 [D loss: (-0.034)(R -0.185, F 0.118)]  [G loss: -0.112] \n",
      "4224 [D loss: (-0.032)(R -0.175, F 0.110)]  [G loss: -0.114] \n",
      "4225 [D loss: (-0.019)(R -0.162, F 0.123)]  [G loss: -0.111] \n",
      "4225 [D loss: (-0.032)(R -0.171, F 0.108)]  [G loss: -0.109] \n",
      "4226 [D loss: (-0.031)(R -0.170, F 0.107)]  [G loss: -0.106] \n",
      "4226 [D loss: (-0.031)(R -0.175, F 0.113)]  [G loss: -0.111] \n",
      "4227 [D loss: (-0.035)(R -0.180, F 0.111)]  [G loss: -0.104] \n",
      "4227 [D loss: (-0.034)(R -0.172, F 0.105)]  [G loss: -0.102] \n",
      "4228 [D loss: (-0.040)(R -0.171, F 0.091)]  [G loss: -0.096] \n",
      "4228 [D loss: (-0.030)(R -0.157, F 0.097)]  [G loss: -0.093] \n",
      "4229 [D loss: (-0.043)(R -0.166, F 0.081)]  [G loss: -0.088] \n",
      "4229 [D loss: (-0.041)(R -0.166, F 0.083)]  [G loss: -0.083] \n",
      "4230 [D loss: (-0.041)(R -0.162, F 0.080)]  [G loss: -0.089] \n",
      "4230 [D loss: (-0.046)(R -0.156, F 0.064)]  [G loss: -0.080] \n",
      "4231 [D loss: (-0.038)(R -0.152, F 0.076)]  [G loss: -0.082] \n",
      "4231 [D loss: (-0.045)(R -0.162, F 0.072)]  [G loss: -0.070] \n",
      "4232 [D loss: (-0.036)(R -0.151, F 0.079)]  [G loss: -0.062] \n",
      "4232 [D loss: (-0.037)(R -0.145, F 0.071)]  [G loss: -0.063] \n",
      "4233 [D loss: (-0.045)(R -0.149, F 0.060)]  [G loss: -0.066] \n",
      "4233 [D loss: (-0.035)(R -0.138, F 0.069)]  [G loss: -0.063] \n",
      "4234 [D loss: (-0.046)(R -0.144, F 0.051)]  [G loss: -0.053] \n",
      "4234 [D loss: (-0.049)(R -0.147, F 0.048)]  [G loss: -0.058] \n",
      "4235 [D loss: (-0.046)(R -0.144, F 0.053)]  [G loss: -0.054] \n",
      "4235 [D loss: (-0.045)(R -0.129, F 0.039)]  [G loss: -0.045] \n",
      "4236 [D loss: (-0.049)(R -0.145, F 0.046)]  [G loss: -0.047] \n",
      "4236 [D loss: (-0.049)(R -0.137, F 0.039)]  [G loss: -0.048] \n",
      "4237 [D loss: (-0.052)(R -0.131, F 0.027)]  [G loss: -0.045] \n",
      "4237 [D loss: (-0.051)(R -0.129, F 0.027)]  [G loss: -0.030] \n",
      "4238 [D loss: (-0.051)(R -0.127, F 0.025)]  [G loss: -0.038] \n",
      "4238 [D loss: (-0.045)(R -0.125, F 0.035)]  [G loss: -0.026] \n",
      "4239 [D loss: (-0.044)(R -0.123, F 0.035)]  [G loss: -0.019] \n",
      "4239 [D loss: (-0.044)(R -0.119, F 0.031)]  [G loss: -0.024] \n",
      "4240 [D loss: (-0.053)(R -0.130, F 0.025)]  [G loss: -0.021] \n",
      "4240 [D loss: (-0.049)(R -0.121, F 0.023)]  [G loss: -0.023] \n",
      "4241 [D loss: (-0.050)(R -0.117, F 0.016)]  [G loss: 0.000] \n",
      "4241 [D loss: (-0.055)(R -0.123, F 0.013)]  [G loss: -0.004] \n",
      "4242 [D loss: (-0.054)(R -0.121, F 0.014)]  [G loss: -0.012] \n",
      "4242 [D loss: (-0.066)(R -0.118, F -0.014)]  [G loss: 0.005] \n",
      "4243 [D loss: (-0.061)(R -0.121, F -0.000)]  [G loss: -0.002] \n",
      "4243 [D loss: (-0.061)(R -0.108, F -0.013)]  [G loss: 0.019] \n",
      "4244 [D loss: (-0.056)(R -0.106, F -0.006)]  [G loss: 0.012] \n",
      "4244 [D loss: (-0.052)(R -0.098, F -0.006)]  [G loss: 0.012] \n",
      "4245 [D loss: (-0.059)(R -0.097, F -0.021)]  [G loss: 0.024] \n",
      "4245 [D loss: (-0.059)(R -0.097, F -0.021)]  [G loss: 0.023] \n",
      "4246 [D loss: (-0.062)(R -0.100, F -0.025)]  [G loss: 0.023] \n",
      "4246 [D loss: (-0.071)(R -0.098, F -0.044)]  [G loss: 0.042] \n",
      "4247 [D loss: (-0.075)(R -0.102, F -0.048)]  [G loss: 0.037] \n",
      "4247 [D loss: (-0.073)(R -0.089, F -0.057)]  [G loss: 0.029] \n",
      "4248 [D loss: (-0.079)(R -0.093, F -0.066)]  [G loss: 0.040] \n",
      "4248 [D loss: (-0.063)(R -0.079, F -0.047)]  [G loss: 0.047] \n",
      "4249 [D loss: (-0.067)(R -0.083, F -0.052)]  [G loss: 0.062] \n",
      "4249 [D loss: (-0.065)(R -0.085, F -0.044)]  [G loss: 0.058] \n",
      "4250 [D loss: (-0.076)(R -0.093, F -0.059)]  [G loss: 0.059] \n",
      "4250 [D loss: (-0.071)(R -0.076, F -0.066)]  [G loss: 0.064] \n",
      "4251 [D loss: (-0.075)(R -0.079, F -0.071)]  [G loss: 0.063] \n",
      "4251 [D loss: (-0.067)(R -0.063, F -0.071)]  [G loss: 0.079] \n",
      "4252 [D loss: (-0.067)(R -0.075, F -0.059)]  [G loss: 0.078] \n",
      "4252 [D loss: (-0.075)(R -0.073, F -0.077)]  [G loss: 0.075] \n",
      "4253 [D loss: (-0.078)(R -0.070, F -0.086)]  [G loss: 0.085] \n",
      "4253 [D loss: (-0.069)(R -0.058, F -0.081)]  [G loss: 0.096] \n",
      "4254 [D loss: (-0.091)(R -0.068, F -0.114)]  [G loss: 0.096] \n",
      "4254 [D loss: (-0.070)(R -0.056, F -0.084)]  [G loss: 0.094] \n",
      "4255 [D loss: (-0.085)(R -0.058, F -0.112)]  [G loss: 0.112] \n",
      "4255 [D loss: (-0.088)(R -0.055, F -0.121)]  [G loss: 0.100] \n",
      "4256 [D loss: (-0.071)(R -0.052, F -0.090)]  [G loss: 0.116] \n",
      "4256 [D loss: (-0.090)(R -0.058, F -0.122)]  [G loss: 0.124] \n",
      "4257 [D loss: (-0.095)(R -0.059, F -0.130)]  [G loss: 0.129] \n",
      "4257 [D loss: (-0.086)(R -0.046, F -0.126)]  [G loss: 0.137] \n",
      "4258 [D loss: (-0.095)(R -0.042, F -0.148)]  [G loss: 0.131] \n",
      "4258 [D loss: (-0.070)(R -0.025, F -0.115)]  [G loss: 0.147] \n",
      "4259 [D loss: (-0.086)(R -0.035, F -0.138)]  [G loss: 0.144] \n",
      "4259 [D loss: (-0.095)(R -0.017, F -0.173)]  [G loss: 0.151] \n",
      "4260 [D loss: (-0.118)(R -0.032, F -0.203)]  [G loss: 0.156] \n",
      "4260 [D loss: (-0.100)(R -0.023, F -0.176)]  [G loss: 0.155] \n",
      "4261 [D loss: (-0.078)(R -0.014, F -0.142)]  [G loss: 0.168] \n",
      "4261 [D loss: (-0.089)(R -0.033, F -0.145)]  [G loss: 0.165] \n",
      "4262 [D loss: (-0.104)(R -0.022, F -0.186)]  [G loss: 0.188] \n",
      "4262 [D loss: (-0.095)(R -0.021, F -0.168)]  [G loss: 0.186] \n",
      "4263 [D loss: (-0.105)(R -0.017, F -0.193)]  [G loss: 0.188] \n",
      "4263 [D loss: (-0.104)(R -0.001, F -0.208)]  [G loss: 0.193] \n",
      "4264 [D loss: (-0.100)(R -0.012, F -0.188)]  [G loss: 0.207] \n",
      "4264 [D loss: (-0.106)(R 0.012, F -0.224)]  [G loss: 0.213] \n",
      "4265 [D loss: (-0.103)(R 0.008, F -0.214)]  [G loss: 0.212] \n",
      "4265 [D loss: (-0.123)(R -0.004, F -0.241)]  [G loss: 0.234] \n",
      "4266 [D loss: (-0.131)(R 0.002, F -0.265)]  [G loss: 0.233] \n",
      "4266 [D loss: (-0.136)(R 0.013, F -0.284)]  [G loss: 0.234] \n",
      "4267 [D loss: (-0.103)(R 0.022, F -0.228)]  [G loss: 0.235] \n",
      "4267 [D loss: (-0.131)(R 0.021, F -0.284)]  [G loss: 0.234] \n",
      "4268 [D loss: (-0.115)(R 0.006, F -0.236)]  [G loss: 0.249] \n",
      "4268 [D loss: (-0.101)(R 0.027, F -0.230)]  [G loss: 0.270] \n",
      "4269 [D loss: (-0.121)(R 0.028, F -0.270)]  [G loss: 0.284] \n",
      "4269 [D loss: (-0.143)(R 0.031, F -0.317)]  [G loss: 0.289] \n",
      "4270 [D loss: (-0.127)(R 0.038, F -0.292)]  [G loss: 0.295] \n",
      "4270 [D loss: (-0.143)(R 0.015, F -0.300)]  [G loss: 0.307] \n",
      "4271 [D loss: (-0.134)(R 0.044, F -0.312)]  [G loss: 0.307] \n",
      "4271 [D loss: (-0.126)(R 0.064, F -0.316)]  [G loss: 0.304] \n",
      "4272 [D loss: (-0.158)(R 0.049, F -0.365)]  [G loss: 0.315] \n",
      "4272 [D loss: (-0.137)(R 0.045, F -0.320)]  [G loss: 0.332] \n",
      "4273 [D loss: (-0.136)(R 0.077, F -0.349)]  [G loss: 0.349] \n",
      "4273 [D loss: (-0.144)(R 0.060, F -0.348)]  [G loss: 0.358] \n",
      "4274 [D loss: (-0.147)(R 0.065, F -0.359)]  [G loss: 0.367] \n",
      "4274 [D loss: (-0.125)(R 0.085, F -0.335)]  [G loss: 0.335] \n",
      "4275 [D loss: (-0.145)(R 0.074, F -0.365)]  [G loss: 0.355] \n",
      "4275 [D loss: (-0.174)(R 0.080, F -0.429)]  [G loss: 0.372] \n",
      "4276 [D loss: (-0.154)(R 0.074, F -0.382)]  [G loss: 0.394] \n",
      "4276 [D loss: (-0.148)(R 0.107, F -0.402)]  [G loss: 0.399] \n",
      "4277 [D loss: (-0.162)(R 0.083, F -0.408)]  [G loss: 0.396] \n",
      "4277 [D loss: (-0.162)(R 0.072, F -0.396)]  [G loss: 0.423] \n",
      "4278 [D loss: (-0.177)(R 0.100, F -0.453)]  [G loss: 0.433] \n",
      "4278 [D loss: (-0.157)(R 0.120, F -0.433)]  [G loss: 0.425] \n",
      "4279 [D loss: (-0.172)(R 0.108, F -0.452)]  [G loss: 0.444] \n",
      "4279 [D loss: (-0.168)(R 0.134, F -0.470)]  [G loss: 0.447] \n",
      "4280 [D loss: (-0.174)(R 0.131, F -0.479)]  [G loss: 0.474] \n",
      "4280 [D loss: (-0.170)(R 0.134, F -0.474)]  [G loss: 0.477] \n",
      "4281 [D loss: (-0.205)(R 0.106, F -0.516)]  [G loss: 0.479] \n",
      "4281 [D loss: (-0.159)(R 0.140, F -0.458)]  [G loss: 0.518] \n",
      "4282 [D loss: (-0.153)(R 0.160, F -0.467)]  [G loss: 0.532] \n",
      "4282 [D loss: (-0.180)(R 0.153, F -0.512)]  [G loss: 0.518] \n",
      "4283 [D loss: (-0.191)(R 0.147, F -0.528)]  [G loss: 0.550] \n",
      "4283 [D loss: (-0.200)(R 0.159, F -0.558)]  [G loss: 0.538] \n",
      "4284 [D loss: (-0.225)(R 0.166, F -0.615)]  [G loss: 0.571] \n",
      "4284 [D loss: (-0.207)(R 0.166, F -0.581)]  [G loss: 0.597] \n",
      "4285 [D loss: (-0.210)(R 0.177, F -0.597)]  [G loss: 0.576] \n",
      "4285 [D loss: (-0.176)(R 0.185, F -0.537)]  [G loss: 0.609] \n",
      "4286 [D loss: (-0.194)(R 0.200, F -0.588)]  [G loss: 0.617] \n",
      "4286 [D loss: (-0.204)(R 0.218, F -0.625)]  [G loss: 0.625] \n",
      "4287 [D loss: (-0.220)(R 0.196, F -0.637)]  [G loss: 0.648] \n",
      "4287 [D loss: (-0.234)(R 0.174, F -0.642)]  [G loss: 0.658] \n",
      "4288 [D loss: (-0.180)(R 0.235, F -0.595)]  [G loss: 0.660] \n",
      "4288 [D loss: (-0.228)(R 0.238, F -0.694)]  [G loss: 0.696] \n",
      "4289 [D loss: (-0.268)(R 0.223, F -0.759)]  [G loss: 0.707] \n",
      "4289 [D loss: (-0.216)(R 0.238, F -0.670)]  [G loss: 0.733] \n",
      "4290 [D loss: (-0.190)(R 0.283, F -0.663)]  [G loss: 0.756] \n",
      "4290 [D loss: (-0.250)(R 0.250, F -0.750)]  [G loss: 0.751] \n",
      "4291 [D loss: (-0.224)(R 0.292, F -0.740)]  [G loss: 0.775] \n",
      "4291 [D loss: (-0.258)(R 0.272, F -0.788)]  [G loss: 0.781] \n",
      "4292 [D loss: (-0.256)(R 0.265, F -0.776)]  [G loss: 0.814] \n",
      "4292 [D loss: (-0.266)(R 0.297, F -0.830)]  [G loss: 0.830] \n",
      "4293 [D loss: (-0.291)(R 0.262, F -0.843)]  [G loss: 0.817] \n",
      "4293 [D loss: (-0.304)(R 0.322, F -0.930)]  [G loss: 0.866] \n",
      "4294 [D loss: (-0.303)(R 0.307, F -0.913)]  [G loss: 0.850] \n",
      "4294 [D loss: (-0.255)(R 0.310, F -0.821)]  [G loss: 0.918] \n",
      "4295 [D loss: (-0.288)(R 0.333, F -0.909)]  [G loss: 0.970] \n",
      "4295 [D loss: (-0.356)(R 0.356, F -1.068)]  [G loss: 0.913] \n",
      "4296 [D loss: (-0.316)(R 0.361, F -0.994)]  [G loss: 0.969] \n",
      "4296 [D loss: (-0.282)(R 0.398, F -0.962)]  [G loss: 1.015] \n",
      "4297 [D loss: (-0.317)(R 0.384, F -1.017)]  [G loss: 0.996] \n",
      "4297 [D loss: (-0.282)(R 0.393, F -0.956)]  [G loss: 1.008] \n",
      "4298 [D loss: (-0.289)(R 0.407, F -0.985)]  [G loss: 1.081] \n",
      "4298 [D loss: (-0.370)(R 0.421, F -1.162)]  [G loss: 1.082] \n",
      "4299 [D loss: (-0.343)(R 0.382, F -1.068)]  [G loss: 1.087] \n",
      "4299 [D loss: (-0.308)(R 0.464, F -1.080)]  [G loss: 1.168] \n",
      "4300 [D loss: (-0.381)(R 0.430, F -1.192)]  [G loss: 1.123] \n",
      "4300 [D loss: (-0.329)(R 0.448, F -1.106)]  [G loss: 1.165] \n",
      "4301 [D loss: (-0.320)(R 0.517, F -1.157)]  [G loss: 1.204] \n",
      "4301 [D loss: (-0.337)(R 0.505, F -1.179)]  [G loss: 1.231] \n",
      "4302 [D loss: (-0.273)(R 0.522, F -1.069)]  [G loss: 1.303] \n",
      "4302 [D loss: (-0.270)(R 0.590, F -1.130)]  [G loss: 1.287] \n",
      "4303 [D loss: (-0.376)(R 0.531, F -1.284)]  [G loss: 1.305] \n",
      "4303 [D loss: (-0.356)(R 0.588, F -1.300)]  [G loss: 1.367] \n",
      "4304 [D loss: (-0.386)(R 0.584, F -1.356)]  [G loss: 1.393] \n",
      "4304 [D loss: (-0.432)(R 0.579, F -1.443)]  [G loss: 1.391] \n",
      "4305 [D loss: (-0.390)(R 0.608, F -1.389)]  [G loss: 1.463] \n",
      "4305 [D loss: (-0.354)(R 0.657, F -1.364)]  [G loss: 1.473] \n",
      "4306 [D loss: (-0.444)(R 0.630, F -1.517)]  [G loss: 1.558] \n",
      "4306 [D loss: (-0.364)(R 0.680, F -1.408)]  [G loss: 1.534] \n",
      "4307 [D loss: (-0.379)(R 0.686, F -1.444)]  [G loss: 1.572] \n",
      "4307 [D loss: (-0.417)(R 0.712, F -1.545)]  [G loss: 1.633] \n",
      "4308 [D loss: (-0.490)(R 0.691, F -1.670)]  [G loss: 1.613] \n",
      "4308 [D loss: (-0.489)(R 0.725, F -1.704)]  [G loss: 1.719] \n",
      "4309 [D loss: (-0.399)(R 0.772, F -1.571)]  [G loss: 1.737] \n",
      "4309 [D loss: (-0.530)(R 0.778, F -1.837)]  [G loss: 1.762] \n",
      "4310 [D loss: (-0.554)(R 0.791, F -1.900)]  [G loss: 1.762] \n",
      "4310 [D loss: (-0.471)(R 0.856, F -1.798)]  [G loss: 1.877] \n",
      "4311 [D loss: (-0.476)(R 0.845, F -1.797)]  [G loss: 1.875] \n",
      "4311 [D loss: (-0.529)(R 0.890, F -1.948)]  [G loss: 1.910] \n",
      "4312 [D loss: (-0.583)(R 0.863, F -2.029)]  [G loss: 2.011] \n",
      "4312 [D loss: (-0.565)(R 0.834, F -1.964)]  [G loss: 2.024] \n",
      "4313 [D loss: (-0.659)(R 0.862, F -2.179)]  [G loss: 2.036] \n",
      "4313 [D loss: (-0.596)(R 0.928, F -2.120)]  [G loss: 2.104] \n",
      "4314 [D loss: (-0.710)(R 0.916, F -2.335)]  [G loss: 2.119] \n",
      "4314 [D loss: (-0.609)(R 0.943, F -2.161)]  [G loss: 2.158] \n",
      "4315 [D loss: (-0.552)(R 0.920, F -2.025)]  [G loss: 2.219] \n",
      "4315 [D loss: (-0.527)(R 1.038, F -2.092)]  [G loss: 2.343] \n",
      "4316 [D loss: (-0.686)(R 1.020, F -2.393)]  [G loss: 2.328] \n",
      "4316 [D loss: (-0.711)(R 1.097, F -2.519)]  [G loss: 2.372] \n",
      "4317 [D loss: (-0.728)(R 1.044, F -2.500)]  [G loss: 2.436] \n",
      "4317 [D loss: (-0.669)(R 1.082, F -2.419)]  [G loss: 2.499] \n",
      "4318 [D loss: (-0.666)(R 1.117, F -2.448)]  [G loss: 2.565] \n",
      "4318 [D loss: (-0.723)(R 1.197, F -2.643)]  [G loss: 2.609] \n",
      "4319 [D loss: (-0.735)(R 1.275, F -2.745)]  [G loss: 2.599] \n",
      "4319 [D loss: (-0.746)(R 1.215, F -2.706)]  [G loss: 2.685] \n",
      "4320 [D loss: (-0.742)(R 1.175, F -2.659)]  [G loss: 2.799] \n",
      "4320 [D loss: (-0.627)(R 1.324, F -2.577)]  [G loss: 2.845] \n",
      "4321 [D loss: (-0.822)(R 1.356, F -3.001)]  [G loss: 2.855] \n",
      "4321 [D loss: (-0.803)(R 1.322, F -2.927)]  [G loss: 2.990] \n",
      "4322 [D loss: (-0.843)(R 1.344, F -3.030)]  [G loss: 2.991] \n",
      "4322 [D loss: (-0.849)(R 1.327, F -3.025)]  [G loss: 3.105] \n",
      "4323 [D loss: (-0.907)(R 1.428, F -3.242)]  [G loss: 3.067] \n",
      "4323 [D loss: (-0.758)(R 1.615, F -3.130)]  [G loss: 3.160] \n",
      "4324 [D loss: (-1.006)(R 1.476, F -3.488)]  [G loss: 3.190] \n",
      "4324 [D loss: (-0.715)(R 1.628, F -3.058)]  [G loss: 3.270] \n",
      "4325 [D loss: (-0.840)(R 1.705, F -3.386)]  [G loss: 3.349] \n",
      "4325 [D loss: (-0.879)(R 1.627, F -3.386)]  [G loss: 3.461] \n",
      "4326 [D loss: (-0.836)(R 1.642, F -3.314)]  [G loss: 3.511] \n",
      "4326 [D loss: (-0.875)(R 1.732, F -3.481)]  [G loss: 3.563] \n",
      "4327 [D loss: (-1.009)(R 1.627, F -3.645)]  [G loss: 3.738] \n",
      "4327 [D loss: (-0.957)(R 1.827, F -3.740)]  [G loss: 3.720] \n",
      "4328 [D loss: (-1.014)(R 1.774, F -3.803)]  [G loss: 3.806] \n",
      "4328 [D loss: (-0.937)(R 1.763, F -3.637)]  [G loss: 3.765] \n",
      "4329 [D loss: (-1.014)(R 1.927, F -3.955)]  [G loss: 3.839] \n",
      "4329 [D loss: (-1.054)(R 1.831, F -3.938)]  [G loss: 3.957] \n",
      "4330 [D loss: (-1.023)(R 1.960, F -4.006)]  [G loss: 4.104] \n",
      "4330 [D loss: (-1.112)(R 1.908, F -4.132)]  [G loss: 4.200] \n",
      "4331 [D loss: (-0.880)(R 2.073, F -3.833)]  [G loss: 4.106] \n",
      "4331 [D loss: (-0.869)(R 2.106, F -3.843)]  [G loss: 4.244] \n",
      "4332 [D loss: (-1.208)(R 2.089, F -4.505)]  [G loss: 4.247] \n",
      "4332 [D loss: (-1.171)(R 2.300, F -4.642)]  [G loss: 4.369] \n",
      "4333 [D loss: (-1.111)(R 2.380, F -4.603)]  [G loss: 4.417] \n",
      "4333 [D loss: (-1.181)(R 2.254, F -4.617)]  [G loss: 4.620] \n",
      "4334 [D loss: (-1.403)(R 2.080, F -4.885)]  [G loss: 4.547] \n",
      "4334 [D loss: (-1.173)(R 2.243, F -4.589)]  [G loss: 4.852] \n",
      "4335 [D loss: (-1.193)(R 2.398, F -4.785)]  [G loss: 4.781] \n",
      "4335 [D loss: (-1.232)(R 2.499, F -4.962)]  [G loss: 4.922] \n",
      "4336 [D loss: (-1.442)(R 2.547, F -5.430)]  [G loss: 4.910] \n",
      "4336 [D loss: (-1.431)(R 2.363, F -5.224)]  [G loss: 4.954] \n",
      "4337 [D loss: (-1.337)(R 2.577, F -5.251)]  [G loss: 5.283] \n",
      "4337 [D loss: (-1.444)(R 2.631, F -5.519)]  [G loss: 5.342] \n",
      "4338 [D loss: (-1.454)(R 2.872, F -5.779)]  [G loss: 5.257] \n",
      "4338 [D loss: (-1.336)(R 2.831, F -5.503)]  [G loss: 5.362] \n",
      "4339 [D loss: (-1.599)(R 2.730, F -5.928)]  [G loss: 5.568] \n",
      "4339 [D loss: (-1.503)(R 2.896, F -5.902)]  [G loss: 5.772] \n",
      "4340 [D loss: (-1.414)(R 3.041, F -5.870)]  [G loss: 5.835] \n",
      "4340 [D loss: (-1.362)(R 2.938, F -5.662)]  [G loss: 5.952] \n",
      "4341 [D loss: (-1.450)(R 2.972, F -5.871)]  [G loss: 6.129] \n",
      "4341 [D loss: (-1.433)(R 3.207, F -6.073)]  [G loss: 6.125] \n",
      "4342 [D loss: (-1.568)(R 2.967, F -6.103)]  [G loss: 6.018] \n",
      "4342 [D loss: (-1.635)(R 3.111, F -6.382)]  [G loss: 6.165] \n",
      "4343 [D loss: (-1.330)(R 3.321, F -5.981)]  [G loss: 6.315] \n",
      "4343 [D loss: (-1.625)(R 3.153, F -6.403)]  [G loss: 6.514] \n",
      "4344 [D loss: (-1.630)(R 3.391, F -6.651)]  [G loss: 6.516] \n",
      "4344 [D loss: (-1.748)(R 3.138, F -6.633)]  [G loss: 6.738] \n",
      "4345 [D loss: (-1.809)(R 3.340, F -6.958)]  [G loss: 6.523] \n",
      "4345 [D loss: (-1.627)(R 3.398, F -6.652)]  [G loss: 6.890] \n",
      "4346 [D loss: (-1.505)(R 3.628, F -6.638)]  [G loss: 6.768] \n",
      "4346 [D loss: (-1.695)(R 3.555, F -6.944)]  [G loss: 7.152] \n",
      "4347 [D loss: (-1.679)(R 3.753, F -7.111)]  [G loss: 7.354] \n",
      "4347 [D loss: (-1.732)(R 3.686, F -7.150)]  [G loss: 7.127] \n",
      "4348 [D loss: (-1.720)(R 3.801, F -7.242)]  [G loss: 7.429] \n",
      "4348 [D loss: (-1.825)(R 3.925, F -7.574)]  [G loss: 7.512] \n",
      "4349 [D loss: (-1.774)(R 3.739, F -7.287)]  [G loss: 7.620] \n",
      "4349 [D loss: (-2.037)(R 3.893, F -7.966)]  [G loss: 7.736] \n",
      "4350 [D loss: (-1.676)(R 4.068, F -7.421)]  [G loss: 7.929] \n",
      "4350 [D loss: (-1.949)(R 4.071, F -7.970)]  [G loss: 7.801] \n",
      "4351 [D loss: (-1.769)(R 4.183, F -7.720)]  [G loss: 8.029] \n",
      "4351 [D loss: (-2.048)(R 4.015, F -8.110)]  [G loss: 8.156] \n",
      "4352 [D loss: (-2.128)(R 4.356, F -8.612)]  [G loss: 8.232] \n",
      "4352 [D loss: (-1.648)(R 4.285, F -7.580)]  [G loss: 8.464] \n",
      "4353 [D loss: (-2.335)(R 4.213, F -8.883)]  [G loss: 8.452] \n",
      "4353 [D loss: (-2.209)(R 4.526, F -8.943)]  [G loss: 8.617] \n",
      "4354 [D loss: (-2.042)(R 4.674, F -8.758)]  [G loss: 8.722] \n",
      "4354 [D loss: (-1.644)(R 4.652, F -7.941)]  [G loss: 8.932] \n",
      "4355 [D loss: (-2.074)(R 4.978, F -9.125)]  [G loss: 9.058] \n",
      "4355 [D loss: (-2.121)(R 4.840, F -9.082)]  [G loss: 9.128] \n",
      "4356 [D loss: (-1.899)(R 4.594, F -8.393)]  [G loss: 9.199] \n",
      "4356 [D loss: (-2.132)(R 4.693, F -8.956)]  [G loss: 9.217] \n",
      "4357 [D loss: (-1.977)(R 4.907, F -8.861)]  [G loss: 9.358] \n",
      "4357 [D loss: (-1.872)(R 5.295, F -9.038)]  [G loss: 9.591] \n",
      "4358 [D loss: (-2.221)(R 5.017, F -9.458)]  [G loss: 9.697] \n",
      "4358 [D loss: (-2.128)(R 5.113, F -9.369)]  [G loss: 9.703] \n",
      "4359 [D loss: (-2.308)(R 5.366, F -9.982)]  [G loss: 10.138] \n",
      "4359 [D loss: (-2.012)(R 5.241, F -9.265)]  [G loss: 9.987] \n",
      "4360 [D loss: (-2.269)(R 5.109, F -9.647)]  [G loss: 10.207] \n",
      "4360 [D loss: (-2.860)(R 5.405, F -11.125)]  [G loss: 10.151] \n",
      "4361 [D loss: (-2.576)(R 5.487, F -10.639)]  [G loss: 10.452] \n",
      "4361 [D loss: (-2.300)(R 5.804, F -10.404)]  [G loss: 10.378] \n",
      "4362 [D loss: (-2.384)(R 5.619, F -10.386)]  [G loss: 10.826] \n",
      "4362 [D loss: (-2.541)(R 5.789, F -10.871)]  [G loss: 10.903] \n",
      "4363 [D loss: (-3.074)(R 5.396, F -11.544)]  [G loss: 10.962] \n",
      "4363 [D loss: (-2.608)(R 5.951, F -11.167)]  [G loss: 10.685] \n",
      "4364 [D loss: (-2.931)(R 6.079, F -11.940)]  [G loss: 11.408] \n",
      "4364 [D loss: (-2.941)(R 5.713, F -11.595)]  [G loss: 11.110] \n",
      "4365 [D loss: (-2.615)(R 5.959, F -11.189)]  [G loss: 11.448] \n",
      "4365 [D loss: (-2.311)(R 6.513, F -11.135)]  [G loss: 11.976] \n",
      "4366 [D loss: (-3.247)(R 6.081, F -12.574)]  [G loss: 11.570] \n",
      "4366 [D loss: (-2.794)(R 6.308, F -11.895)]  [G loss: 11.869] \n",
      "4367 [D loss: (-2.550)(R 6.868, F -11.968)]  [G loss: 12.218] \n",
      "4367 [D loss: (-2.640)(R 6.244, F -11.523)]  [G loss: 11.991] \n",
      "4368 [D loss: (-2.422)(R 6.742, F -11.586)]  [G loss: 12.217] \n",
      "4368 [D loss: (-3.310)(R 6.665, F -13.284)]  [G loss: 12.180] \n",
      "4369 [D loss: (-3.177)(R 6.519, F -12.873)]  [G loss: 12.523] \n",
      "4369 [D loss: (-2.499)(R 6.840, F -11.838)]  [G loss: 12.860] \n",
      "4370 [D loss: (-2.586)(R 6.527, F -11.698)]  [G loss: 12.596] \n",
      "4370 [D loss: (-3.277)(R 6.928, F -13.482)]  [G loss: 13.151] \n",
      "4371 [D loss: (-3.064)(R 7.073, F -13.200)]  [G loss: 12.909] \n",
      "4371 [D loss: (-3.415)(R 6.714, F -13.544)]  [G loss: 13.540] \n",
      "4372 [D loss: (-2.542)(R 7.469, F -12.553)]  [G loss: 13.747] \n",
      "4372 [D loss: (-2.824)(R 7.309, F -12.956)]  [G loss: 13.457] \n",
      "4373 [D loss: (-3.054)(R 6.954, F -13.062)]  [G loss: 13.831] \n",
      "4373 [D loss: (-2.650)(R 8.040, F -13.339)]  [G loss: 13.700] \n",
      "4374 [D loss: (-2.516)(R 7.769, F -12.802)]  [G loss: 14.227] \n",
      "4374 [D loss: (-3.426)(R 7.307, F -14.159)]  [G loss: 14.311] \n",
      "4375 [D loss: (-3.411)(R 7.930, F -14.751)]  [G loss: 14.180] \n",
      "4375 [D loss: (-3.187)(R 7.824, F -14.198)]  [G loss: 14.290] \n",
      "4376 [D loss: (-2.869)(R 8.340, F -14.079)]  [G loss: 14.264] \n",
      "4376 [D loss: (-3.211)(R 8.026, F -14.447)]  [G loss: 14.808] \n",
      "4377 [D loss: (-2.969)(R 8.285, F -14.222)]  [G loss: 14.908] \n",
      "4377 [D loss: (-3.061)(R 8.053, F -14.175)]  [G loss: 15.102] \n",
      "4378 [D loss: (-3.333)(R 8.400, F -15.066)]  [G loss: 15.088] \n",
      "4378 [D loss: (-3.849)(R 8.276, F -15.974)]  [G loss: 15.267] \n",
      "4379 [D loss: (-3.056)(R 8.578, F -14.689)]  [G loss: 15.394] \n",
      "4379 [D loss: (-3.368)(R 8.701, F -15.437)]  [G loss: 15.446] \n",
      "4380 [D loss: (-3.377)(R 8.481, F -15.235)]  [G loss: 16.063] \n",
      "4380 [D loss: (-2.995)(R 8.547, F -14.538)]  [G loss: 15.944] \n",
      "4381 [D loss: (-3.392)(R 8.560, F -15.344)]  [G loss: 16.113] \n",
      "4381 [D loss: (-3.662)(R 9.028, F -16.353)]  [G loss: 16.087] \n",
      "4382 [D loss: (-3.910)(R 9.259, F -17.079)]  [G loss: 16.616] \n",
      "4382 [D loss: (-3.231)(R 9.157, F -15.619)]  [G loss: 16.441] \n",
      "4383 [D loss: (-4.275)(R 9.131, F -17.681)]  [G loss: 16.747] \n",
      "4383 [D loss: (-3.922)(R 9.398, F -17.243)]  [G loss: 16.861] \n",
      "4384 [D loss: (-3.182)(R 10.028, F -16.393)]  [G loss: 17.001] \n",
      "4384 [D loss: (-3.382)(R 9.786, F -16.549)]  [G loss: 17.186] \n",
      "4385 [D loss: (-3.721)(R 9.509, F -16.950)]  [G loss: 17.104] \n",
      "4385 [D loss: (-4.232)(R 9.740, F -18.204)]  [G loss: 17.550] \n",
      "4386 [D loss: (-4.358)(R 9.775, F -18.490)]  [G loss: 17.899] \n",
      "4386 [D loss: (-3.792)(R 9.651, F -17.236)]  [G loss: 17.607] \n",
      "4387 [D loss: (-4.140)(R 9.946, F -18.227)]  [G loss: 17.800] \n",
      "4387 [D loss: (-4.046)(R 9.889, F -17.981)]  [G loss: 18.290] \n",
      "4388 [D loss: (-4.099)(R 10.121, F -18.319)]  [G loss: 18.456] \n",
      "4388 [D loss: (-3.876)(R 10.094, F -17.846)]  [G loss: 18.693] \n",
      "4389 [D loss: (-4.391)(R 10.190, F -18.972)]  [G loss: 18.472] \n",
      "4389 [D loss: (-4.523)(R 10.375, F -19.421)]  [G loss: 18.885] \n",
      "4390 [D loss: (-4.902)(R 10.792, F -20.596)]  [G loss: 19.050] \n",
      "4390 [D loss: (-4.985)(R 10.239, F -20.210)]  [G loss: 19.461] \n",
      "4391 [D loss: (-4.891)(R 10.532, F -20.313)]  [G loss: 19.750] \n",
      "4391 [D loss: (-5.040)(R 10.428, F -20.508)]  [G loss: 19.081] \n",
      "4392 [D loss: (-4.095)(R 9.916, F -18.107)]  [G loss: 19.999] \n",
      "4392 [D loss: (-4.603)(R 11.230, F -20.436)]  [G loss: 20.305] \n",
      "4393 [D loss: (-4.882)(R 11.033, F -20.798)]  [G loss: 20.243] \n",
      "4393 [D loss: (-4.785)(R 10.944, F -20.514)]  [G loss: 20.554] \n",
      "4394 [D loss: (-4.656)(R 11.349, F -20.661)]  [G loss: 20.467] \n",
      "4394 [D loss: (-4.752)(R 11.516, F -21.019)]  [G loss: 20.789] \n",
      "4395 [D loss: (-4.080)(R 11.534, F -19.694)]  [G loss: 20.782] \n",
      "4395 [D loss: (-4.316)(R 11.827, F -20.458)]  [G loss: 20.668] \n",
      "4396 [D loss: (-3.770)(R 11.855, F -19.394)]  [G loss: 20.885] \n",
      "4396 [D loss: (-4.230)(R 12.091, F -20.551)]  [G loss: 21.315] \n",
      "4397 [D loss: (-4.376)(R 12.218, F -20.970)]  [G loss: 21.737] \n",
      "4397 [D loss: (-5.492)(R 11.595, F -22.578)]  [G loss: 21.524] \n",
      "4398 [D loss: (-4.040)(R 12.546, F -20.627)]  [G loss: 22.210] \n",
      "4398 [D loss: (-4.141)(R 13.346, F -21.628)]  [G loss: 22.051] \n",
      "4399 [D loss: (-5.043)(R 12.459, F -22.545)]  [G loss: 22.213] \n",
      "4399 [D loss: (-4.985)(R 12.893, F -22.863)]  [G loss: 22.256] \n",
      "4400 [D loss: (-5.132)(R 12.618, F -22.882)]  [G loss: 22.809] \n",
      "4400 [D loss: (-5.785)(R 12.721, F -24.291)]  [G loss: 22.913] \n",
      "4401 [D loss: (-4.924)(R 13.027, F -22.876)]  [G loss: 22.792] \n",
      "4401 [D loss: (-4.754)(R 12.564, F -22.072)]  [G loss: 22.614] \n",
      "4402 [D loss: (-5.252)(R 13.211, F -23.714)]  [G loss: 23.340] \n",
      "4402 [D loss: (-4.690)(R 13.005, F -22.384)]  [G loss: 23.295] \n",
      "4403 [D loss: (-4.873)(R 14.210, F -23.955)]  [G loss: 23.719] \n",
      "4403 [D loss: (-5.214)(R 13.312, F -23.739)]  [G loss: 24.048] \n",
      "4404 [D loss: (-4.795)(R 14.041, F -23.631)]  [G loss: 24.123] \n",
      "4404 [D loss: (-5.138)(R 13.234, F -23.511)]  [G loss: 23.903] \n",
      "4405 [D loss: (-5.211)(R 13.920, F -24.342)]  [G loss: 24.218] \n",
      "4405 [D loss: (-4.910)(R 14.065, F -23.885)]  [G loss: 24.596] \n",
      "4406 [D loss: (-5.262)(R 14.098, F -24.622)]  [G loss: 25.075] \n",
      "4406 [D loss: (-5.191)(R 13.678, F -24.060)]  [G loss: 24.980] \n",
      "4407 [D loss: (-4.536)(R 14.328, F -23.399)]  [G loss: 25.550] \n",
      "4407 [D loss: (-5.797)(R 14.772, F -26.367)]  [G loss: 25.769] \n",
      "4408 [D loss: (-5.024)(R 14.677, F -24.725)]  [G loss: 25.804] \n",
      "4408 [D loss: (-4.271)(R 15.235, F -23.777)]  [G loss: 25.598] \n",
      "4409 [D loss: (-5.159)(R 14.998, F -25.316)]  [G loss: 26.280] \n",
      "4409 [D loss: (-5.714)(R 14.466, F -25.894)]  [G loss: 25.982] \n",
      "4410 [D loss: (-5.736)(R 14.091, F -25.563)]  [G loss: 26.215] \n",
      "4410 [D loss: (-5.716)(R 14.951, F -26.382)]  [G loss: 26.494] \n",
      "4411 [D loss: (-5.132)(R 14.811, F -25.075)]  [G loss: 25.994] \n",
      "4411 [D loss: (-6.479)(R 14.700, F -27.657)]  [G loss: 26.294] \n",
      "4412 [D loss: (-5.671)(R 15.120, F -26.461)]  [G loss: 26.729] \n",
      "4412 [D loss: (-5.996)(R 15.164, F -27.156)]  [G loss: 26.592] \n",
      "4413 [D loss: (-5.245)(R 15.987, F -26.477)]  [G loss: 27.181] \n",
      "4413 [D loss: (-6.738)(R 15.174, F -28.650)]  [G loss: 27.451] \n",
      "4414 [D loss: (-5.324)(R 16.001, F -26.648)]  [G loss: 27.553] \n",
      "4414 [D loss: (-6.226)(R 16.212, F -28.664)]  [G loss: 27.625] \n",
      "4415 [D loss: (-6.864)(R 15.758, F -29.486)]  [G loss: 28.353] \n",
      "4415 [D loss: (-7.066)(R 15.211, F -29.344)]  [G loss: 28.721] \n",
      "4416 [D loss: (-6.813)(R 15.883, F -29.509)]  [G loss: 28.349] \n",
      "4416 [D loss: (-5.361)(R 16.408, F -27.130)]  [G loss: 28.574] \n",
      "4417 [D loss: (-6.587)(R 16.831, F -30.005)]  [G loss: 28.525] \n",
      "4417 [D loss: (-6.512)(R 16.685, F -29.709)]  [G loss: 29.190] \n",
      "4418 [D loss: (-6.581)(R 16.518, F -29.680)]  [G loss: 29.191] \n",
      "4418 [D loss: (-6.370)(R 18.013, F -30.752)]  [G loss: 29.238] \n",
      "4419 [D loss: (-6.889)(R 17.398, F -31.176)]  [G loss: 29.185] \n",
      "4419 [D loss: (-6.207)(R 17.429, F -29.843)]  [G loss: 30.288] \n",
      "4420 [D loss: (-7.232)(R 16.645, F -31.109)]  [G loss: 30.086] \n",
      "4420 [D loss: (-6.952)(R 17.977, F -31.881)]  [G loss: 29.919] \n",
      "4421 [D loss: (-6.340)(R 17.521, F -30.202)]  [G loss: 29.947] \n",
      "4421 [D loss: (-6.333)(R 17.687, F -30.352)]  [G loss: 30.430] \n",
      "4422 [D loss: (-6.173)(R 17.469, F -29.815)]  [G loss: 30.249] \n",
      "4422 [D loss: (-5.343)(R 17.704, F -28.391)]  [G loss: 31.341] \n",
      "4423 [D loss: (-6.332)(R 18.175, F -30.838)]  [G loss: 30.972] \n",
      "4423 [D loss: (-5.409)(R 18.106, F -28.924)]  [G loss: 30.884] \n",
      "4424 [D loss: (-7.076)(R 17.377, F -31.530)]  [G loss: 31.940] \n",
      "4424 [D loss: (-7.698)(R 16.847, F -32.243)]  [G loss: 31.387] \n",
      "4425 [D loss: (-6.757)(R 18.417, F -31.931)]  [G loss: 31.805] \n",
      "4425 [D loss: (-7.301)(R 18.643, F -33.244)]  [G loss: 30.654] \n",
      "4426 [D loss: (-7.038)(R 18.436, F -32.512)]  [G loss: 31.887] \n",
      "4426 [D loss: (-7.786)(R 18.251, F -33.822)]  [G loss: 31.433] \n",
      "4427 [D loss: (-6.425)(R 18.681, F -31.532)]  [G loss: 33.049] \n",
      "4427 [D loss: (-7.030)(R 17.818, F -31.878)]  [G loss: 32.137] \n",
      "4428 [D loss: (-6.801)(R 17.893, F -31.494)]  [G loss: 33.006] \n",
      "4428 [D loss: (-7.564)(R 19.807, F -34.935)]  [G loss: 32.550] \n",
      "4429 [D loss: (-6.906)(R 19.281, F -33.094)]  [G loss: 32.839] \n",
      "4429 [D loss: (-6.297)(R 19.895, F -32.490)]  [G loss: 33.274] \n",
      "4430 [D loss: (-7.051)(R 19.439, F -33.541)]  [G loss: 33.492] \n",
      "4430 [D loss: (-6.588)(R 19.583, F -32.758)]  [G loss: 34.068] \n",
      "4431 [D loss: (-7.062)(R 19.592, F -33.715)]  [G loss: 33.030] \n",
      "4431 [D loss: (-7.120)(R 19.369, F -33.609)]  [G loss: 35.062] \n",
      "4432 [D loss: (-7.142)(R 19.709, F -33.992)]  [G loss: 34.288] \n",
      "4432 [D loss: (-6.991)(R 20.044, F -34.027)]  [G loss: 34.214] \n",
      "4433 [D loss: (-7.346)(R 19.542, F -34.234)]  [G loss: 34.652] \n",
      "4433 [D loss: (-7.747)(R 20.331, F -35.825)]  [G loss: 34.257] \n",
      "4434 [D loss: (-6.807)(R 20.112, F -33.725)]  [G loss: 35.185] \n",
      "4434 [D loss: (-7.516)(R 21.391, F -36.424)]  [G loss: 34.952] \n",
      "4435 [D loss: (-7.754)(R 20.328, F -35.836)]  [G loss: 36.207] \n",
      "4435 [D loss: (-7.755)(R 20.019, F -35.529)]  [G loss: 35.869] \n",
      "4436 [D loss: (-6.612)(R 20.400, F -33.624)]  [G loss: 36.450] \n",
      "4436 [D loss: (-8.131)(R 21.400, F -37.662)]  [G loss: 35.111] \n",
      "4437 [D loss: (-7.715)(R 21.038, F -36.468)]  [G loss: 36.022] \n",
      "4437 [D loss: (-8.388)(R 20.292, F -37.069)]  [G loss: 35.910] \n",
      "4438 [D loss: (-7.155)(R 21.265, F -35.574)]  [G loss: 36.203] \n",
      "4438 [D loss: (-7.266)(R 20.485, F -35.018)]  [G loss: 36.566] \n",
      "4439 [D loss: (-7.451)(R 21.495, F -36.398)]  [G loss: 36.657] \n",
      "4439 [D loss: (-6.155)(R 23.061, F -35.370)]  [G loss: 37.395] \n",
      "4440 [D loss: (-7.777)(R 21.869, F -37.422)]  [G loss: 37.197] \n",
      "4440 [D loss: (-7.989)(R 21.954, F -37.932)]  [G loss: 37.832] \n",
      "4441 [D loss: (-8.023)(R 22.232, F -38.279)]  [G loss: 38.511] \n",
      "4441 [D loss: (-8.371)(R 21.084, F -37.827)]  [G loss: 36.858] \n",
      "4442 [D loss: (-9.068)(R 21.764, F -39.900)]  [G loss: 37.463] \n",
      "4442 [D loss: (-8.889)(R 22.876, F -40.654)]  [G loss: 38.389] \n",
      "4443 [D loss: (-9.443)(R 21.229, F -40.116)]  [G loss: 38.949] \n",
      "4443 [D loss: (-7.620)(R 22.407, F -37.647)]  [G loss: 38.244] \n",
      "4444 [D loss: (-8.001)(R 22.955, F -38.957)]  [G loss: 39.229] \n",
      "4444 [D loss: (-7.537)(R 22.799, F -37.874)]  [G loss: 38.567] \n",
      "4445 [D loss: (-8.691)(R 22.598, F -39.979)]  [G loss: 38.307] \n",
      "4445 [D loss: (-9.410)(R 22.606, F -41.426)]  [G loss: 40.741] \n",
      "4446 [D loss: (-8.801)(R 23.205, F -40.806)]  [G loss: 39.698] \n",
      "4446 [D loss: (-10.316)(R 23.452, F -44.084)]  [G loss: 38.829] \n",
      "4447 [D loss: (-6.956)(R 22.606, F -36.517)]  [G loss: 39.097] \n",
      "4447 [D loss: (-8.920)(R 23.523, F -41.363)]  [G loss: 40.162] \n",
      "4448 [D loss: (-9.068)(R 23.518, F -41.655)]  [G loss: 40.677] \n",
      "4448 [D loss: (-9.209)(R 22.700, F -41.118)]  [G loss: 40.957] \n",
      "4449 [D loss: (-9.571)(R 23.505, F -42.647)]  [G loss: 41.087] \n",
      "4449 [D loss: (-10.498)(R 24.597, F -45.594)]  [G loss: 41.049] \n",
      "4450 [D loss: (-5.813)(R 24.710, F -36.336)]  [G loss: 40.659] \n",
      "4450 [D loss: (-7.773)(R 25.325, F -40.871)]  [G loss: 41.147] \n",
      "4451 [D loss: (-8.770)(R 24.078, F -41.618)]  [G loss: 42.006] \n",
      "4451 [D loss: (-8.332)(R 24.364, F -41.028)]  [G loss: 40.974] \n",
      "4452 [D loss: (-8.950)(R 24.906, F -42.806)]  [G loss: 41.048] \n",
      "4452 [D loss: (-8.050)(R 23.001, F -39.102)]  [G loss: 41.348] \n",
      "4453 [D loss: (-9.841)(R 23.778, F -43.461)]  [G loss: 42.311] \n",
      "4453 [D loss: (-8.673)(R 25.489, F -42.834)]  [G loss: 42.127] \n",
      "4454 [D loss: (-7.294)(R 26.192, F -40.780)]  [G loss: 42.527] \n",
      "4454 [D loss: (-10.002)(R 24.889, F -44.893)]  [G loss: 42.103] \n",
      "4455 [D loss: (-8.652)(R 25.695, F -42.998)]  [G loss: 42.153] \n",
      "4455 [D loss: (-9.800)(R 25.893, F -45.493)]  [G loss: 42.956] \n",
      "4456 [D loss: (-10.018)(R 26.795, F -46.831)]  [G loss: 43.291] \n",
      "4456 [D loss: (-8.403)(R 25.748, F -42.553)]  [G loss: 44.243] \n",
      "4457 [D loss: (-7.415)(R 26.644, F -41.474)]  [G loss: 44.374] \n",
      "4457 [D loss: (-8.995)(R 25.134, F -43.124)]  [G loss: 44.852] \n",
      "4458 [D loss: (-7.704)(R 26.666, F -42.075)]  [G loss: 44.159] \n",
      "4458 [D loss: (-8.721)(R 25.353, F -42.795)]  [G loss: 45.314] \n",
      "4459 [D loss: (-9.409)(R 26.461, F -45.278)]  [G loss: 45.069] \n",
      "4459 [D loss: (-10.338)(R 26.595, F -47.271)]  [G loss: 44.931] \n",
      "4460 [D loss: (-9.395)(R 26.441, F -45.232)]  [G loss: 45.199] \n",
      "4460 [D loss: (-10.090)(R 27.872, F -48.052)]  [G loss: 44.902] \n",
      "4461 [D loss: (-9.392)(R 25.817, F -44.601)]  [G loss: 44.742] \n",
      "4461 [D loss: (-11.916)(R 26.797, F -50.629)]  [G loss: 46.287] \n",
      "4462 [D loss: (-9.937)(R 28.818, F -48.692)]  [G loss: 45.960] \n",
      "4462 [D loss: (-11.162)(R 27.388, F -49.713)]  [G loss: 46.684] \n",
      "4463 [D loss: (-8.815)(R 27.516, F -45.146)]  [G loss: 45.723] \n",
      "4463 [D loss: (-8.770)(R 28.109, F -45.649)]  [G loss: 45.861] \n",
      "4464 [D loss: (-10.261)(R 28.246, F -48.768)]  [G loss: 44.501] \n",
      "4464 [D loss: (-9.074)(R 27.267, F -45.416)]  [G loss: 47.053] \n",
      "4465 [D loss: (-9.542)(R 26.615, F -45.700)]  [G loss: 47.223] \n",
      "4465 [D loss: (-8.592)(R 30.149, F -47.333)]  [G loss: 46.993] \n",
      "4466 [D loss: (-10.445)(R 28.372, F -49.262)]  [G loss: 47.469] \n",
      "4466 [D loss: (-10.812)(R 28.115, F -49.740)]  [G loss: 48.523] \n",
      "4467 [D loss: (-9.933)(R 27.651, F -47.517)]  [G loss: 47.046] \n",
      "4467 [D loss: (-8.808)(R 28.310, F -45.925)]  [G loss: 48.386] \n",
      "4468 [D loss: (-11.029)(R 29.524, F -51.582)]  [G loss: 47.738] \n",
      "4468 [D loss: (-9.589)(R 27.783, F -46.962)]  [G loss: 48.458] \n",
      "4469 [D loss: (-10.260)(R 27.648, F -48.169)]  [G loss: 48.646] \n",
      "4469 [D loss: (-12.536)(R 28.292, F -53.365)]  [G loss: 50.114] \n",
      "4470 [D loss: (-10.398)(R 28.085, F -48.880)]  [G loss: 48.830] \n",
      "4470 [D loss: (-12.152)(R 28.771, F -53.075)]  [G loss: 49.357] \n",
      "4471 [D loss: (-9.909)(R 29.775, F -49.593)]  [G loss: 49.820] \n",
      "4471 [D loss: (-9.140)(R 29.597, F -47.878)]  [G loss: 50.291] \n",
      "4472 [D loss: (-9.963)(R 30.399, F -50.325)]  [G loss: 48.445] \n",
      "4472 [D loss: (-9.079)(R 30.418, F -48.576)]  [G loss: 50.151] \n",
      "4473 [D loss: (-11.325)(R 28.872, F -51.522)]  [G loss: 50.496] \n",
      "4473 [D loss: (-8.587)(R 30.348, F -47.522)]  [G loss: 51.410] \n",
      "4474 [D loss: (-7.327)(R 30.375, F -45.028)]  [G loss: 50.989] \n",
      "4474 [D loss: (-9.853)(R 31.094, F -50.800)]  [G loss: 49.660] \n",
      "4475 [D loss: (-9.459)(R 30.051, F -48.969)]  [G loss: 49.678] \n",
      "4475 [D loss: (-10.818)(R 30.284, F -51.921)]  [G loss: 50.637] \n",
      "4476 [D loss: (-12.367)(R 29.654, F -54.388)]  [G loss: 50.968] \n",
      "4476 [D loss: (-10.764)(R 30.199, F -51.727)]  [G loss: 51.272] \n",
      "4477 [D loss: (-10.550)(R 29.302, F -50.402)]  [G loss: 51.335] \n",
      "4477 [D loss: (-9.232)(R 32.591, F -51.054)]  [G loss: 51.311] \n",
      "4478 [D loss: (-11.091)(R 31.011, F -53.194)]  [G loss: 51.409] \n",
      "4478 [D loss: (-9.531)(R 31.924, F -50.986)]  [G loss: 53.005] \n",
      "4479 [D loss: (-11.748)(R 30.687, F -54.183)]  [G loss: 52.214] \n",
      "4479 [D loss: (-10.596)(R 31.073, F -52.265)]  [G loss: 52.826] \n",
      "4480 [D loss: (-10.311)(R 32.120, F -52.742)]  [G loss: 52.280] \n",
      "4480 [D loss: (-11.875)(R 32.088, F -55.837)]  [G loss: 51.393] \n",
      "4481 [D loss: (-10.041)(R 32.605, F -52.686)]  [G loss: 52.537] \n",
      "4481 [D loss: (-11.529)(R 32.219, F -55.276)]  [G loss: 54.575] \n",
      "4482 [D loss: (-10.741)(R 33.109, F -54.591)]  [G loss: 52.298] \n",
      "4482 [D loss: (-10.887)(R 32.772, F -54.546)]  [G loss: 52.971] \n",
      "4483 [D loss: (-13.099)(R 32.287, F -58.484)]  [G loss: 52.770] \n",
      "4483 [D loss: (-11.716)(R 32.736, F -56.167)]  [G loss: 53.697] \n",
      "4484 [D loss: (-10.520)(R 33.616, F -54.656)]  [G loss: 53.391] \n",
      "4484 [D loss: (-13.273)(R 31.045, F -57.590)]  [G loss: 55.939] \n",
      "4485 [D loss: (-11.969)(R 33.115, F -57.054)]  [G loss: 53.350] \n",
      "4485 [D loss: (-9.980)(R 34.609, F -54.569)]  [G loss: 55.114] \n",
      "4486 [D loss: (-11.115)(R 34.188, F -56.419)]  [G loss: 53.713] \n",
      "4486 [D loss: (-9.370)(R 32.645, F -51.385)]  [G loss: 55.263] \n",
      "4487 [D loss: (-11.673)(R 33.195, F -56.541)]  [G loss: 53.669] \n",
      "4487 [D loss: (-13.584)(R 31.735, F -58.903)]  [G loss: 54.899] \n",
      "4488 [D loss: (-11.928)(R 34.974, F -58.830)]  [G loss: 54.801] \n",
      "4488 [D loss: (-14.506)(R 33.076, F -62.087)]  [G loss: 56.657] \n",
      "4489 [D loss: (-8.865)(R 36.039, F -53.768)]  [G loss: 55.172] \n",
      "4489 [D loss: (-10.859)(R 34.806, F -56.524)]  [G loss: 56.049] \n",
      "4490 [D loss: (-11.891)(R 31.780, F -55.562)]  [G loss: 56.368] \n",
      "4490 [D loss: (-8.970)(R 35.478, F -53.418)]  [G loss: 57.479] \n",
      "4491 [D loss: (-11.165)(R 37.194, F -59.524)]  [G loss: 57.025] \n",
      "4491 [D loss: (-12.209)(R 34.134, F -58.552)]  [G loss: 55.440] \n",
      "4492 [D loss: (-12.735)(R 37.307, F -62.777)]  [G loss: 58.020] \n",
      "4492 [D loss: (-10.466)(R 34.395, F -55.326)]  [G loss: 57.200] \n",
      "4493 [D loss: (-12.456)(R 34.281, F -59.193)]  [G loss: 57.235] \n",
      "4493 [D loss: (-12.922)(R 34.348, F -60.192)]  [G loss: 57.221] \n",
      "4494 [D loss: (-10.657)(R 36.082, F -57.397)]  [G loss: 58.310] \n",
      "4494 [D loss: (-12.550)(R 36.296, F -61.395)]  [G loss: 60.309] \n",
      "4495 [D loss: (-12.812)(R 35.571, F -61.195)]  [G loss: 58.470] \n",
      "4495 [D loss: (-13.439)(R 37.733, F -64.611)]  [G loss: 58.754] \n",
      "4496 [D loss: (-13.117)(R 36.760, F -62.995)]  [G loss: 59.122] \n",
      "4496 [D loss: (-10.196)(R 36.633, F -57.025)]  [G loss: 59.734] \n",
      "4497 [D loss: (-12.706)(R 38.014, F -63.427)]  [G loss: 58.571] \n",
      "4497 [D loss: (-11.721)(R 34.326, F -57.768)]  [G loss: 59.804] \n",
      "4498 [D loss: (-10.123)(R 36.351, F -56.597)]  [G loss: 59.905] \n",
      "4498 [D loss: (-14.052)(R 36.060, F -64.164)]  [G loss: 59.569] \n",
      "4499 [D loss: (-13.134)(R 36.639, F -62.906)]  [G loss: 60.046] \n",
      "4499 [D loss: (-12.704)(R 37.502, F -62.910)]  [G loss: 59.430] \n",
      "4500 [D loss: (-11.680)(R 36.969, F -60.329)]  [G loss: 59.449] \n",
      "4500 [D loss: (-10.451)(R 38.983, F -59.885)]  [G loss: 59.682] \n",
      "4501 [D loss: (-12.630)(R 35.825, F -61.085)]  [G loss: 59.416] \n",
      "4501 [D loss: (-8.497)(R 37.419, F -54.413)]  [G loss: 62.438] \n",
      "4502 [D loss: (-11.481)(R 38.070, F -61.033)]  [G loss: 63.388] \n",
      "4502 [D loss: (-11.845)(R 38.497, F -62.187)]  [G loss: 61.394] \n",
      "4503 [D loss: (-13.900)(R 37.992, F -65.793)]  [G loss: 60.850] \n",
      "4503 [D loss: (-13.346)(R 37.729, F -64.422)]  [G loss: 61.996] \n",
      "4504 [D loss: (-15.074)(R 37.494, F -67.641)]  [G loss: 60.886] \n",
      "4504 [D loss: (-10.738)(R 38.540, F -60.017)]  [G loss: 61.386] \n",
      "4505 [D loss: (-8.823)(R 40.272, F -57.918)]  [G loss: 60.141] \n",
      "4505 [D loss: (-11.656)(R 40.464, F -63.776)]  [G loss: 62.239] \n",
      "4506 [D loss: (-12.235)(R 38.753, F -63.223)]  [G loss: 62.444] \n",
      "4506 [D loss: (-9.819)(R 37.936, F -57.574)]  [G loss: 62.039] \n",
      "4507 [D loss: (-9.810)(R 39.865, F -59.485)]  [G loss: 62.812] \n",
      "4507 [D loss: (-9.877)(R 39.012, F -58.766)]  [G loss: 63.425] \n",
      "4508 [D loss: (-13.087)(R 38.600, F -64.774)]  [G loss: 63.074] \n",
      "4508 [D loss: (-12.269)(R 39.929, F -64.466)]  [G loss: 62.494] \n",
      "4509 [D loss: (-10.317)(R 40.005, F -60.639)]  [G loss: 63.023] \n",
      "4509 [D loss: (-12.984)(R 39.394, F -65.362)]  [G loss: 62.863] \n",
      "4510 [D loss: (-10.970)(R 40.418, F -62.358)]  [G loss: 62.775] \n",
      "4510 [D loss: (-14.766)(R 38.124, F -67.657)]  [G loss: 64.574] \n",
      "4511 [D loss: (-12.685)(R 39.052, F -64.421)]  [G loss: 63.438] \n",
      "4511 [D loss: (-10.805)(R 39.314, F -60.925)]  [G loss: 61.351] \n",
      "4512 [D loss: (-11.380)(R 40.798, F -63.558)]  [G loss: 63.258] \n",
      "4512 [D loss: (-11.877)(R 39.400, F -63.153)]  [G loss: 63.698] \n",
      "4513 [D loss: (-13.915)(R 39.713, F -67.542)]  [G loss: 63.693] \n",
      "4513 [D loss: (-11.739)(R 39.684, F -63.161)]  [G loss: 63.414] \n",
      "4514 [D loss: (-12.119)(R 41.429, F -65.667)]  [G loss: 64.070] \n",
      "4514 [D loss: (-10.390)(R 42.139, F -62.920)]  [G loss: 65.100] \n",
      "4515 [D loss: (-13.515)(R 38.695, F -65.726)]  [G loss: 63.816] \n",
      "4515 [D loss: (-9.388)(R 40.784, F -59.561)]  [G loss: 65.257] \n",
      "4516 [D loss: (-12.027)(R 40.941, F -64.996)]  [G loss: 65.869] \n",
      "4516 [D loss: (-14.508)(R 40.421, F -69.438)]  [G loss: 67.029] \n",
      "4517 [D loss: (-12.964)(R 40.313, F -66.242)]  [G loss: 64.728] \n",
      "4517 [D loss: (-11.757)(R 39.669, F -63.183)]  [G loss: 65.722] \n",
      "4518 [D loss: (-11.612)(R 39.868, F -63.092)]  [G loss: 65.999] \n",
      "4518 [D loss: (-11.093)(R 41.785, F -63.971)]  [G loss: 66.200] \n",
      "4519 [D loss: (-10.259)(R 40.745, F -61.262)]  [G loss: 65.305] \n",
      "4519 [D loss: (-10.200)(R 41.066, F -61.465)]  [G loss: 67.401] \n",
      "4520 [D loss: (-14.907)(R 40.933, F -70.746)]  [G loss: 66.310] \n",
      "4520 [D loss: (-11.721)(R 42.211, F -65.652)]  [G loss: 67.562] \n",
      "4521 [D loss: (-14.996)(R 43.275, F -73.267)]  [G loss: 66.990] \n",
      "4521 [D loss: (-12.403)(R 41.738, F -66.544)]  [G loss: 66.888] \n",
      "4522 [D loss: (-11.638)(R 43.121, F -66.398)]  [G loss: 65.468] \n",
      "4522 [D loss: (-11.076)(R 42.799, F -64.951)]  [G loss: 67.177] \n",
      "4523 [D loss: (-10.589)(R 41.902, F -63.080)]  [G loss: 67.787] \n",
      "4523 [D loss: (-11.768)(R 41.135, F -64.672)]  [G loss: 66.966] \n",
      "4524 [D loss: (-13.551)(R 41.017, F -68.118)]  [G loss: 66.786] \n",
      "4524 [D loss: (-8.007)(R 43.629, F -59.642)]  [G loss: 67.693] \n",
      "4525 [D loss: (-11.681)(R 43.608, F -66.969)]  [G loss: 68.140] \n",
      "4525 [D loss: (-11.394)(R 42.314, F -65.102)]  [G loss: 67.323] \n",
      "4526 [D loss: (-11.794)(R 43.732, F -67.320)]  [G loss: 70.098] \n",
      "4526 [D loss: (-10.715)(R 43.361, F -64.792)]  [G loss: 69.309] \n",
      "4527 [D loss: (-10.333)(R 47.004, F -67.669)]  [G loss: 67.662] \n",
      "4527 [D loss: (-11.915)(R 43.308, F -67.137)]  [G loss: 68.461] \n",
      "4528 [D loss: (-13.929)(R 42.592, F -70.450)]  [G loss: 68.672] \n",
      "4528 [D loss: (-11.606)(R 43.016, F -66.228)]  [G loss: 69.293] \n",
      "4529 [D loss: (-12.704)(R 42.651, F -68.059)]  [G loss: 70.684] \n",
      "4529 [D loss: (-14.516)(R 42.627, F -71.659)]  [G loss: 70.514] \n",
      "4530 [D loss: (-14.689)(R 41.818, F -71.195)]  [G loss: 68.576] \n",
      "4530 [D loss: (-11.763)(R 45.110, F -68.636)]  [G loss: 69.178] \n",
      "4531 [D loss: (-15.854)(R 43.746, F -75.453)]  [G loss: 69.300] \n",
      "4531 [D loss: (-13.543)(R 43.361, F -70.447)]  [G loss: 69.930] \n",
      "4532 [D loss: (-14.792)(R 42.575, F -72.160)]  [G loss: 70.010] \n",
      "4532 [D loss: (-10.287)(R 44.437, F -65.010)]  [G loss: 70.163] \n",
      "4533 [D loss: (-12.605)(R 45.381, F -70.591)]  [G loss: 71.932] \n",
      "4533 [D loss: (-10.213)(R 44.694, F -65.120)]  [G loss: 72.027] \n",
      "4534 [D loss: (-10.008)(R 48.315, F -68.332)]  [G loss: 71.101] \n",
      "4534 [D loss: (-15.576)(R 44.466, F -75.617)]  [G loss: 71.390] \n",
      "4535 [D loss: (-10.747)(R 45.758, F -67.251)]  [G loss: 68.904] \n",
      "4535 [D loss: (-12.436)(R 46.570, F -71.443)]  [G loss: 70.792] \n",
      "4536 [D loss: (-12.667)(R 46.185, F -71.518)]  [G loss: 69.915] \n",
      "4536 [D loss: (-10.712)(R 43.151, F -64.575)]  [G loss: 71.727] \n",
      "4537 [D loss: (-13.136)(R 44.754, F -71.026)]  [G loss: 71.666] \n",
      "4537 [D loss: (-10.075)(R 44.204, F -64.355)]  [G loss: 70.163] \n",
      "4538 [D loss: (-15.444)(R 44.504, F -75.392)]  [G loss: 70.953] \n",
      "4538 [D loss: (-15.096)(R 45.583, F -75.775)]  [G loss: 70.752] \n",
      "4539 [D loss: (-12.929)(R 45.960, F -71.817)]  [G loss: 72.393] \n",
      "4539 [D loss: (-12.933)(R 45.243, F -71.108)]  [G loss: 71.843] \n",
      "4540 [D loss: (-14.277)(R 44.852, F -73.405)]  [G loss: 71.369] \n",
      "4540 [D loss: (-13.684)(R 46.135, F -73.502)]  [G loss: 72.479] \n",
      "4541 [D loss: (-13.324)(R 45.303, F -71.951)]  [G loss: 72.656] \n",
      "4541 [D loss: (-12.178)(R 46.201, F -70.558)]  [G loss: 72.739] \n",
      "4542 [D loss: (-12.947)(R 50.284, F -76.178)]  [G loss: 74.538] \n",
      "4542 [D loss: (-14.402)(R 46.560, F -75.364)]  [G loss: 73.513] \n",
      "4543 [D loss: (-15.599)(R 46.125, F -77.322)]  [G loss: 71.465] \n",
      "4543 [D loss: (-13.963)(R 48.059, F -75.984)]  [G loss: 72.488] \n",
      "4544 [D loss: (-12.369)(R 46.937, F -71.675)]  [G loss: 74.488] \n",
      "4544 [D loss: (-17.601)(R 45.040, F -80.243)]  [G loss: 73.107] \n",
      "4545 [D loss: (-10.231)(R 47.167, F -67.629)]  [G loss: 73.494] \n",
      "4545 [D loss: (-12.939)(R 48.191, F -74.070)]  [G loss: 73.230] \n",
      "4546 [D loss: (-13.723)(R 44.474, F -71.921)]  [G loss: 74.173] \n",
      "4546 [D loss: (-11.493)(R 48.680, F -71.665)]  [G loss: 72.756] \n",
      "4547 [D loss: (-13.827)(R 44.084, F -71.738)]  [G loss: 73.771] \n",
      "4547 [D loss: (-15.966)(R 45.575, F -77.508)]  [G loss: 73.626] \n",
      "4548 [D loss: (-12.896)(R 46.538, F -72.331)]  [G loss: 73.846] \n",
      "4548 [D loss: (-15.132)(R 46.209, F -76.474)]  [G loss: 72.029] \n",
      "4549 [D loss: (-15.519)(R 46.013, F -77.050)]  [G loss: 74.186] \n",
      "4549 [D loss: (-16.172)(R 47.817, F -80.160)]  [G loss: 75.043] \n",
      "4550 [D loss: (-13.729)(R 46.152, F -73.610)]  [G loss: 74.209] \n",
      "4550 [D loss: (-14.215)(R 48.856, F -77.286)]  [G loss: 74.673] \n",
      "4551 [D loss: (-12.942)(R 46.509, F -72.394)]  [G loss: 75.016] \n",
      "4551 [D loss: (-10.905)(R 49.540, F -71.349)]  [G loss: 74.223] \n",
      "4552 [D loss: (-15.723)(R 47.722, F -79.168)]  [G loss: 76.492] \n",
      "4552 [D loss: (-16.734)(R 45.683, F -79.150)]  [G loss: 74.223] \n",
      "4553 [D loss: (-12.449)(R 47.386, F -72.284)]  [G loss: 74.802] \n",
      "4553 [D loss: (-14.669)(R 46.197, F -75.535)]  [G loss: 76.500] \n",
      "4554 [D loss: (-16.087)(R 48.505, F -80.680)]  [G loss: 76.867] \n",
      "4554 [D loss: (-12.637)(R 48.907, F -74.182)]  [G loss: 73.581] \n",
      "4555 [D loss: (-13.562)(R 47.835, F -74.959)]  [G loss: 75.777] \n",
      "4555 [D loss: (-15.769)(R 47.744, F -79.282)]  [G loss: 76.386] \n",
      "4556 [D loss: (-16.002)(R 48.932, F -80.936)]  [G loss: 77.281] \n",
      "4556 [D loss: (-13.391)(R 48.552, F -75.333)]  [G loss: 76.979] \n",
      "4557 [D loss: (-14.771)(R 47.544, F -77.086)]  [G loss: 76.963] \n",
      "4557 [D loss: (-12.827)(R 48.694, F -74.349)]  [G loss: 77.603] \n",
      "4558 [D loss: (-11.618)(R 49.748, F -72.985)]  [G loss: 75.737] \n",
      "4558 [D loss: (-12.957)(R 50.125, F -76.038)]  [G loss: 75.489] \n",
      "4559 [D loss: (-17.344)(R 46.087, F -80.775)]  [G loss: 75.321] \n",
      "4559 [D loss: (-14.517)(R 49.082, F -78.116)]  [G loss: 76.183] \n",
      "4560 [D loss: (-14.304)(R 47.398, F -76.005)]  [G loss: 77.193] \n",
      "4560 [D loss: (-12.527)(R 50.954, F -76.009)]  [G loss: 77.479] \n",
      "4561 [D loss: (-12.168)(R 51.206, F -75.542)]  [G loss: 75.773] \n",
      "4561 [D loss: (-12.578)(R 51.307, F -76.464)]  [G loss: 76.661] \n",
      "4562 [D loss: (-13.501)(R 48.659, F -75.662)]  [G loss: 78.111] \n",
      "4562 [D loss: (-13.074)(R 49.828, F -75.976)]  [G loss: 77.425] \n",
      "4563 [D loss: (-12.618)(R 49.509, F -74.745)]  [G loss: 76.623] \n",
      "4563 [D loss: (-13.494)(R 49.079, F -76.068)]  [G loss: 76.628] \n",
      "4564 [D loss: (-13.764)(R 49.586, F -77.114)]  [G loss: 76.082] \n",
      "4564 [D loss: (-17.180)(R 50.461, F -84.820)]  [G loss: 79.102] \n",
      "4565 [D loss: (-13.843)(R 50.463, F -78.150)]  [G loss: 77.563] \n",
      "4565 [D loss: (-13.585)(R 51.289, F -78.459)]  [G loss: 77.344] \n",
      "4566 [D loss: (-10.055)(R 49.408, F -69.517)]  [G loss: 78.714] \n",
      "4566 [D loss: (-11.723)(R 51.548, F -74.994)]  [G loss: 79.356] \n",
      "4567 [D loss: (-14.070)(R 52.014, F -80.154)]  [G loss: 79.010] \n",
      "4567 [D loss: (-13.410)(R 48.941, F -75.761)]  [G loss: 77.399] \n",
      "4568 [D loss: (-12.375)(R 50.940, F -75.690)]  [G loss: 78.392] \n",
      "4568 [D loss: (-13.622)(R 50.936, F -78.180)]  [G loss: 77.868] \n",
      "4569 [D loss: (-14.570)(R 50.135, F -79.274)]  [G loss: 79.500] \n",
      "4569 [D loss: (-14.897)(R 51.490, F -81.284)]  [G loss: 79.345] \n",
      "4570 [D loss: (-13.748)(R 52.389, F -79.884)]  [G loss: 79.125] \n",
      "4570 [D loss: (-12.888)(R 52.521, F -78.298)]  [G loss: 78.166] \n",
      "4571 [D loss: (-13.233)(R 50.567, F -77.033)]  [G loss: 80.009] \n",
      "4571 [D loss: (-14.710)(R 52.065, F -81.485)]  [G loss: 79.067] \n",
      "4572 [D loss: (-14.377)(R 51.094, F -79.848)]  [G loss: 80.287] \n",
      "4572 [D loss: (-16.742)(R 50.712, F -84.196)]  [G loss: 80.856] \n",
      "4573 [D loss: (-12.144)(R 50.763, F -75.050)]  [G loss: 78.448] \n",
      "4573 [D loss: (-12.023)(R 52.457, F -76.503)]  [G loss: 80.069] \n",
      "4574 [D loss: (-16.814)(R 50.539, F -84.166)]  [G loss: 78.802] \n",
      "4574 [D loss: (-11.932)(R 51.705, F -75.570)]  [G loss: 78.793] \n",
      "4575 [D loss: (-12.979)(R 54.369, F -80.327)]  [G loss: 77.773] \n",
      "4575 [D loss: (-10.531)(R 51.491, F -72.553)]  [G loss: 77.287] \n",
      "4576 [D loss: (-15.312)(R 51.249, F -81.874)]  [G loss: 80.045] \n",
      "4576 [D loss: (-14.377)(R 52.462, F -81.217)]  [G loss: 78.901] \n",
      "4577 [D loss: (-16.402)(R 49.771, F -82.576)]  [G loss: 81.310] \n",
      "4577 [D loss: (-14.392)(R 51.802, F -80.587)]  [G loss: 80.345] \n",
      "4578 [D loss: (-15.090)(R 50.242, F -80.422)]  [G loss: 78.371] \n",
      "4578 [D loss: (-13.440)(R 52.106, F -78.986)]  [G loss: 79.703] \n",
      "4579 [D loss: (-11.687)(R 53.466, F -76.840)]  [G loss: 79.927] \n",
      "4579 [D loss: (-17.326)(R 54.673, F -89.325)]  [G loss: 81.392] \n",
      "4580 [D loss: (-15.871)(R 54.027, F -85.769)]  [G loss: 82.334] \n",
      "4580 [D loss: (-11.924)(R 53.441, F -77.289)]  [G loss: 80.449] \n",
      "4581 [D loss: (-13.739)(R 52.377, F -79.856)]  [G loss: 81.865] \n",
      "4581 [D loss: (-12.858)(R 51.627, F -77.344)]  [G loss: 80.030] \n",
      "4582 [D loss: (-13.801)(R 54.344, F -81.945)]  [G loss: 80.256] \n",
      "4582 [D loss: (-15.075)(R 51.864, F -82.015)]  [G loss: 80.623] \n",
      "4583 [D loss: (-11.959)(R 53.413, F -77.332)]  [G loss: 81.318] \n",
      "4583 [D loss: (-11.912)(R 55.394, F -79.219)]  [G loss: 79.635] \n",
      "4584 [D loss: (-14.251)(R 54.762, F -83.264)]  [G loss: 82.433] \n",
      "4584 [D loss: (-9.023)(R 55.802, F -73.847)]  [G loss: 80.146] \n",
      "4585 [D loss: (-9.229)(R 56.295, F -74.753)]  [G loss: 80.412] \n",
      "4585 [D loss: (-14.092)(R 52.559, F -80.743)]  [G loss: 79.529] \n",
      "4586 [D loss: (-12.316)(R 54.325, F -78.957)]  [G loss: 81.589] \n",
      "4586 [D loss: (-16.954)(R 51.925, F -85.832)]  [G loss: 79.701] \n",
      "4587 [D loss: (-10.924)(R 55.460, F -77.308)]  [G loss: 80.758] \n",
      "4587 [D loss: (-12.773)(R 54.556, F -80.101)]  [G loss: 81.614] \n",
      "4588 [D loss: (-11.176)(R 54.853, F -77.204)]  [G loss: 82.328] \n",
      "4588 [D loss: (-10.592)(R 53.944, F -75.128)]  [G loss: 80.270] \n",
      "4589 [D loss: (-15.493)(R 53.857, F -84.842)]  [G loss: 83.549] \n",
      "4589 [D loss: (-16.718)(R 54.157, F -87.592)]  [G loss: 81.728] \n",
      "4590 [D loss: (-14.965)(R 52.063, F -81.994)]  [G loss: 82.290] \n",
      "4590 [D loss: (-16.459)(R 53.988, F -86.906)]  [G loss: 82.183] \n",
      "4591 [D loss: (-15.834)(R 53.783, F -85.452)]  [G loss: 84.045] \n",
      "4591 [D loss: (-9.758)(R 54.415, F -73.931)]  [G loss: 82.807] \n",
      "4592 [D loss: (-11.852)(R 52.636, F -76.341)]  [G loss: 82.187] \n",
      "4592 [D loss: (-13.949)(R 55.530, F -83.428)]  [G loss: 81.178] \n",
      "4593 [D loss: (-15.464)(R 53.614, F -84.543)]  [G loss: 81.159] \n",
      "4593 [D loss: (-15.322)(R 53.357, F -84.002)]  [G loss: 82.641] \n",
      "4594 [D loss: (-16.337)(R 54.977, F -87.652)]  [G loss: 81.699] \n",
      "4594 [D loss: (-12.355)(R 53.699, F -78.409)]  [G loss: 82.789] \n",
      "4595 [D loss: (-13.019)(R 56.133, F -82.171)]  [G loss: 83.240] \n",
      "4595 [D loss: (-11.717)(R 54.423, F -77.857)]  [G loss: 82.624] \n",
      "4596 [D loss: (-15.341)(R 54.543, F -85.226)]  [G loss: 82.541] \n",
      "4596 [D loss: (-14.232)(R 56.222, F -84.685)]  [G loss: 82.479] \n",
      "4597 [D loss: (-15.952)(R 57.122, F -89.027)]  [G loss: 84.325] \n",
      "4597 [D loss: (-15.670)(R 57.345, F -88.685)]  [G loss: 84.160] \n",
      "4598 [D loss: (-13.528)(R 56.511, F -83.567)]  [G loss: 82.560] \n",
      "4598 [D loss: (-11.601)(R 56.454, F -79.656)]  [G loss: 85.210] \n",
      "4599 [D loss: (-12.828)(R 53.429, F -79.084)]  [G loss: 83.722] \n",
      "4599 [D loss: (-10.667)(R 58.717, F -80.050)]  [G loss: 86.671] \n",
      "4600 [D loss: (-15.834)(R 55.655, F -87.323)]  [G loss: 83.430] \n",
      "4600 [D loss: (-13.252)(R 55.498, F -82.002)]  [G loss: 84.354] \n",
      "4601 [D loss: (-15.143)(R 53.966, F -84.252)]  [G loss: 83.957] \n",
      "4601 [D loss: (-11.499)(R 55.587, F -78.585)]  [G loss: 82.092] \n",
      "4602 [D loss: (-13.469)(R 55.265, F -82.202)]  [G loss: 82.865] \n",
      "4602 [D loss: (-9.376)(R 57.846, F -76.599)]  [G loss: 84.245] \n",
      "4603 [D loss: (-14.405)(R 56.700, F -85.509)]  [G loss: 84.145] \n",
      "4603 [D loss: (-10.501)(R 56.764, F -77.767)]  [G loss: 83.737] \n",
      "4604 [D loss: (-10.555)(R 55.800, F -76.911)]  [G loss: 84.382] \n",
      "4604 [D loss: (-14.458)(R 56.272, F -85.187)]  [G loss: 81.879] \n",
      "4605 [D loss: (-11.258)(R 55.713, F -78.230)]  [G loss: 82.870] \n",
      "4605 [D loss: (-16.525)(R 54.013, F -87.062)]  [G loss: 82.925] \n",
      "4606 [D loss: (-11.132)(R 58.211, F -80.475)]  [G loss: 85.642] \n",
      "4606 [D loss: (-13.880)(R 57.330, F -85.090)]  [G loss: 84.169] \n",
      "4607 [D loss: (-10.807)(R 55.934, F -77.549)]  [G loss: 84.483] \n",
      "4607 [D loss: (-15.995)(R 55.647, F -87.637)]  [G loss: 85.546] \n",
      "4608 [D loss: (-13.642)(R 56.346, F -83.631)]  [G loss: 85.782] \n",
      "4608 [D loss: (-15.040)(R 54.254, F -84.334)]  [G loss: 83.065] \n",
      "4609 [D loss: (-14.994)(R 57.600, F -87.588)]  [G loss: 82.509] \n",
      "4609 [D loss: (-13.223)(R 56.206, F -82.651)]  [G loss: 84.015] \n",
      "4610 [D loss: (-13.884)(R 57.174, F -84.943)]  [G loss: 82.999] \n",
      "4610 [D loss: (-17.003)(R 56.730, F -90.736)]  [G loss: 82.835] \n",
      "4611 [D loss: (-14.462)(R 55.907, F -84.831)]  [G loss: 86.026] \n",
      "4611 [D loss: (-13.825)(R 56.845, F -84.495)]  [G loss: 85.330] \n",
      "4612 [D loss: (-16.677)(R 57.901, F -91.256)]  [G loss: 83.688] \n",
      "4612 [D loss: (-12.517)(R 58.299, F -83.334)]  [G loss: 84.800] \n",
      "4613 [D loss: (-15.230)(R 57.383, F -87.843)]  [G loss: 84.776] \n",
      "4613 [D loss: (-16.301)(R 53.412, F -86.014)]  [G loss: 85.677] \n",
      "4614 [D loss: (-12.950)(R 60.895, F -86.796)]  [G loss: 84.602] \n",
      "4614 [D loss: (-16.811)(R 57.755, F -91.377)]  [G loss: 83.274] \n",
      "4615 [D loss: (-12.689)(R 58.563, F -83.942)]  [G loss: 84.650] \n",
      "4615 [D loss: (-14.393)(R 55.460, F -84.246)]  [G loss: 86.295] \n",
      "4616 [D loss: (-14.580)(R 57.510, F -86.669)]  [G loss: 84.956] \n",
      "4616 [D loss: (-12.330)(R 56.900, F -81.559)]  [G loss: 85.647] \n",
      "4617 [D loss: (-11.807)(R 56.623, F -80.238)]  [G loss: 85.746] \n",
      "4617 [D loss: (-11.646)(R 59.204, F -82.496)]  [G loss: 86.353] \n",
      "4618 [D loss: (-12.770)(R 57.285, F -82.826)]  [G loss: 84.596] \n",
      "4618 [D loss: (-10.970)(R 58.396, F -80.335)]  [G loss: 85.376] \n",
      "4619 [D loss: (-10.579)(R 58.678, F -79.835)]  [G loss: 86.747] \n",
      "4619 [D loss: (-12.151)(R 55.762, F -80.063)]  [G loss: 85.447] \n",
      "4620 [D loss: (-14.261)(R 54.774, F -83.296)]  [G loss: 85.311] \n",
      "4620 [D loss: (-12.215)(R 55.460, F -79.890)]  [G loss: 87.052] \n",
      "4621 [D loss: (-12.801)(R 55.782, F -81.385)]  [G loss: 86.260] \n",
      "4621 [D loss: (-10.950)(R 56.405, F -78.304)]  [G loss: 84.589] \n",
      "4622 [D loss: (-11.106)(R 57.543, F -79.755)]  [G loss: 88.329] \n",
      "4622 [D loss: (-12.397)(R 58.779, F -83.573)]  [G loss: 86.079] \n",
      "4623 [D loss: (-15.349)(R 60.640, F -91.338)]  [G loss: 86.341] \n",
      "4623 [D loss: (-12.221)(R 61.135, F -85.576)]  [G loss: 86.430] \n",
      "4624 [D loss: (-11.214)(R 55.862, F -78.290)]  [G loss: 85.075] \n",
      "4624 [D loss: (-16.189)(R 55.848, F -88.225)]  [G loss: 86.996] \n",
      "4625 [D loss: (-13.606)(R 58.288, F -85.500)]  [G loss: 86.878] \n",
      "4625 [D loss: (-14.215)(R 58.202, F -86.632)]  [G loss: 86.393] \n",
      "4626 [D loss: (-15.317)(R 57.659, F -88.292)]  [G loss: 86.596] \n",
      "4626 [D loss: (-10.034)(R 58.917, F -78.985)]  [G loss: 86.270] \n",
      "4627 [D loss: (-13.448)(R 59.002, F -85.897)]  [G loss: 85.293] \n",
      "4627 [D loss: (-14.259)(R 57.824, F -86.343)]  [G loss: 85.553] \n",
      "4628 [D loss: (-13.203)(R 58.859, F -85.264)]  [G loss: 86.823] \n",
      "4628 [D loss: (-12.994)(R 60.177, F -86.165)]  [G loss: 87.607] \n",
      "4629 [D loss: (-12.004)(R 59.322, F -83.331)]  [G loss: 86.019] \n",
      "4629 [D loss: (-14.382)(R 58.057, F -86.821)]  [G loss: 85.686] \n",
      "4630 [D loss: (-15.945)(R 57.887, F -89.778)]  [G loss: 87.983] \n",
      "4630 [D loss: (-16.641)(R 58.148, F -91.429)]  [G loss: 86.986] \n",
      "4631 [D loss: (-10.523)(R 59.385, F -80.431)]  [G loss: 87.710] \n",
      "4631 [D loss: (-13.478)(R 58.698, F -85.654)]  [G loss: 84.684] \n",
      "4632 [D loss: (-12.379)(R 58.848, F -83.606)]  [G loss: 86.898] \n",
      "4632 [D loss: (-14.393)(R 56.160, F -84.946)]  [G loss: 85.565] \n",
      "4633 [D loss: (-17.266)(R 57.299, F -91.831)]  [G loss: 86.292] \n",
      "4633 [D loss: (-15.149)(R 61.450, F -91.748)]  [G loss: 87.861] \n",
      "4634 [D loss: (-14.266)(R 60.174, F -88.706)]  [G loss: 84.616] \n",
      "4634 [D loss: (-14.023)(R 59.566, F -87.612)]  [G loss: 89.191] \n",
      "4635 [D loss: (-15.641)(R 60.654, F -91.937)]  [G loss: 89.112] \n",
      "4635 [D loss: (-16.413)(R 56.658, F -89.484)]  [G loss: 87.923] \n",
      "4636 [D loss: (-11.871)(R 56.205, F -79.947)]  [G loss: 87.622] \n",
      "4636 [D loss: (-15.876)(R 61.744, F -93.495)]  [G loss: 88.535] \n",
      "4637 [D loss: (-14.733)(R 59.479, F -88.945)]  [G loss: 85.522] \n",
      "4637 [D loss: (-10.157)(R 65.452, F -85.766)]  [G loss: 86.169] \n",
      "4638 [D loss: (-14.845)(R 60.412, F -90.101)]  [G loss: 89.386] \n",
      "4638 [D loss: (-17.549)(R 62.078, F -97.175)]  [G loss: 87.829] \n",
      "4639 [D loss: (-15.008)(R 59.511, F -89.527)]  [G loss: 88.828] \n",
      "4639 [D loss: (-17.709)(R 60.567, F -95.984)]  [G loss: 88.298] \n",
      "4640 [D loss: (-12.462)(R 61.097, F -86.022)]  [G loss: 88.990] \n",
      "4640 [D loss: (-10.700)(R 60.072, F -81.473)]  [G loss: 86.318] \n",
      "4641 [D loss: (-11.822)(R 63.317, F -86.960)]  [G loss: 86.661] \n",
      "4641 [D loss: (-12.187)(R 61.642, F -86.016)]  [G loss: 86.941] \n",
      "4642 [D loss: (-13.970)(R 59.722, F -87.662)]  [G loss: 88.908] \n",
      "4642 [D loss: (-12.070)(R 60.886, F -85.027)]  [G loss: 89.040] \n",
      "4643 [D loss: (-15.458)(R 60.433, F -91.349)]  [G loss: 87.322] \n",
      "4643 [D loss: (-10.946)(R 63.845, F -85.737)]  [G loss: 86.873] \n",
      "4644 [D loss: (-13.820)(R 59.991, F -87.631)]  [G loss: 88.276] \n",
      "4644 [D loss: (-15.664)(R 60.672, F -91.999)]  [G loss: 88.070] \n",
      "4645 [D loss: (-16.203)(R 60.844, F -93.251)]  [G loss: 88.654] \n",
      "4645 [D loss: (-14.171)(R 61.861, F -90.202)]  [G loss: 87.905] \n",
      "4646 [D loss: (-12.412)(R 61.754, F -86.579)]  [G loss: 89.822] \n",
      "4646 [D loss: (-14.604)(R 59.414, F -88.621)]  [G loss: 88.004] \n",
      "4647 [D loss: (-14.969)(R 60.116, F -90.054)]  [G loss: 90.348] \n",
      "4647 [D loss: (-14.693)(R 62.335, F -91.721)]  [G loss: 87.065] \n",
      "4648 [D loss: (-13.187)(R 60.488, F -86.863)]  [G loss: 85.773] \n",
      "4648 [D loss: (-11.414)(R 64.975, F -87.803)]  [G loss: 87.816] \n",
      "4649 [D loss: (-13.706)(R 58.908, F -86.320)]  [G loss: 88.959] \n",
      "4649 [D loss: (-13.424)(R 60.137, F -86.985)]  [G loss: 87.555] \n",
      "4650 [D loss: (-10.134)(R 60.094, F -80.362)]  [G loss: 87.665] \n",
      "4650 [D loss: (-12.449)(R 61.288, F -86.186)]  [G loss: 89.816] \n",
      "4651 [D loss: (-12.646)(R 63.097, F -88.389)]  [G loss: 91.405] \n",
      "4651 [D loss: (-11.691)(R 60.413, F -83.795)]  [G loss: 87.694] \n",
      "4652 [D loss: (-10.263)(R 58.855, F -79.382)]  [G loss: 89.232] \n",
      "4652 [D loss: (-13.113)(R 59.652, F -85.878)]  [G loss: 87.387] \n",
      "4653 [D loss: (-13.529)(R 62.522, F -89.581)]  [G loss: 87.208] \n",
      "4653 [D loss: (-16.920)(R 59.291, F -93.131)]  [G loss: 88.619] \n",
      "4654 [D loss: (-16.184)(R 58.169, F -90.538)]  [G loss: 88.086] \n",
      "4654 [D loss: (-16.051)(R 61.828, F -93.929)]  [G loss: 88.519] \n",
      "4655 [D loss: (-16.265)(R 60.172, F -92.701)]  [G loss: 89.197] \n",
      "4655 [D loss: (-16.501)(R 62.383, F -95.386)]  [G loss: 89.091] \n",
      "4656 [D loss: (-15.959)(R 61.424, F -93.342)]  [G loss: 88.181] \n",
      "4656 [D loss: (-12.046)(R 62.016, F -86.107)]  [G loss: 91.351] \n",
      "4657 [D loss: (-16.145)(R 61.231, F -93.521)]  [G loss: 89.124] \n",
      "4657 [D loss: (-15.405)(R 62.991, F -93.801)]  [G loss: 90.385] \n",
      "4658 [D loss: (-15.826)(R 61.024, F -92.677)]  [G loss: 88.590] \n",
      "4658 [D loss: (-11.637)(R 65.590, F -88.865)]  [G loss: 88.795] \n",
      "4659 [D loss: (-13.129)(R 62.812, F -89.071)]  [G loss: 89.355] \n",
      "4659 [D loss: (-14.563)(R 65.082, F -94.208)]  [G loss: 90.165] \n",
      "4660 [D loss: (-14.329)(R 61.216, F -89.874)]  [G loss: 89.099] \n",
      "4660 [D loss: (-13.598)(R 63.337, F -90.533)]  [G loss: 87.653] \n",
      "4661 [D loss: (-15.537)(R 62.523, F -93.597)]  [G loss: 90.434] \n",
      "4661 [D loss: (-13.181)(R 61.995, F -88.357)]  [G loss: 89.961] \n",
      "4662 [D loss: (-9.553)(R 64.246, F -83.351)]  [G loss: 89.888] \n",
      "4662 [D loss: (-16.572)(R 61.411, F -94.555)]  [G loss: 87.974] \n",
      "4663 [D loss: (-11.097)(R 62.461, F -84.655)]  [G loss: 88.747] \n",
      "4663 [D loss: (-10.300)(R 64.902, F -85.501)]  [G loss: 89.119] \n",
      "4664 [D loss: (-12.270)(R 62.875, F -87.415)]  [G loss: 88.405] \n",
      "4664 [D loss: (-12.530)(R 61.472, F -86.533)]  [G loss: 90.360] \n",
      "4665 [D loss: (-8.947)(R 65.951, F -83.844)]  [G loss: 88.144] \n",
      "4665 [D loss: (-14.849)(R 61.275, F -90.974)]  [G loss: 89.127] \n",
      "4666 [D loss: (-12.031)(R 62.954, F -87.017)]  [G loss: 88.452] \n",
      "4666 [D loss: (-14.887)(R 62.086, F -91.861)]  [G loss: 88.170] \n",
      "4667 [D loss: (-10.440)(R 64.379, F -85.260)]  [G loss: 90.609] \n",
      "4667 [D loss: (-12.619)(R 66.810, F -92.049)]  [G loss: 90.326] \n",
      "4668 [D loss: (-14.849)(R 62.628, F -92.326)]  [G loss: 90.032] \n",
      "4668 [D loss: (-11.894)(R 62.390, F -86.179)]  [G loss: 90.291] \n",
      "4669 [D loss: (-11.073)(R 64.241, F -86.387)]  [G loss: 91.260] \n",
      "4669 [D loss: (-12.656)(R 62.073, F -87.386)]  [G loss: 89.471] \n",
      "4670 [D loss: (-9.184)(R 62.520, F -80.887)]  [G loss: 89.091] \n",
      "4670 [D loss: (-12.669)(R 64.008, F -89.346)]  [G loss: 88.913] \n",
      "4671 [D loss: (-15.452)(R 64.447, F -95.351)]  [G loss: 89.051] \n",
      "4671 [D loss: (-9.929)(R 64.069, F -83.927)]  [G loss: 89.913] \n",
      "4672 [D loss: (-12.031)(R 64.104, F -88.167)]  [G loss: 88.385] \n",
      "4672 [D loss: (-12.460)(R 60.708, F -85.628)]  [G loss: 88.288] \n",
      "4673 [D loss: (-12.873)(R 65.965, F -91.711)]  [G loss: 88.903] \n",
      "4673 [D loss: (-12.681)(R 63.340, F -88.701)]  [G loss: 87.743] \n",
      "4674 [D loss: (-10.831)(R 63.642, F -85.305)]  [G loss: 91.684] \n",
      "4674 [D loss: (-9.601)(R 65.872, F -85.073)]  [G loss: 87.066] \n",
      "4675 [D loss: (-13.125)(R 63.255, F -89.506)]  [G loss: 88.659] \n",
      "4675 [D loss: (-14.279)(R 64.057, F -92.615)]  [G loss: 90.904] \n",
      "4676 [D loss: (-12.630)(R 64.411, F -89.671)]  [G loss: 89.728] \n",
      "4676 [D loss: (-10.611)(R 66.169, F -87.392)]  [G loss: 89.768] \n",
      "4677 [D loss: (-11.710)(R 63.953, F -87.373)]  [G loss: 89.597] \n",
      "4677 [D loss: (-12.342)(R 64.160, F -88.845)]  [G loss: 88.886] \n",
      "4678 [D loss: (-16.564)(R 62.091, F -95.218)]  [G loss: 91.005] \n",
      "4678 [D loss: (-12.479)(R 62.491, F -87.448)]  [G loss: 89.151] \n",
      "4679 [D loss: (-12.294)(R 64.004, F -88.592)]  [G loss: 90.412] \n",
      "4679 [D loss: (-9.365)(R 64.476, F -83.206)]  [G loss: 88.995] \n",
      "4680 [D loss: (-11.590)(R 64.258, F -87.437)]  [G loss: 88.400] \n",
      "4680 [D loss: (-13.074)(R 66.626, F -92.773)]  [G loss: 90.281] \n",
      "4681 [D loss: (-9.655)(R 66.331, F -85.641)]  [G loss: 88.500] \n",
      "4681 [D loss: (-10.904)(R 64.517, F -86.326)]  [G loss: 89.296] \n",
      "4682 [D loss: (-15.265)(R 59.668, F -90.197)]  [G loss: 91.480] \n",
      "4682 [D loss: (-15.022)(R 60.943, F -90.987)]  [G loss: 88.072] \n",
      "4683 [D loss: (-14.237)(R 68.131, F -96.605)]  [G loss: 88.412] \n",
      "4683 [D loss: (-8.991)(R 65.239, F -83.221)]  [G loss: 89.206] \n",
      "4684 [D loss: (-11.284)(R 62.873, F -85.441)]  [G loss: 92.455] \n",
      "4684 [D loss: (-9.532)(R 65.510, F -84.574)]  [G loss: 89.154] \n",
      "4685 [D loss: (-9.128)(R 66.475, F -84.732)]  [G loss: 92.205] \n",
      "4685 [D loss: (-11.124)(R 64.113, F -86.361)]  [G loss: 89.253] \n",
      "4686 [D loss: (-9.883)(R 65.646, F -85.411)]  [G loss: 89.282] \n",
      "4686 [D loss: (-8.184)(R 65.842, F -82.211)]  [G loss: 89.481] \n",
      "4687 [D loss: (-13.758)(R 62.460, F -89.977)]  [G loss: 87.843] \n",
      "4687 [D loss: (-11.580)(R 64.788, F -87.947)]  [G loss: 89.216] \n",
      "4688 [D loss: (-13.453)(R 63.399, F -90.306)]  [G loss: 89.070] \n",
      "4688 [D loss: (-15.464)(R 65.590, F -96.518)]  [G loss: 87.380] \n",
      "4689 [D loss: (-12.999)(R 64.801, F -90.800)]  [G loss: 88.688] \n",
      "4689 [D loss: (-16.466)(R 63.670, F -96.601)]  [G loss: 90.671] \n",
      "4690 [D loss: (-13.066)(R 66.591, F -92.723)]  [G loss: 90.578] \n",
      "4690 [D loss: (-9.791)(R 67.674, F -87.256)]  [G loss: 89.404] \n",
      "4691 [D loss: (-12.536)(R 63.943, F -89.015)]  [G loss: 89.664] \n",
      "4691 [D loss: (-13.634)(R 65.138, F -92.405)]  [G loss: 90.240] \n",
      "4692 [D loss: (-9.566)(R 65.833, F -84.965)]  [G loss: 91.297] \n",
      "4692 [D loss: (-16.472)(R 63.902, F -96.847)]  [G loss: 89.292] \n",
      "4693 [D loss: (-13.384)(R 64.407, F -91.175)]  [G loss: 90.815] \n",
      "4693 [D loss: (-9.965)(R 65.398, F -85.328)]  [G loss: 88.909] \n",
      "4694 [D loss: (-15.759)(R 63.891, F -95.410)]  [G loss: 88.178] \n",
      "4694 [D loss: (-12.415)(R 65.935, F -90.766)]  [G loss: 89.446] \n",
      "4695 [D loss: (-8.326)(R 64.703, F -81.354)]  [G loss: 91.231] \n",
      "4695 [D loss: (-9.838)(R 65.747, F -85.423)]  [G loss: 88.889] \n",
      "4696 [D loss: (-13.059)(R 63.681, F -89.798)]  [G loss: 91.669] \n",
      "4696 [D loss: (-11.677)(R 63.877, F -87.232)]  [G loss: 91.249] \n",
      "4697 [D loss: (-13.702)(R 64.381, F -91.786)]  [G loss: 88.956] \n",
      "4697 [D loss: (-7.109)(R 68.247, F -82.464)]  [G loss: 87.790] \n",
      "4698 [D loss: (-14.816)(R 62.923, F -92.556)]  [G loss: 88.853] \n",
      "4698 [D loss: (-13.205)(R 64.085, F -90.495)]  [G loss: 89.210] \n",
      "4699 [D loss: (-10.687)(R 64.422, F -85.796)]  [G loss: 90.429] \n",
      "4699 [D loss: (-12.270)(R 62.987, F -87.527)]  [G loss: 89.536] \n",
      "4700 [D loss: (-11.123)(R 67.074, F -89.321)]  [G loss: 89.223] \n",
      "4700 [D loss: (-7.559)(R 69.331, F -84.450)]  [G loss: 89.536] \n",
      "4701 [D loss: (-8.210)(R 68.014, F -84.434)]  [G loss: 89.940] \n",
      "4701 [D loss: (-12.632)(R 64.780, F -90.044)]  [G loss: 89.002] \n",
      "4702 [D loss: (-11.318)(R 64.329, F -86.965)]  [G loss: 90.918] \n",
      "4702 [D loss: (-9.861)(R 65.553, F -85.275)]  [G loss: 88.454] \n",
      "4703 [D loss: (-9.611)(R 67.344, F -86.565)]  [G loss: 89.190] \n",
      "4703 [D loss: (-8.168)(R 67.473, F -83.810)]  [G loss: 90.083] \n",
      "4704 [D loss: (-9.839)(R 65.770, F -85.449)]  [G loss: 91.417] \n",
      "4704 [D loss: (-11.751)(R 63.933, F -87.435)]  [G loss: 90.333] \n",
      "4705 [D loss: (-12.285)(R 67.141, F -91.712)]  [G loss: 91.361] \n",
      "4705 [D loss: (-10.060)(R 65.437, F -85.556)]  [G loss: 90.440] \n",
      "4706 [D loss: (-10.186)(R 66.575, F -86.947)]  [G loss: 90.019] \n",
      "4706 [D loss: (-10.194)(R 66.441, F -86.828)]  [G loss: 89.013] \n",
      "4707 [D loss: (-12.108)(R 65.981, F -90.196)]  [G loss: 88.746] \n",
      "4707 [D loss: (-9.731)(R 64.162, F -83.624)]  [G loss: 90.245] \n",
      "4708 [D loss: (-11.806)(R 67.605, F -91.216)]  [G loss: 89.977] \n",
      "4708 [D loss: (-13.616)(R 63.460, F -90.692)]  [G loss: 86.349] \n",
      "4709 [D loss: (-13.664)(R 67.586, F -94.914)]  [G loss: 89.691] \n",
      "4709 [D loss: (-12.065)(R 68.166, F -92.296)]  [G loss: 90.706] \n",
      "4710 [D loss: (-14.706)(R 64.236, F -93.647)]  [G loss: 89.380] \n",
      "4710 [D loss: (-12.678)(R 65.207, F -90.564)]  [G loss: 90.833] \n",
      "4711 [D loss: (-11.808)(R 66.721, F -90.337)]  [G loss: 88.515] \n",
      "4711 [D loss: (-12.508)(R 65.352, F -90.369)]  [G loss: 89.530] \n",
      "4712 [D loss: (-11.458)(R 69.308, F -92.224)]  [G loss: 88.781] \n",
      "4712 [D loss: (-8.148)(R 68.869, F -85.165)]  [G loss: 89.669] \n",
      "4713 [D loss: (-9.952)(R 67.361, F -87.265)]  [G loss: 88.737] \n",
      "4713 [D loss: (-10.954)(R 67.771, F -89.678)]  [G loss: 89.609] \n",
      "4714 [D loss: (-8.737)(R 66.244, F -83.717)]  [G loss: 90.852] \n",
      "4714 [D loss: (-11.341)(R 65.686, F -88.369)]  [G loss: 87.572] \n",
      "4715 [D loss: (-11.938)(R 65.462, F -89.338)]  [G loss: 88.943] \n",
      "4715 [D loss: (-9.015)(R 71.132, F -89.162)]  [G loss: 87.690] \n",
      "4716 [D loss: (-9.084)(R 68.194, F -86.362)]  [G loss: 87.987] \n",
      "4716 [D loss: (-15.836)(R 69.108, F -100.781)]  [G loss: 88.118] \n",
      "4717 [D loss: (-10.444)(R 69.228, F -90.116)]  [G loss: 88.560] \n",
      "4717 [D loss: (-14.831)(R 64.314, F -93.976)]  [G loss: 88.762] \n",
      "4718 [D loss: (-11.079)(R 66.530, F -88.688)]  [G loss: 89.901] \n",
      "4718 [D loss: (-12.392)(R 66.288, F -91.071)]  [G loss: 89.462] \n",
      "4719 [D loss: (-9.529)(R 67.567, F -86.625)]  [G loss: 88.482] \n",
      "4719 [D loss: (-12.918)(R 67.191, F -93.028)]  [G loss: 88.681] \n",
      "4720 [D loss: (-5.923)(R 70.222, F -82.069)]  [G loss: 88.683] \n",
      "4720 [D loss: (-11.064)(R 62.786, F -84.914)]  [G loss: 91.316] \n",
      "4721 [D loss: (-9.593)(R 65.650, F -84.836)]  [G loss: 88.618] \n",
      "4721 [D loss: (-13.617)(R 66.394, F -93.629)]  [G loss: 88.760] \n",
      "4722 [D loss: (-13.451)(R 65.001, F -91.903)]  [G loss: 90.003] \n",
      "4722 [D loss: (-8.551)(R 68.812, F -85.913)]  [G loss: 89.624] \n",
      "4723 [D loss: (-9.825)(R 66.448, F -86.097)]  [G loss: 89.992] \n",
      "4723 [D loss: (-11.329)(R 67.289, F -89.947)]  [G loss: 90.327] \n",
      "4724 [D loss: (-9.421)(R 66.037, F -84.878)]  [G loss: 90.905] \n",
      "4724 [D loss: (-10.186)(R 69.994, F -90.366)]  [G loss: 92.122] \n",
      "4725 [D loss: (-14.763)(R 65.456, F -94.982)]  [G loss: 89.428] \n",
      "4725 [D loss: (-10.279)(R 72.111, F -92.668)]  [G loss: 87.096] \n",
      "4726 [D loss: (-11.301)(R 65.748, F -88.350)]  [G loss: 91.321] \n",
      "4726 [D loss: (-11.350)(R 67.454, F -90.155)]  [G loss: 88.884] \n",
      "4727 [D loss: (-13.090)(R 66.075, F -92.254)]  [G loss: 89.890] \n",
      "4727 [D loss: (-15.698)(R 64.529, F -95.924)]  [G loss: 88.524] \n",
      "4728 [D loss: (-12.407)(R 69.191, F -94.004)]  [G loss: 90.244] \n",
      "4728 [D loss: (-11.929)(R 69.133, F -92.991)]  [G loss: 90.403] \n",
      "4729 [D loss: (-10.451)(R 64.220, F -85.122)]  [G loss: 89.140] \n",
      "4729 [D loss: (-9.732)(R 69.377, F -88.840)]  [G loss: 87.824] \n",
      "4730 [D loss: (-16.216)(R 60.030, F -92.463)]  [G loss: 89.572] \n",
      "4730 [D loss: (-15.943)(R 65.260, F -97.146)]  [G loss: 90.385] \n",
      "4731 [D loss: (-11.041)(R 65.150, F -87.233)]  [G loss: 89.624] \n",
      "4731 [D loss: (-14.472)(R 66.200, F -95.144)]  [G loss: 87.548] \n",
      "4732 [D loss: (-10.891)(R 68.438, F -90.221)]  [G loss: 88.396] \n",
      "4732 [D loss: (-9.188)(R 69.590, F -87.966)]  [G loss: 87.488] \n",
      "4733 [D loss: (-9.477)(R 70.493, F -89.448)]  [G loss: 88.697] \n",
      "4733 [D loss: (-14.020)(R 64.485, F -92.524)]  [G loss: 88.417] \n",
      "4734 [D loss: (-12.288)(R 67.319, F -91.894)]  [G loss: 88.035] \n",
      "4734 [D loss: (-8.341)(R 69.685, F -86.367)]  [G loss: 89.371] \n",
      "4735 [D loss: (-10.610)(R 65.864, F -87.085)]  [G loss: 89.814] \n",
      "4735 [D loss: (-4.549)(R 67.908, F -77.005)]  [G loss: 89.637] \n",
      "4736 [D loss: (-11.138)(R 68.386, F -90.663)]  [G loss: 88.789] \n",
      "4736 [D loss: (-9.761)(R 65.489, F -85.010)]  [G loss: 88.491] \n",
      "4737 [D loss: (-7.854)(R 71.152, F -86.859)]  [G loss: 88.426] \n",
      "4737 [D loss: (-8.008)(R 68.416, F -84.433)]  [G loss: 88.964] \n",
      "4738 [D loss: (-11.388)(R 67.605, F -90.381)]  [G loss: 88.565] \n",
      "4738 [D loss: (-10.861)(R 66.453, F -88.175)]  [G loss: 88.981] \n",
      "4739 [D loss: (-13.583)(R 66.584, F -93.750)]  [G loss: 89.380] \n",
      "4739 [D loss: (-10.156)(R 69.943, F -90.254)]  [G loss: 89.383] \n",
      "4740 [D loss: (-10.298)(R 64.883, F -85.479)]  [G loss: 90.019] \n",
      "4740 [D loss: (-12.732)(R 67.430, F -92.894)]  [G loss: 90.963] \n",
      "4741 [D loss: (-12.103)(R 67.444, F -91.649)]  [G loss: 89.501] \n",
      "4741 [D loss: (-9.595)(R 66.642, F -85.833)]  [G loss: 88.814] \n",
      "4742 [D loss: (-9.650)(R 69.095, F -88.396)]  [G loss: 87.544] \n",
      "4742 [D loss: (-10.673)(R 67.658, F -89.005)]  [G loss: 89.357] \n",
      "4743 [D loss: (-13.158)(R 67.066, F -93.382)]  [G loss: 91.126] \n",
      "4743 [D loss: (-13.935)(R 68.071, F -95.942)]  [G loss: 88.907] \n",
      "4744 [D loss: (-10.729)(R 66.211, F -87.669)]  [G loss: 89.543] \n",
      "4744 [D loss: (-10.141)(R 68.109, F -88.391)]  [G loss: 87.375] \n",
      "4745 [D loss: (-8.162)(R 69.273, F -85.597)]  [G loss: 89.820] \n",
      "4745 [D loss: (-9.550)(R 68.493, F -87.592)]  [G loss: 87.365] \n",
      "4746 [D loss: (-11.587)(R 68.687, F -91.860)]  [G loss: 91.317] \n",
      "4746 [D loss: (-9.691)(R 69.851, F -89.234)]  [G loss: 89.333] \n",
      "4747 [D loss: (-12.799)(R 67.050, F -92.649)]  [G loss: 88.389] \n",
      "4747 [D loss: (-12.173)(R 66.192, F -90.539)]  [G loss: 90.307] \n",
      "4748 [D loss: (-8.360)(R 69.521, F -86.240)]  [G loss: 88.829] \n",
      "4748 [D loss: (-10.629)(R 65.986, F -87.243)]  [G loss: 88.400] \n",
      "4749 [D loss: (-9.083)(R 70.667, F -88.832)]  [G loss: 90.776] \n",
      "4749 [D loss: (-14.762)(R 67.382, F -96.906)]  [G loss: 87.514] \n",
      "4750 [D loss: (-10.458)(R 66.810, F -87.726)]  [G loss: 87.934] \n",
      "4750 [D loss: (-12.746)(R 68.163, F -93.655)]  [G loss: 90.225] \n",
      "4751 [D loss: (-10.500)(R 66.643, F -87.643)]  [G loss: 91.388] \n",
      "4751 [D loss: (-9.072)(R 68.954, F -87.098)]  [G loss: 88.649] \n",
      "4752 [D loss: (-8.804)(R 68.665, F -86.273)]  [G loss: 87.475] \n",
      "4752 [D loss: (-10.975)(R 66.518, F -88.467)]  [G loss: 88.804] \n",
      "4753 [D loss: (-6.782)(R 72.161, F -85.725)]  [G loss: 88.678] \n",
      "4753 [D loss: (-9.782)(R 69.434, F -88.998)]  [G loss: 89.253] \n",
      "4754 [D loss: (-10.358)(R 69.891, F -90.608)]  [G loss: 88.520] \n",
      "4754 [D loss: (-7.418)(R 69.873, F -84.709)]  [G loss: 88.052] \n",
      "4755 [D loss: (-12.430)(R 67.563, F -92.423)]  [G loss: 88.236] \n",
      "4755 [D loss: (-12.886)(R 68.050, F -93.823)]  [G loss: 88.566] \n",
      "4756 [D loss: (-6.803)(R 68.805, F -82.411)]  [G loss: 87.430] \n",
      "4756 [D loss: (-7.310)(R 71.381, F -86.002)]  [G loss: 88.582] \n",
      "4757 [D loss: (-8.185)(R 70.601, F -86.971)]  [G loss: 86.446] \n",
      "4757 [D loss: (-9.818)(R 70.674, F -90.309)]  [G loss: 89.306] \n",
      "4758 [D loss: (-9.920)(R 70.440, F -90.280)]  [G loss: 88.828] \n",
      "4758 [D loss: (-8.668)(R 72.577, F -89.914)]  [G loss: 88.380] \n",
      "4759 [D loss: (-8.696)(R 69.482, F -86.874)]  [G loss: 88.195] \n",
      "4759 [D loss: (-11.549)(R 69.921, F -93.018)]  [G loss: 87.472] \n",
      "4760 [D loss: (-8.507)(R 65.352, F -82.367)]  [G loss: 86.421] \n",
      "4760 [D loss: (-10.585)(R 68.638, F -89.809)]  [G loss: 89.726] \n",
      "4761 [D loss: (-10.027)(R 68.374, F -88.429)]  [G loss: 87.300] \n",
      "4761 [D loss: (-8.146)(R 68.513, F -84.804)]  [G loss: 88.630] \n",
      "4762 [D loss: (-7.355)(R 71.459, F -86.170)]  [G loss: 89.544] \n",
      "4762 [D loss: (-7.102)(R 68.222, F -82.426)]  [G loss: 88.769] \n",
      "4763 [D loss: (-12.520)(R 66.856, F -91.896)]  [G loss: 88.245] \n",
      "4763 [D loss: (-10.584)(R 70.739, F -91.906)]  [G loss: 88.045] \n",
      "4764 [D loss: (-8.378)(R 68.288, F -85.044)]  [G loss: 89.157] \n",
      "4764 [D loss: (-8.966)(R 70.125, F -88.058)]  [G loss: 87.521] \n",
      "4765 [D loss: (-7.315)(R 70.743, F -85.373)]  [G loss: 87.288] \n",
      "4765 [D loss: (-8.455)(R 66.520, F -83.430)]  [G loss: 88.539] \n",
      "4766 [D loss: (-9.072)(R 69.046, F -87.191)]  [G loss: 88.196] \n",
      "4766 [D loss: (-7.885)(R 69.014, F -84.783)]  [G loss: 88.764] \n",
      "4767 [D loss: (-8.629)(R 67.051, F -84.309)]  [G loss: 86.118] \n",
      "4767 [D loss: (-10.891)(R 66.492, F -88.274)]  [G loss: 89.396] \n",
      "4768 [D loss: (-7.499)(R 68.606, F -83.604)]  [G loss: 88.303] \n",
      "4768 [D loss: (-4.360)(R 72.095, F -80.815)]  [G loss: 84.255] \n",
      "4769 [D loss: (-6.318)(R 68.798, F -81.433)]  [G loss: 87.476] \n",
      "4769 [D loss: (-8.642)(R 68.477, F -85.760)]  [G loss: 88.220] \n",
      "4770 [D loss: (-7.825)(R 66.927, F -82.578)]  [G loss: 87.699] \n",
      "4770 [D loss: (-9.144)(R 70.376, F -88.664)]  [G loss: 89.946] \n",
      "4771 [D loss: (-6.158)(R 68.158, F -80.474)]  [G loss: 87.928] \n",
      "4771 [D loss: (-5.116)(R 67.086, F -77.317)]  [G loss: 87.806] \n",
      "4772 [D loss: (-8.492)(R 67.548, F -84.531)]  [G loss: 89.374] \n",
      "4772 [D loss: (-4.124)(R 69.402, F -77.651)]  [G loss: 87.671] \n",
      "4773 [D loss: (-8.291)(R 68.843, F -85.425)]  [G loss: 88.457] \n",
      "4773 [D loss: (-8.934)(R 66.426, F -84.293)]  [G loss: 87.852] \n",
      "4774 [D loss: (-12.484)(R 66.075, F -91.043)]  [G loss: 87.168] \n",
      "4774 [D loss: (-11.072)(R 65.579, F -87.723)]  [G loss: 84.820] \n",
      "4775 [D loss: (-13.007)(R 66.859, F -92.874)]  [G loss: 87.470] \n",
      "4775 [D loss: (-10.486)(R 68.936, F -89.908)]  [G loss: 88.398] \n",
      "4776 [D loss: (-9.036)(R 67.977, F -86.049)]  [G loss: 87.158] \n",
      "4776 [D loss: (-6.764)(R 69.993, F -83.521)]  [G loss: 87.325] \n",
      "4777 [D loss: (-10.295)(R 69.781, F -90.372)]  [G loss: 87.934] \n",
      "4777 [D loss: (-13.136)(R 69.065, F -95.337)]  [G loss: 86.072] \n",
      "4778 [D loss: (-9.049)(R 71.119, F -89.218)]  [G loss: 86.260] \n",
      "4778 [D loss: (-10.392)(R 67.713, F -88.498)]  [G loss: 84.631] \n",
      "4779 [D loss: (-8.375)(R 70.150, F -86.899)]  [G loss: 88.625] \n",
      "4779 [D loss: (-9.949)(R 68.185, F -88.083)]  [G loss: 88.083] \n",
      "4780 [D loss: (-6.387)(R 64.861, F -77.636)]  [G loss: 86.386] \n",
      "4780 [D loss: (-5.712)(R 72.353, F -83.778)]  [G loss: 87.982] \n",
      "4781 [D loss: (-13.287)(R 66.035, F -92.609)]  [G loss: 86.138] \n",
      "4781 [D loss: (-12.046)(R 69.176, F -93.267)]  [G loss: 85.646] \n",
      "4782 [D loss: (-7.366)(R 68.276, F -83.008)]  [G loss: 89.348] \n",
      "4782 [D loss: (-11.251)(R 65.144, F -87.647)]  [G loss: 88.421] \n",
      "4783 [D loss: (-5.899)(R 70.693, F -82.492)]  [G loss: 85.818] \n",
      "4783 [D loss: (-12.230)(R 64.964, F -89.423)]  [G loss: 88.233] \n",
      "4784 [D loss: (-5.575)(R 70.829, F -81.979)]  [G loss: 86.502] \n",
      "4784 [D loss: (-9.599)(R 69.632, F -88.831)]  [G loss: 86.101] \n",
      "4785 [D loss: (-5.915)(R 70.324, F -82.155)]  [G loss: 87.189] \n",
      "4785 [D loss: (-10.359)(R 67.189, F -87.906)]  [G loss: 88.350] \n",
      "4786 [D loss: (-11.526)(R 73.132, F -96.184)]  [G loss: 86.123] \n",
      "4786 [D loss: (-10.800)(R 65.481, F -87.081)]  [G loss: 87.772] \n",
      "4787 [D loss: (-10.030)(R 66.716, F -86.776)]  [G loss: 87.337] \n",
      "4787 [D loss: (-5.035)(R 72.785, F -82.855)]  [G loss: 85.183] \n",
      "4788 [D loss: (-13.461)(R 65.068, F -91.989)]  [G loss: 84.165] \n",
      "4788 [D loss: (-11.815)(R 65.901, F -89.531)]  [G loss: 86.434] \n",
      "4789 [D loss: (-11.817)(R 71.933, F -95.566)]  [G loss: 86.386] \n",
      "4789 [D loss: (-12.957)(R 68.312, F -94.226)]  [G loss: 85.008] \n",
      "4790 [D loss: (-8.835)(R 72.205, F -89.874)]  [G loss: 85.831] \n",
      "4790 [D loss: (-7.417)(R 69.976, F -84.810)]  [G loss: 85.720] \n",
      "4791 [D loss: (-8.024)(R 68.064, F -84.111)]  [G loss: 86.093] \n",
      "4791 [D loss: (-8.375)(R 72.162, F -88.912)]  [G loss: 85.870] \n",
      "4792 [D loss: (-9.198)(R 70.136, F -88.533)]  [G loss: 86.333] \n",
      "4792 [D loss: (-6.066)(R 68.476, F -80.607)]  [G loss: 88.377] \n",
      "4793 [D loss: (-8.118)(R 69.230, F -85.467)]  [G loss: 87.664] \n",
      "4793 [D loss: (-12.025)(R 71.368, F -95.418)]  [G loss: 85.500] \n",
      "4794 [D loss: (-7.599)(R 70.038, F -85.236)]  [G loss: 86.078] \n",
      "4794 [D loss: (-9.470)(R 69.442, F -88.383)]  [G loss: 88.347] \n",
      "4795 [D loss: (-5.836)(R 69.867, F -81.539)]  [G loss: 85.773] \n",
      "4795 [D loss: (-5.385)(R 67.893, F -78.664)]  [G loss: 87.464] \n",
      "4796 [D loss: (-10.900)(R 69.270, F -91.070)]  [G loss: 83.885] \n",
      "4796 [D loss: (-10.785)(R 68.760, F -90.331)]  [G loss: 86.867] \n",
      "4797 [D loss: (-12.269)(R 66.240, F -90.779)]  [G loss: 85.078] \n",
      "4797 [D loss: (-9.567)(R 68.302, F -87.435)]  [G loss: 88.880] \n",
      "4798 [D loss: (-9.605)(R 69.657, F -88.867)]  [G loss: 85.614] \n",
      "4798 [D loss: (-9.619)(R 69.552, F -88.790)]  [G loss: 87.854] \n",
      "4799 [D loss: (-8.475)(R 68.024, F -84.975)]  [G loss: 85.646] \n",
      "4799 [D loss: (-9.048)(R 67.522, F -85.618)]  [G loss: 87.358] \n",
      "4800 [D loss: (-4.719)(R 73.291, F -82.730)]  [G loss: 88.122] \n",
      "4800 [D loss: (-2.622)(R 73.469, F -78.714)]  [G loss: 84.862] \n",
      "4801 [D loss: (-7.758)(R 66.267, F -81.783)]  [G loss: 87.422] \n",
      "4801 [D loss: (-7.992)(R 71.254, F -87.238)]  [G loss: 86.981] \n",
      "4802 [D loss: (-11.192)(R 68.383, F -90.767)]  [G loss: 85.465] \n",
      "4802 [D loss: (-6.990)(R 69.502, F -83.482)]  [G loss: 88.123] \n",
      "4803 [D loss: (-8.962)(R 68.033, F -85.957)]  [G loss: 85.485] \n",
      "4803 [D loss: (-10.302)(R 67.628, F -88.233)]  [G loss: 83.221] \n",
      "4804 [D loss: (-8.846)(R 69.153, F -86.846)]  [G loss: 86.704] \n",
      "4804 [D loss: (-8.851)(R 67.889, F -85.590)]  [G loss: 86.940] \n",
      "4805 [D loss: (-10.362)(R 69.326, F -90.050)]  [G loss: 86.185] \n",
      "4805 [D loss: (-11.692)(R 68.599, F -91.982)]  [G loss: 86.718] \n",
      "4806 [D loss: (-5.263)(R 69.889, F -80.416)]  [G loss: 86.511] \n",
      "4806 [D loss: (-6.630)(R 69.555, F -82.814)]  [G loss: 87.063] \n",
      "4807 [D loss: (-7.882)(R 71.432, F -87.197)]  [G loss: 87.698] \n",
      "4807 [D loss: (-9.252)(R 70.033, F -88.538)]  [G loss: 85.323] \n",
      "4808 [D loss: (-6.196)(R 70.368, F -82.760)]  [G loss: 88.197] \n",
      "4808 [D loss: (-9.233)(R 69.756, F -88.221)]  [G loss: 85.517] \n",
      "4809 [D loss: (-8.804)(R 71.518, F -89.126)]  [G loss: 87.002] \n",
      "4809 [D loss: (-7.627)(R 69.267, F -84.521)]  [G loss: 85.166] \n",
      "4810 [D loss: (-8.035)(R 70.038, F -86.107)]  [G loss: 85.971] \n",
      "4810 [D loss: (-5.988)(R 69.132, F -81.108)]  [G loss: 84.857] \n",
      "4811 [D loss: (-9.003)(R 65.637, F -83.642)]  [G loss: 84.892] \n",
      "4811 [D loss: (-3.970)(R 75.238, F -83.178)]  [G loss: 85.128] \n",
      "4812 [D loss: (-5.649)(R 72.332, F -83.630)]  [G loss: 86.054] \n",
      "4812 [D loss: (-7.637)(R 70.525, F -85.799)]  [G loss: 85.263] \n",
      "4813 [D loss: (-7.041)(R 71.194, F -85.275)]  [G loss: 84.642] \n",
      "4813 [D loss: (-7.245)(R 69.609, F -84.100)]  [G loss: 83.369] \n",
      "4814 [D loss: (-7.252)(R 70.911, F -85.415)]  [G loss: 83.068] \n",
      "4814 [D loss: (-6.838)(R 65.966, F -79.641)]  [G loss: 85.218] \n",
      "4815 [D loss: (-10.241)(R 68.966, F -89.448)]  [G loss: 84.437] \n",
      "4815 [D loss: (-6.163)(R 72.473, F -84.800)]  [G loss: 84.788] \n",
      "4816 [D loss: (-8.195)(R 67.868, F -84.257)]  [G loss: 84.270] \n",
      "4816 [D loss: (-6.575)(R 71.493, F -84.643)]  [G loss: 86.629] \n",
      "4817 [D loss: (-12.079)(R 69.376, F -93.534)]  [G loss: 86.511] \n",
      "4817 [D loss: (-6.963)(R 67.138, F -81.064)]  [G loss: 84.962] \n",
      "4818 [D loss: (-5.621)(R 69.603, F -80.845)]  [G loss: 84.456] \n",
      "4818 [D loss: (-6.562)(R 72.671, F -85.796)]  [G loss: 85.026] \n",
      "4819 [D loss: (-11.076)(R 67.526, F -89.678)]  [G loss: 83.742] \n",
      "4819 [D loss: (-4.007)(R 70.135, F -78.149)]  [G loss: 85.780] \n",
      "4820 [D loss: (-8.366)(R 69.781, F -86.514)]  [G loss: 83.305] \n",
      "4820 [D loss: (-10.706)(R 69.271, F -90.683)]  [G loss: 83.382] \n",
      "4821 [D loss: (-5.788)(R 70.460, F -82.037)]  [G loss: 84.995] \n",
      "4821 [D loss: (-7.873)(R 68.427, F -84.172)]  [G loss: 86.363] \n",
      "4822 [D loss: (-6.518)(R 71.577, F -84.613)]  [G loss: 84.604] \n",
      "4822 [D loss: (-7.998)(R 70.196, F -86.193)]  [G loss: 85.142] \n",
      "4823 [D loss: (-7.437)(R 66.929, F -81.803)]  [G loss: 84.877] \n",
      "4823 [D loss: (-5.886)(R 70.934, F -82.705)]  [G loss: 84.406] \n",
      "4824 [D loss: (-7.838)(R 67.750, F -83.426)]  [G loss: 85.274] \n",
      "4824 [D loss: (-6.129)(R 71.035, F -83.294)]  [G loss: 84.350] \n",
      "4825 [D loss: (-8.881)(R 67.180, F -84.943)]  [G loss: 86.973] \n",
      "4825 [D loss: (-7.486)(R 67.541, F -82.513)]  [G loss: 85.287] \n",
      "4826 [D loss: (-7.384)(R 67.913, F -82.681)]  [G loss: 84.263] \n",
      "4826 [D loss: (-14.367)(R 67.808, F -96.541)]  [G loss: 85.750] \n",
      "4827 [D loss: (-11.327)(R 67.119, F -89.772)]  [G loss: 81.508] \n",
      "4827 [D loss: (-7.655)(R 70.595, F -85.904)]  [G loss: 84.564] \n",
      "4828 [D loss: (-7.270)(R 67.119, F -81.658)]  [G loss: 83.573] \n",
      "4828 [D loss: (-7.195)(R 69.080, F -83.470)]  [G loss: 85.261] \n",
      "4829 [D loss: (-6.183)(R 68.530, F -80.897)]  [G loss: 85.634] \n",
      "4829 [D loss: (-7.623)(R 68.237, F -83.482)]  [G loss: 84.103] \n",
      "4830 [D loss: (-5.963)(R 71.454, F -83.380)]  [G loss: 83.215] \n",
      "4830 [D loss: (-3.720)(R 72.427, F -79.867)]  [G loss: 85.913] \n",
      "4831 [D loss: (-5.382)(R 70.728, F -81.493)]  [G loss: 82.781] \n",
      "4831 [D loss: (-8.979)(R 67.145, F -85.103)]  [G loss: 85.013] \n",
      "4832 [D loss: (-6.602)(R 71.404, F -84.608)]  [G loss: 86.607] \n",
      "4832 [D loss: (-10.280)(R 70.899, F -91.459)]  [G loss: 82.979] \n",
      "4833 [D loss: (-6.397)(R 69.380, F -82.175)]  [G loss: 85.076] \n",
      "4833 [D loss: (-9.303)(R 72.869, F -91.474)]  [G loss: 83.478] \n",
      "4834 [D loss: (-4.202)(R 68.906, F -77.309)]  [G loss: 85.751] \n",
      "4834 [D loss: (-8.908)(R 70.388, F -88.203)]  [G loss: 85.226] \n",
      "4835 [D loss: (-9.231)(R 67.583, F -86.045)]  [G loss: 84.018] \n",
      "4835 [D loss: (-6.015)(R 71.884, F -83.914)]  [G loss: 85.640] \n",
      "4836 [D loss: (-7.496)(R 70.654, F -85.647)]  [G loss: 83.010] \n",
      "4836 [D loss: (-9.303)(R 66.160, F -84.766)]  [G loss: 84.148] \n",
      "4837 [D loss: (-8.778)(R 68.235, F -85.790)]  [G loss: 84.885] \n",
      "4837 [D loss: (-4.068)(R 73.235, F -81.372)]  [G loss: 85.379] \n",
      "4838 [D loss: (-8.116)(R 71.360, F -87.592)]  [G loss: 83.609] \n",
      "4838 [D loss: (-8.482)(R 70.532, F -87.495)]  [G loss: 84.112] \n",
      "4839 [D loss: (-7.119)(R 71.206, F -85.445)]  [G loss: 84.260] \n",
      "4839 [D loss: (-6.785)(R 68.783, F -82.354)]  [G loss: 81.812] \n",
      "4840 [D loss: (-5.280)(R 74.618, F -85.179)]  [G loss: 84.294] \n",
      "4840 [D loss: (-7.191)(R 67.590, F -81.972)]  [G loss: 83.416] \n",
      "4841 [D loss: (-5.883)(R 70.671, F -82.437)]  [G loss: 83.724] \n",
      "4841 [D loss: (-9.285)(R 67.041, F -85.611)]  [G loss: 84.273] \n",
      "4842 [D loss: (-5.374)(R 71.943, F -82.691)]  [G loss: 84.713] \n",
      "4842 [D loss: (-7.448)(R 68.089, F -82.985)]  [G loss: 81.628] \n",
      "4843 [D loss: (-9.416)(R 68.581, F -87.413)]  [G loss: 83.452] \n",
      "4843 [D loss: (-4.933)(R 71.343, F -81.208)]  [G loss: 84.339] \n",
      "4844 [D loss: (-5.844)(R 69.020, F -80.708)]  [G loss: 84.491] \n",
      "4844 [D loss: (-2.922)(R 69.751, F -75.595)]  [G loss: 84.280] \n",
      "4845 [D loss: (-4.928)(R 70.576, F -80.431)]  [G loss: 85.573] \n",
      "4845 [D loss: (-6.741)(R 72.434, F -85.917)]  [G loss: 83.584] \n",
      "4846 [D loss: (-4.074)(R 71.717, F -79.864)]  [G loss: 83.209] \n",
      "4846 [D loss: (-5.742)(R 67.509, F -78.994)]  [G loss: 84.394] \n",
      "4847 [D loss: (-8.359)(R 71.320, F -88.039)]  [G loss: 85.038] \n",
      "4847 [D loss: (-7.117)(R 71.689, F -85.923)]  [G loss: 83.717] \n",
      "4848 [D loss: (-6.123)(R 64.764, F -77.009)]  [G loss: 82.659] \n",
      "4848 [D loss: (-6.844)(R 68.985, F -82.672)]  [G loss: 83.201] \n",
      "4849 [D loss: (-3.435)(R 70.483, F -77.354)]  [G loss: 82.589] \n",
      "4849 [D loss: (-7.071)(R 67.061, F -81.203)]  [G loss: 82.800] \n",
      "4850 [D loss: (-6.307)(R 71.127, F -83.741)]  [G loss: 81.812] \n",
      "4850 [D loss: (-6.200)(R 69.035, F -81.435)]  [G loss: 83.217] \n",
      "4851 [D loss: (-6.628)(R 70.558, F -83.815)]  [G loss: 83.885] \n",
      "4851 [D loss: (-6.253)(R 71.223, F -83.729)]  [G loss: 83.179] \n",
      "4852 [D loss: (-6.310)(R 73.301, F -85.921)]  [G loss: 83.881] \n",
      "4852 [D loss: (-5.851)(R 71.418, F -83.120)]  [G loss: 84.385] \n",
      "4853 [D loss: (-6.320)(R 68.424, F -81.064)]  [G loss: 82.259] \n",
      "4853 [D loss: (-7.244)(R 71.695, F -86.183)]  [G loss: 80.555] \n",
      "4854 [D loss: (-10.124)(R 69.296, F -89.544)]  [G loss: 84.214] \n",
      "4854 [D loss: (-4.962)(R 70.691, F -80.615)]  [G loss: 81.420] \n",
      "4855 [D loss: (-4.582)(R 74.297, F -83.460)]  [G loss: 82.001] \n",
      "4855 [D loss: (-10.272)(R 73.298, F -93.841)]  [G loss: 81.699] \n",
      "4856 [D loss: (-8.613)(R 68.033, F -85.260)]  [G loss: 84.170] \n",
      "4856 [D loss: (-7.226)(R 70.139, F -84.591)]  [G loss: 82.923] \n",
      "4857 [D loss: (-9.771)(R 70.215, F -89.756)]  [G loss: 83.217] \n",
      "4857 [D loss: (-4.110)(R 70.244, F -78.464)]  [G loss: 81.269] \n",
      "4858 [D loss: (-6.468)(R 71.003, F -83.938)]  [G loss: 84.631] \n",
      "4858 [D loss: (-8.280)(R 68.617, F -85.176)]  [G loss: 85.047] \n",
      "4859 [D loss: (-3.761)(R 72.178, F -79.700)]  [G loss: 81.141] \n",
      "4859 [D loss: (-8.071)(R 70.075, F -86.218)]  [G loss: 83.797] \n",
      "4860 [D loss: (-4.338)(R 69.463, F -78.139)]  [G loss: 82.633] \n",
      "4860 [D loss: (-6.391)(R 67.662, F -80.444)]  [G loss: 84.271] \n",
      "4861 [D loss: (-4.805)(R 73.056, F -82.666)]  [G loss: 84.632] \n",
      "4861 [D loss: (-5.204)(R 67.595, F -78.002)]  [G loss: 84.608] \n",
      "4862 [D loss: (-8.177)(R 68.946, F -85.301)]  [G loss: 81.390] \n",
      "4862 [D loss: (-5.204)(R 70.215, F -80.623)]  [G loss: 82.075] \n",
      "4863 [D loss: (-5.339)(R 73.027, F -83.705)]  [G loss: 82.816] \n",
      "4863 [D loss: (-6.862)(R 72.895, F -86.620)]  [G loss: 81.371] \n",
      "4864 [D loss: (-5.249)(R 72.497, F -82.994)]  [G loss: 81.450] \n",
      "4864 [D loss: (-7.661)(R 69.514, F -84.837)]  [G loss: 81.155] \n",
      "4865 [D loss: (-5.386)(R 69.587, F -80.359)]  [G loss: 81.820] \n",
      "4865 [D loss: (-7.644)(R 67.986, F -83.273)]  [G loss: 84.055] \n",
      "4866 [D loss: (-4.723)(R 71.249, F -80.695)]  [G loss: 83.355] \n",
      "4866 [D loss: (-8.963)(R 70.467, F -88.392)]  [G loss: 82.538] \n",
      "4867 [D loss: (-6.971)(R 70.133, F -84.074)]  [G loss: 81.435] \n",
      "4867 [D loss: (-4.706)(R 71.245, F -80.656)]  [G loss: 85.203] \n",
      "4868 [D loss: (-6.987)(R 69.345, F -83.320)]  [G loss: 81.373] \n",
      "4868 [D loss: (-5.495)(R 72.201, F -83.192)]  [G loss: 83.700] \n",
      "4869 [D loss: (-10.565)(R 66.798, F -87.927)]  [G loss: 80.636] \n",
      "4869 [D loss: (-5.635)(R 71.812, F -83.082)]  [G loss: 80.503] \n",
      "4870 [D loss: (-5.065)(R 71.718, F -81.847)]  [G loss: 81.192] \n",
      "4870 [D loss: (-10.244)(R 67.285, F -87.773)]  [G loss: 81.270] \n",
      "4871 [D loss: (-5.663)(R 72.569, F -83.894)]  [G loss: 81.326] \n",
      "4871 [D loss: (-8.497)(R 70.460, F -87.454)]  [G loss: 83.619] \n",
      "4872 [D loss: (-7.777)(R 69.843, F -85.396)]  [G loss: 80.900] \n",
      "4872 [D loss: (-6.912)(R 69.469, F -83.294)]  [G loss: 81.221] \n",
      "4873 [D loss: (-4.571)(R 73.872, F -83.015)]  [G loss: 82.354] \n",
      "4873 [D loss: (-4.294)(R 71.760, F -80.349)]  [G loss: 79.824] \n",
      "4874 [D loss: (-8.918)(R 70.036, F -87.871)]  [G loss: 83.015] \n",
      "4874 [D loss: (-4.682)(R 71.250, F -80.615)]  [G loss: 83.523] \n",
      "4875 [D loss: (-8.989)(R 66.893, F -84.871)]  [G loss: 82.152] \n",
      "4875 [D loss: (-3.452)(R 70.819, F -77.722)]  [G loss: 80.233] \n",
      "4876 [D loss: (-6.330)(R 71.063, F -83.723)]  [G loss: 81.918] \n",
      "4876 [D loss: (-7.158)(R 70.355, F -84.671)]  [G loss: 82.665] \n",
      "4877 [D loss: (-5.244)(R 72.037, F -82.524)]  [G loss: 81.079] \n",
      "4877 [D loss: (-2.180)(R 72.059, F -76.419)]  [G loss: 80.431] \n",
      "4878 [D loss: (-8.567)(R 68.351, F -85.486)]  [G loss: 81.871] \n",
      "4878 [D loss: (-8.956)(R 67.563, F -85.474)]  [G loss: 82.377] \n",
      "4879 [D loss: (-6.801)(R 69.503, F -83.104)]  [G loss: 81.266] \n",
      "4879 [D loss: (-8.041)(R 67.471, F -83.554)]  [G loss: 81.584] \n",
      "4880 [D loss: (-6.755)(R 70.650, F -84.160)]  [G loss: 78.859] \n",
      "4880 [D loss: (-7.098)(R 71.389, F -85.585)]  [G loss: 83.740] \n",
      "4881 [D loss: (-2.657)(R 71.647, F -76.960)]  [G loss: 79.384] \n",
      "4881 [D loss: (-6.843)(R 67.632, F -81.318)]  [G loss: 78.959] \n",
      "4882 [D loss: (-5.569)(R 69.116, F -80.253)]  [G loss: 81.851] \n",
      "4882 [D loss: (-6.720)(R 70.493, F -83.933)]  [G loss: 80.808] \n",
      "4883 [D loss: (-5.651)(R 67.580, F -78.881)]  [G loss: 79.137] \n",
      "4883 [D loss: (-5.012)(R 70.821, F -80.846)]  [G loss: 80.693] \n",
      "4884 [D loss: (-2.491)(R 69.571, F -74.553)]  [G loss: 81.605] \n",
      "4884 [D loss: (-1.643)(R 69.991, F -73.278)]  [G loss: 80.484] \n",
      "4885 [D loss: (-3.479)(R 71.454, F -78.412)]  [G loss: 80.982] \n",
      "4885 [D loss: (-4.922)(R 70.388, F -80.232)]  [G loss: 81.614] \n",
      "4886 [D loss: (-4.801)(R 71.011, F -80.613)]  [G loss: 79.625] \n",
      "4886 [D loss: (-7.434)(R 66.822, F -81.690)]  [G loss: 77.685] \n",
      "4887 [D loss: (-4.778)(R 72.958, F -82.514)]  [G loss: 80.750] \n",
      "4887 [D loss: (-4.421)(R 68.898, F -77.741)]  [G loss: 82.764] \n",
      "4888 [D loss: (-4.986)(R 71.324, F -81.295)]  [G loss: 81.842] \n",
      "4888 [D loss: (-7.188)(R 66.988, F -81.364)]  [G loss: 81.066] \n",
      "4889 [D loss: (-5.109)(R 72.863, F -83.081)]  [G loss: 81.196] \n",
      "4889 [D loss: (-5.171)(R 69.899, F -80.242)]  [G loss: 80.636] \n",
      "4890 [D loss: (-7.320)(R 69.665, F -84.305)]  [G loss: 78.811] \n",
      "4890 [D loss: (-3.937)(R 70.455, F -78.329)]  [G loss: 80.871] \n",
      "4891 [D loss: (-4.878)(R 71.542, F -81.298)]  [G loss: 81.367] \n",
      "4891 [D loss: (-8.413)(R 69.975, F -86.802)]  [G loss: 81.310] \n",
      "4892 [D loss: (-6.238)(R 70.395, F -82.871)]  [G loss: 80.709] \n",
      "4892 [D loss: (-5.905)(R 70.804, F -82.614)]  [G loss: 82.218] \n",
      "4893 [D loss: (-3.335)(R 72.787, F -79.456)]  [G loss: 80.074] \n",
      "4893 [D loss: (-6.774)(R 70.631, F -84.179)]  [G loss: 80.576] \n",
      "4894 [D loss: (-1.597)(R 68.195, F -71.388)]  [G loss: 81.493] \n",
      "4894 [D loss: (-5.911)(R 67.325, F -79.146)]  [G loss: 79.914] \n",
      "4895 [D loss: (-2.753)(R 69.691, F -75.196)]  [G loss: 81.834] \n",
      "4895 [D loss: (-5.322)(R 71.818, F -82.462)]  [G loss: 79.665] \n",
      "4896 [D loss: (-6.189)(R 67.287, F -79.665)]  [G loss: 79.490] \n",
      "4896 [D loss: (-5.213)(R 71.321, F -81.746)]  [G loss: 80.002] \n",
      "4897 [D loss: (-4.973)(R 71.028, F -80.973)]  [G loss: 78.791] \n",
      "4897 [D loss: (-3.913)(R 71.176, F -79.003)]  [G loss: 81.029] \n",
      "4898 [D loss: (-5.224)(R 71.336, F -81.785)]  [G loss: 79.407] \n",
      "4898 [D loss: (-2.188)(R 73.459, F -77.834)]  [G loss: 79.017] \n",
      "4899 [D loss: (-1.901)(R 72.326, F -76.127)]  [G loss: 80.638] \n",
      "4899 [D loss: (-6.416)(R 68.749, F -81.582)]  [G loss: 78.971] \n",
      "4900 [D loss: (-2.131)(R 70.553, F -74.815)]  [G loss: 79.565] \n",
      "4900 [D loss: (-7.347)(R 70.488, F -85.182)]  [G loss: 78.957] \n",
      "4901 [D loss: (-2.729)(R 69.195, F -74.653)]  [G loss: 80.862] \n",
      "4901 [D loss: (-1.777)(R 75.056, F -78.609)]  [G loss: 78.160] \n",
      "4902 [D loss: (-1.201)(R 71.132, F -73.534)]  [G loss: 80.334] \n",
      "4902 [D loss: (-8.676)(R 67.071, F -84.422)]  [G loss: 80.626] \n",
      "4903 [D loss: (-4.003)(R 71.245, F -79.250)]  [G loss: 79.032] \n",
      "4903 [D loss: (-3.080)(R 69.630, F -75.789)]  [G loss: 81.555] \n",
      "4904 [D loss: (-2.914)(R 70.208, F -76.036)]  [G loss: 77.856] \n",
      "4904 [D loss: (-5.244)(R 72.001, F -82.488)]  [G loss: 80.160] \n",
      "4905 [D loss: (-2.384)(R 70.917, F -75.686)]  [G loss: 80.929] \n",
      "4905 [D loss: (-2.218)(R 71.625, F -76.060)]  [G loss: 79.970] \n",
      "4906 [D loss: (-4.940)(R 72.439, F -82.318)]  [G loss: 78.750] \n",
      "4906 [D loss: (-1.375)(R 70.266, F -73.015)]  [G loss: 79.398] \n",
      "4907 [D loss: (-3.688)(R 67.349, F -74.725)]  [G loss: 79.497] \n",
      "4907 [D loss: (-4.359)(R 69.198, F -77.916)]  [G loss: 79.446] \n",
      "4908 [D loss: (-2.663)(R 68.856, F -74.183)]  [G loss: 78.981] \n",
      "4908 [D loss: (-6.560)(R 68.775, F -81.895)]  [G loss: 79.493] \n",
      "4909 [D loss: (-9.707)(R 66.484, F -85.897)]  [G loss: 80.338] \n",
      "4909 [D loss: (-6.312)(R 69.180, F -81.804)]  [G loss: 79.370] \n",
      "4910 [D loss: (-3.565)(R 74.308, F -81.438)]  [G loss: 77.306] \n",
      "4910 [D loss: (-5.541)(R 70.195, F -81.276)]  [G loss: 79.870] \n",
      "4911 [D loss: (-2.645)(R 71.633, F -76.923)]  [G loss: 81.681] \n",
      "4911 [D loss: (-5.883)(R 67.827, F -79.592)]  [G loss: 79.728] \n",
      "4912 [D loss: (-1.828)(R 73.240, F -76.896)]  [G loss: 78.166] \n",
      "4912 [D loss: (-4.696)(R 69.512, F -78.904)]  [G loss: 80.497] \n",
      "4913 [D loss: (-3.315)(R 70.472, F -77.102)]  [G loss: 80.614] \n",
      "4913 [D loss: (-3.135)(R 68.829, F -75.100)]  [G loss: 79.936] \n",
      "4914 [D loss: (-7.053)(R 71.858, F -85.965)]  [G loss: 77.454] \n",
      "4914 [D loss: (-6.672)(R 69.120, F -82.464)]  [G loss: 79.164] \n",
      "4915 [D loss: (-5.937)(R 70.569, F -82.442)]  [G loss: 78.529] \n",
      "4915 [D loss: (-2.732)(R 68.352, F -73.817)]  [G loss: 78.512] \n",
      "4916 [D loss: (-4.117)(R 70.411, F -78.645)]  [G loss: 79.572] \n",
      "4916 [D loss: (-3.667)(R 69.436, F -76.771)]  [G loss: 78.539] \n",
      "4917 [D loss: (-4.591)(R 67.342, F -76.524)]  [G loss: 78.105] \n",
      "4917 [D loss: (-6.186)(R 71.528, F -83.900)]  [G loss: 79.958] \n",
      "4918 [D loss: (-1.459)(R 69.346, F -72.264)]  [G loss: 79.640] \n",
      "4918 [D loss: (-5.466)(R 71.225, F -82.158)]  [G loss: 79.379] \n",
      "4919 [D loss: (-2.737)(R 68.231, F -73.706)]  [G loss: 77.109] \n",
      "4919 [D loss: (-4.861)(R 70.358, F -80.080)]  [G loss: 77.453] \n",
      "4920 [D loss: (-0.089)(R 68.320, F -68.498)]  [G loss: 76.951] \n",
      "4920 [D loss: (-4.197)(R 70.323, F -78.717)]  [G loss: 79.824] \n",
      "4921 [D loss: (-1.178)(R 70.926, F -73.282)]  [G loss: 77.365] \n",
      "4921 [D loss: (-3.001)(R 67.573, F -73.575)]  [G loss: 77.925] \n",
      "4922 [D loss: (-4.207)(R 68.873, F -77.287)]  [G loss: 79.344] \n",
      "4922 [D loss: (-4.902)(R 72.765, F -82.568)]  [G loss: 77.820] \n",
      "4923 [D loss: (-7.850)(R 67.809, F -83.509)]  [G loss: 77.728] \n",
      "4923 [D loss: (-4.064)(R 67.066, F -75.194)]  [G loss: 79.376] \n",
      "4924 [D loss: (-4.087)(R 71.455, F -79.630)]  [G loss: 78.422] \n",
      "4924 [D loss: (-4.405)(R 71.527, F -80.336)]  [G loss: 78.177] \n",
      "4925 [D loss: (-6.674)(R 71.645, F -84.993)]  [G loss: 78.891] \n",
      "4925 [D loss: (-2.578)(R 71.790, F -76.947)]  [G loss: 81.138] \n",
      "4926 [D loss: (-4.782)(R 71.841, F -81.406)]  [G loss: 77.651] \n",
      "4926 [D loss: (-5.684)(R 68.487, F -79.854)]  [G loss: 79.010] \n",
      "4927 [D loss: (-4.161)(R 71.423, F -79.745)]  [G loss: 77.932] \n",
      "4927 [D loss: (-4.743)(R 72.037, F -81.523)]  [G loss: 78.645] \n",
      "4928 [D loss: (-4.061)(R 71.405, F -79.528)]  [G loss: 78.931] \n",
      "4928 [D loss: (-0.338)(R 68.775, F -69.450)]  [G loss: 78.762] \n",
      "4929 [D loss: (-4.184)(R 69.505, F -77.874)]  [G loss: 77.556] \n",
      "4929 [D loss: (-6.755)(R 70.048, F -83.557)]  [G loss: 75.448] \n",
      "4930 [D loss: (-4.364)(R 69.075, F -77.802)]  [G loss: 77.914] \n",
      "4930 [D loss: (-2.942)(R 71.689, F -77.573)]  [G loss: 80.111] \n",
      "4931 [D loss: (-0.566)(R 71.288, F -72.420)]  [G loss: 77.625] \n",
      "4931 [D loss: (-7.534)(R 71.577, F -86.644)]  [G loss: 79.889] \n",
      "4932 [D loss: (-2.516)(R 66.762, F -71.795)]  [G loss: 78.152] \n",
      "4932 [D loss: (-0.598)(R 72.112, F -73.308)]  [G loss: 77.896] \n",
      "4933 [D loss: (-7.337)(R 69.321, F -83.996)]  [G loss: 78.876] \n",
      "4933 [D loss: (-4.819)(R 67.570, F -77.208)]  [G loss: 77.775] \n",
      "4934 [D loss: (-6.674)(R 66.752, F -80.101)]  [G loss: 76.965] \n",
      "4934 [D loss: (-0.920)(R 71.296, F -73.137)]  [G loss: 76.354] \n",
      "4935 [D loss: (-3.194)(R 65.959, F -72.346)]  [G loss: 77.055] \n",
      "4935 [D loss: (-2.756)(R 73.099, F -78.611)]  [G loss: 77.051] \n",
      "4936 [D loss: (-3.606)(R 72.095, F -79.306)]  [G loss: 78.153] \n",
      "4936 [D loss: (-3.534)(R 70.397, F -77.465)]  [G loss: 78.844] \n",
      "4937 [D loss: (-1.155)(R 72.780, F -75.091)]  [G loss: 78.347] \n",
      "4937 [D loss: (-4.589)(R 69.945, F -79.123)]  [G loss: 78.677] \n",
      "4938 [D loss: (-4.565)(R 70.087, F -79.217)]  [G loss: 80.245] \n",
      "4938 [D loss: (-6.534)(R 65.708, F -78.776)]  [G loss: 77.091] \n",
      "4939 [D loss: (-4.163)(R 68.612, F -76.939)]  [G loss: 78.402] \n",
      "4939 [D loss: (-4.302)(R 68.497, F -77.102)]  [G loss: 77.350] \n",
      "4940 [D loss: (-1.598)(R 67.083, F -70.280)]  [G loss: 77.298] \n",
      "4940 [D loss: (-8.362)(R 67.398, F -84.122)]  [G loss: 75.678] \n",
      "4941 [D loss: (-5.113)(R 67.934, F -78.161)]  [G loss: 77.992] \n",
      "4941 [D loss: (-4.411)(R 72.612, F -81.435)]  [G loss: 76.643] \n",
      "4942 [D loss: (-5.133)(R 70.727, F -80.994)]  [G loss: 77.950] \n",
      "4942 [D loss: (-0.392)(R 69.369, F -70.152)]  [G loss: 78.228] \n",
      "4943 [D loss: (-3.861)(R 72.522, F -80.243)]  [G loss: 76.915] \n",
      "4943 [D loss: (-2.197)(R 70.285, F -74.679)]  [G loss: 75.334] \n",
      "4944 [D loss: (-5.783)(R 70.745, F -82.311)]  [G loss: 77.981] \n",
      "4944 [D loss: (-0.460)(R 67.068, F -67.988)]  [G loss: 76.875] \n",
      "4945 [D loss: (-1.883)(R 70.407, F -74.172)]  [G loss: 76.685] \n",
      "4945 [D loss: (-4.254)(R 69.403, F -77.910)]  [G loss: 77.076] \n",
      "4946 [D loss: (-5.393)(R 69.435, F -80.221)]  [G loss: 79.386] \n",
      "4946 [D loss: (-1.491)(R 68.778, F -71.759)]  [G loss: 77.015] \n",
      "4947 [D loss: (-3.955)(R 71.618, F -79.528)]  [G loss: 75.721] \n",
      "4947 [D loss: (-1.712)(R 72.233, F -75.658)]  [G loss: 74.515] \n",
      "4948 [D loss: (-3.214)(R 69.511, F -75.938)]  [G loss: 75.673] \n",
      "4948 [D loss: (-2.884)(R 68.313, F -74.081)]  [G loss: 76.830] \n",
      "4949 [D loss: (-2.220)(R 70.914, F -75.355)]  [G loss: 75.591] \n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"深層学習_最終課題_upsampling_高解像度\")\n",
    "\n",
    "d_history = []\n",
    "g_history = []\n",
    "save_fig_path = os.path.abspath(\"gan_pokemon_upsample_2\")\n",
    "save_model_path = os.path.abspath(\"model_upsample_2\")\n",
    "\n",
    "\n",
    "def train(data, n_epochs=6000, batch_size=256, crip_threshold=0.01):\n",
    "    \n",
    "    batch_per_epoch = int(data.shape[0] / batch_size)\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = -np.ones((batch_size, 1))\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        for i in range(batch_per_epoch):\n",
    "            \n",
    "            for j in range(5):\n",
    "                idx = np.random.randint(0, len(data[0]), batch_size)\n",
    "                true_imgs = data[idx]\n",
    "\n",
    "                z = np.random.normal(0, 1, (batch_size, z_dims))\n",
    "                gen_imgs = gen.predict(z)\n",
    "\n",
    "                d_loss_real = disc.train_on_batch(true_imgs, valid)\n",
    "                d_loss_fake = disc.train_on_batch(gen_imgs, fake)\n",
    "                d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "\n",
    "                for l in disc.layers:\n",
    "                    weights = l.get_weights()\n",
    "                    weights = [np.clip(w, -crip_threshold, crip_threshold) for w in weights]\n",
    "                    l.set_weights(weights)\n",
    "            \n",
    "\n",
    "            z = np.random.normal(0, 1, (batch_size, z_dims))\n",
    "            g_loss = gan.train_on_batch(z, valid)\n",
    "            g_history.append(g_loss)\n",
    "            \n",
    "            print (\"%d [D loss: (%.3f)(R %.3f, F %.3f)]  [G loss: %.3f] \" % (epoch, d_loss, d_loss_real, d_loss_fake, g_loss))\n",
    "            \n",
    "            wandb.log({\"d_loss\": d_loss,\n",
    "                       \"g_loss\": g_loss})\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            sample_images(epoch)\n",
    "            save_model(epoch)\n",
    "        \n",
    "        K.set_value(disc.optimizer.learning_rate, K.get_value(disc.optimizer.learning_rate)*0.999)\n",
    "        K.set_value(gan.optimizer.learning_rate, K.get_value(gan.optimizer.learning_rate)*0.999)\n",
    "    \n",
    "def sample_images(epoch):\n",
    "    r, c = 5, 5\n",
    "    z = np.random.normal(0, 1, (25, z_dims))\n",
    "    gen_imgs = gen.predict(z)\n",
    "\n",
    "\n",
    "    gen_imgs = 0.5 * (gen_imgs + 1)\n",
    "    gen_imgs = np.clip(gen_imgs, 0, 1)\n",
    "\n",
    "    fig, axs = plt.subplots(r, c, figsize=(15,15))\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(np.squeeze(gen_imgs[cnt, :,:,:]))\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(os.path.join(save_fig_path, f\"epoch_{epoch}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def save_model(epoch):\n",
    "    save_folder = os.path.join(save_model_path, f\"model_epoch{epoch}\")\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    disc.save(os.path.join(save_folder, 'discriminator.h5'))\n",
    "    gen.save(os.path.join(save_folder, 'generator.h5'))\n",
    "    gan.save(os.path.join(save_folder, 'gan.h5'))\n",
    "    \n",
    "train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
