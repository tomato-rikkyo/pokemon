{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow 2.3でメモリを指定及び節約して使うためのおまじない。\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(physical_devices[9], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[9], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/Anaconda3/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, smart_resize\n",
    "\n",
    "INPUT_SHAPE = (256, 256)\n",
    "\n",
    "data = np.array([])\n",
    "train_path = os.path.abspath(\"cropped_images\")\n",
    "images = glob(os.path.join(train_path, \"*.*\"))\n",
    "\n",
    "data = np.stack([img_to_array(load_img(img).resize(INPUT_SHAPE)) for img in images]) / 127.5 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout, ZeroPadding2D, UpSampling2D\n",
    "from keras.layers import Layer\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from keras.utils import plot_model\n",
    "from keras.initializers import RandomNormal\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_conv_filters = [16,32,64,128,256,512]\n",
    "discriminator_conv_kernel_size = [5,5,5,5,5,5]\n",
    "discriminator_conv_strides = [2,2,2,2,2,2]\n",
    "\n",
    "initial_layer_shape = (4, 4, 512)\n",
    "generator_conv_filters = [256,128,64,32,16,3]\n",
    "generator_conv_kernel_size = [5,5,5,5,5,5]\n",
    "generator_conv_strides = [2,2,2,2,2,2]\n",
    "discriminator_input_shape = (256, 256, 3)\n",
    "\n",
    "\n",
    "def generator(z_dim, initial_layer_shape, generator_conv_filters, generator_conv_kernel_size, generator_conv_strides):\n",
    "    generator_input = Input(shape=(z_dim,))\n",
    "    x = generator_input\n",
    "    x = Dense(np.prod(initial_layer_shape), kernel_initializer=\"he_normal\")(x)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = Reshape(initial_layer_shape)(x)\n",
    "    for i in range(6):\n",
    "        x = Conv2DTranspose(\n",
    "            filters=generator_conv_filters[i],\n",
    "            kernel_size=generator_conv_kernel_size[i],\n",
    "            padding=\"same\",\n",
    "            strides=generator_conv_strides[i],\n",
    "            kernel_initializer=\"he_normal\")(x)\n",
    "        if i < 6 - 1:\n",
    "            x = BatchNormalization(momentum=0.9)(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "        else:\n",
    "            generator_output = Activation(\"tanh\")(x)\n",
    "            \n",
    "    return Model(generator_input, generator_output)\n",
    "\n",
    "def discriminator(discriminator_input_shape, discriminator_conv_filters, discriminator_conv_kernel_size, discriminator_conv_strides):\n",
    "    discriminator_input = Input(discriminator_input_shape)\n",
    "    x = discriminator_input\n",
    "    \n",
    "    for i in range(6):\n",
    "        x = Conv2D(filters=discriminator_conv_filters[i],\n",
    "                   kernel_size=discriminator_conv_kernel_size[i],\n",
    "                   padding=\"same\",\n",
    "                   strides=discriminator_conv_strides[i],\n",
    "                   kernel_initializer=\"he_normal\")(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    discriminator_output = Dense(1, activation=None, kernel_initializer=\"he_normal\")(x)\n",
    "    \n",
    "    return Model(discriminator_input, discriminator_output)\n",
    "\n",
    "def wasserstein(y_true, y_pred):\n",
    "        return -K.mean(y_true * y_pred)\n",
    "\n",
    "\n",
    "class RandomWeightedAverage(Layer):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((self.batch_size, 1, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
    "    \n",
    "def gradient_penalty_loss(y_true, y_pred, interpolated_samples):\n",
    "    g = tf.Graph()\n",
    "    with g.as_default():\n",
    "        gradients = K.gradients(y_pred, interpolated_samples)[0]\n",
    "\n",
    "        gradients_sqr = K.square(gradients)\n",
    "        gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                                  axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "        gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "    return K.mean(gradient_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dims = 100\n",
    "\n",
    "disc = discriminator(discriminator_input_shape, discriminator_conv_filters, discriminator_conv_kernel_size, discriminator_conv_strides)\n",
    "\n",
    "disc.compile(optimizer=RMSprop(lr=0.00005),\n",
    "             loss=wasserstein)\n",
    "\n",
    "disc.trainable = False\n",
    "\n",
    "gen = generator(z_dims, initial_layer_shape, generator_conv_filters, generator_conv_kernel_size, generator_conv_strides)\n",
    "\n",
    "gan_input = Input(shape=(z_dims,))\n",
    "gan_output = disc(gen(gan_input))\n",
    "gan = Model(gan_input, gan_output)\n",
    "gan.compile(optimizer=RMSprop(lr=0.00005),\n",
    "            loss=wasserstein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "functional_3 (Functional)    (None, 256, 256, 3)       5228643   \n",
      "_________________________________________________________________\n",
      "functional_1 (Functional)    (None, 1)                 4375201   \n",
      "=================================================================\n",
      "Total params: 9,603,844\n",
      "Trainable params: 5,211,267\n",
      "Non-trainable params: 4,392,577\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtomato-ai\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.12<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">colorful-sky-10</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/tomato-ai/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92_%E6%9C%80%E7%B5%82%E8%AA%B2%E9%A1%8C\" target=\"_blank\">https://wandb.ai/tomato-ai/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92_%E6%9C%80%E7%B5%82%E8%AA%B2%E9%A1%8C</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/tomato-ai/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92_%E6%9C%80%E7%B5%82%E8%AA%B2%E9%A1%8C/runs/1wpxh3hk\" target=\"_blank\">https://wandb.ai/tomato-ai/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92_%E6%9C%80%E7%B5%82%E8%AA%B2%E9%A1%8C/runs/1wpxh3hk</a><br/>\n",
       "                Run data is saved locally in <code>/workdir/homework/deep_NN/pokemon/wandb/run-20210119_014223-1wpxh3hk</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: (0.000)(R -0.000, F 0.001)]  [G loss: -0.000] \n",
      "0 [D loss: (0.000)(R -0.001, F 0.001)]  [G loss: -0.000] \n",
      "0 [D loss: (0.000)(R -0.001, F 0.001)]  [G loss: -0.000] \n",
      "1 [D loss: (0.000)(R -0.001, F 0.001)]  [G loss: -0.000] \n",
      "1 [D loss: (0.000)(R -0.001, F 0.001)]  [G loss: -0.000] \n",
      "1 [D loss: (0.000)(R -0.001, F 0.001)]  [G loss: -0.000] \n",
      "2 [D loss: (0.000)(R -0.001, F 0.001)]  [G loss: -0.000] \n",
      "2 [D loss: (0.000)(R -0.001, F 0.001)]  [G loss: -0.000] \n",
      "2 [D loss: (0.000)(R -0.001, F 0.001)]  [G loss: -0.000] \n",
      "3 [D loss: (0.000)(R -0.001, F 0.001)]  [G loss: -0.000] \n",
      "3 [D loss: (0.000)(R -0.001, F 0.001)]  [G loss: -0.000] \n",
      "3 [D loss: (0.000)(R -0.001, F 0.001)]  [G loss: -0.000] \n",
      "4 [D loss: (0.000)(R -0.001, F 0.001)]  [G loss: -0.000] \n",
      "4 [D loss: (0.000)(R -0.001, F 0.001)]  [G loss: -0.000] \n",
      "4 [D loss: (0.000)(R -0.001, F 0.001)]  [G loss: -0.000] \n",
      "5 [D loss: (0.000)(R -0.001, F 0.001)]  [G loss: -0.000] \n",
      "5 [D loss: (0.000)(R -0.001, F 0.001)]  [G loss: -0.000] \n",
      "5 [D loss: (0.000)(R -0.001, F 0.001)]  [G loss: -0.000] \n",
      "6 [D loss: (0.000)(R -0.001, F 0.001)]  [G loss: -0.000] \n",
      "6 [D loss: (0.000)(R -0.001, F 0.001)]  [G loss: -0.000] \n",
      "6 [D loss: (-0.000)(R -0.001, F 0.001)]  [G loss: -0.000] \n",
      "7 [D loss: (-0.000)(R -0.001, F 0.001)]  [G loss: -0.000] \n",
      "7 [D loss: (-0.000)(R -0.001, F 0.001)]  [G loss: -0.000] \n",
      "7 [D loss: (-0.000)(R -0.001, F 0.001)]  [G loss: -0.000] \n",
      "8 [D loss: (-0.001)(R -0.002, F 0.001)]  [G loss: 0.000] \n",
      "8 [D loss: (-0.002)(R -0.003, F 0.000)]  [G loss: 0.001] \n",
      "8 [D loss: (-0.003)(R -0.007, F -0.000)]  [G loss: 0.002] \n",
      "9 [D loss: (-0.008)(R -0.014, F -0.002)]  [G loss: 0.004] \n",
      "9 [D loss: (-0.019)(R -0.033, F -0.004)]  [G loss: 0.007] \n",
      "9 [D loss: (-0.046)(R -0.084, F -0.009)]  [G loss: 0.014] \n",
      "10 [D loss: (-0.108)(R -0.199, F -0.016)]  [G loss: 0.023] \n",
      "10 [D loss: (-0.222)(R -0.419, F -0.026)]  [G loss: 0.036] \n",
      "10 [D loss: (-0.447)(R -0.856, F -0.039)]  [G loss: 0.053] \n",
      "11 [D loss: (-0.794)(R -1.535, F -0.052)]  [G loss: 0.072] \n",
      "11 [D loss: (-1.387)(R -2.708, F -0.066)]  [G loss: 0.096] \n",
      "11 [D loss: (-2.290)(R -4.496, F -0.083)]  [G loss: 0.131] \n",
      "12 [D loss: (-3.690)(R -7.283, F -0.097)]  [G loss: 0.178] \n",
      "12 [D loss: (-5.275)(R -10.493, F -0.057)]  [G loss: 0.239] \n",
      "12 [D loss: (-7.332)(R -14.794, F 0.130)]  [G loss: 0.320] \n",
      "13 [D loss: (-9.931)(R -20.241, F 0.380)]  [G loss: 0.405] \n",
      "13 [D loss: (-13.758)(R -28.393, F 0.877)]  [G loss: 0.491] \n",
      "13 [D loss: (-17.788)(R -37.298, F 1.722)]  [G loss: 0.553] \n",
      "14 [D loss: (-21.810)(R -46.597, F 2.977)]  [G loss: 0.575] \n",
      "14 [D loss: (-26.590)(R -57.914, F 4.734)]  [G loss: 0.537] \n",
      "14 [D loss: (-31.882)(R -71.473, F 7.709)]  [G loss: 0.282] \n",
      "15 [D loss: (-37.974)(R -87.889, F 11.940)]  [G loss: -0.409] \n",
      "15 [D loss: (-46.119)(R -109.447, F 17.209)]  [G loss: -1.515] \n",
      "15 [D loss: (-54.606)(R -131.949, F 22.737)]  [G loss: -2.921] \n",
      "16 [D loss: (-59.764)(R -148.369, F 28.841)]  [G loss: -4.595] \n",
      "16 [D loss: (-78.288)(R -193.449, F 36.872)]  [G loss: -6.875] \n",
      "16 [D loss: (-83.362)(R -211.454, F 44.731)]  [G loss: -9.442] \n",
      "17 [D loss: (-95.613)(R -245.302, F 54.076)]  [G loss: -12.554] \n",
      "17 [D loss: (-110.055)(R -284.629, F 64.518)]  [G loss: -15.988] \n",
      "17 [D loss: (-115.670)(R -306.643, F 75.304)]  [G loss: -19.914] \n",
      "18 [D loss: (-129.526)(R -345.397, F 86.345)]  [G loss: -24.061] \n",
      "18 [D loss: (-141.371)(R -382.331, F 99.589)]  [G loss: -28.869] \n",
      "18 [D loss: (-157.757)(R -430.948, F 115.434)]  [G loss: -34.476] \n",
      "19 [D loss: (-177.851)(R -487.262, F 131.560)]  [G loss: -40.792] \n",
      "19 [D loss: (-182.845)(R -517.149, F 151.459)]  [G loss: -48.677] \n",
      "19 [D loss: (-205.880)(R -581.817, F 170.056)]  [G loss: -57.062] \n",
      "20 [D loss: (-230.830)(R -656.125, F 194.464)]  [G loss: -66.364] \n",
      "20 [D loss: (-243.593)(R -704.468, F 217.281)]  [G loss: -76.326] \n",
      "20 [D loss: (-282.436)(R -808.309, F 243.438)]  [G loss: -89.074] \n",
      "21 [D loss: (-268.439)(R -808.370, F 271.492)]  [G loss: -102.191] \n",
      "21 [D loss: (-299.555)(R -902.014, F 302.904)]  [G loss: -117.617] \n",
      "21 [D loss: (-323.039)(R -985.500, F 339.422)]  [G loss: -132.755] \n",
      "22 [D loss: (-345.551)(R -1061.604, F 370.503)]  [G loss: -149.147] \n",
      "22 [D loss: (-395.586)(R -1197.966, F 406.795)]  [G loss: -166.190] \n",
      "22 [D loss: (-431.296)(R -1311.617, F 449.025)]  [G loss: -186.827] \n",
      "23 [D loss: (-392.529)(R -1271.174, F 486.116)]  [G loss: -209.216] \n",
      "23 [D loss: (-434.877)(R -1393.758, F 524.004)]  [G loss: -226.731] \n",
      "23 [D loss: (-499.137)(R -1575.136, F 576.862)]  [G loss: -250.173] \n",
      "24 [D loss: (-538.586)(R -1696.574, F 619.402)]  [G loss: -276.735] \n",
      "24 [D loss: (-537.208)(R -1741.982, F 667.566)]  [G loss: -301.779] \n",
      "24 [D loss: (-581.527)(R -1881.651, F 718.596)]  [G loss: -327.392] \n",
      "25 [D loss: (-586.965)(R -1938.067, F 764.137)]  [G loss: -354.214] \n",
      "25 [D loss: (-667.962)(R -2160.303, F 824.379)]  [G loss: -383.765] \n",
      "25 [D loss: (-678.400)(R -2222.509, F 865.709)]  [G loss: -413.620] \n",
      "26 [D loss: (-721.544)(R -2366.032, F 922.944)]  [G loss: -441.752] \n",
      "26 [D loss: (-750.779)(R -2495.863, F 994.304)]  [G loss: -480.834] \n",
      "26 [D loss: (-747.802)(R -2531.627, F 1036.024)]  [G loss: -511.752] \n",
      "27 [D loss: (-894.186)(R -2887.976, F 1099.603)]  [G loss: -547.691] \n",
      "27 [D loss: (-827.161)(R -2811.999, F 1157.676)]  [G loss: -581.327] \n",
      "27 [D loss: (-995.087)(R -3196.551, F 1206.376)]  [G loss: -629.173] \n",
      "28 [D loss: (-990.689)(R -3273.837, F 1292.458)]  [G loss: -674.661] \n",
      "28 [D loss: (-1074.296)(R -3515.403, F 1366.811)]  [G loss: -718.753] \n",
      "28 [D loss: (-1087.180)(R -3601.826, F 1427.466)]  [G loss: -769.220] \n",
      "29 [D loss: (-1048.433)(R -3596.654, F 1499.788)]  [G loss: -813.911] \n",
      "29 [D loss: (-1133.807)(R -3872.136, F 1604.521)]  [G loss: -864.657] \n",
      "29 [D loss: (-1294.831)(R -4286.797, F 1697.134)]  [G loss: -925.652] \n",
      "30 [D loss: (-1305.324)(R -4361.980, F 1751.333)]  [G loss: -977.945] \n",
      "30 [D loss: (-1432.686)(R -4730.884, F 1865.512)]  [G loss: -1041.334] \n",
      "30 [D loss: (-1442.281)(R -4815.154, F 1930.593)]  [G loss: -1100.101] \n",
      "31 [D loss: (-1474.640)(R -4961.851, F 2012.570)]  [G loss: -1165.225] \n",
      "31 [D loss: (-1532.468)(R -5226.714, F 2161.778)]  [G loss: -1235.526] \n",
      "31 [D loss: (-1603.776)(R -5476.642, F 2269.089)]  [G loss: -1303.238] \n",
      "32 [D loss: (-1729.844)(R -5812.880, F 2353.192)]  [G loss: -1379.307] \n",
      "32 [D loss: (-1722.920)(R -5950.585, F 2504.745)]  [G loss: -1443.556] \n",
      "32 [D loss: (-1922.346)(R -6455.587, F 2610.895)]  [G loss: -1527.364] \n",
      "33 [D loss: (-1891.621)(R -6531.483, F 2748.241)]  [G loss: -1598.143] \n",
      "33 [D loss: (-1985.594)(R -6779.580, F 2808.392)]  [G loss: -1689.593] \n",
      "33 [D loss: (-2137.629)(R -7251.475, F 2976.217)]  [G loss: -1757.923] \n",
      "34 [D loss: (-2206.578)(R -7432.805, F 3019.650)]  [G loss: -1863.282] \n",
      "34 [D loss: (-2226.527)(R -7667.304, F 3214.249)]  [G loss: -1966.368] \n",
      "34 [D loss: (-2233.473)(R -7797.146, F 3330.200)]  [G loss: -2046.125] \n",
      "35 [D loss: (-2329.239)(R -8123.275, F 3464.797)]  [G loss: -2159.009] \n",
      "35 [D loss: (-2493.242)(R -8643.855, F 3657.371)]  [G loss: -2265.901] \n",
      "35 [D loss: (-2662.423)(R -9086.383, F 3761.536)]  [G loss: -2353.667] \n",
      "36 [D loss: (-2938.002)(R -9675.449, F 3799.446)]  [G loss: -2477.746] \n",
      "36 [D loss: (-2671.636)(R -9306.686, F 3963.413)]  [G loss: -2596.698] \n",
      "36 [D loss: (-2845.651)(R -9701.547, F 4010.244)]  [G loss: -2693.483] \n",
      "37 [D loss: (-2861.550)(R -10014.613, F 4291.513)]  [G loss: -2817.917] \n",
      "37 [D loss: (-3099.501)(R -10570.576, F 4371.575)]  [G loss: -2951.204] \n",
      "37 [D loss: (-3153.086)(R -10986.360, F 4680.189)]  [G loss: -3089.284] \n",
      "38 [D loss: (-3357.252)(R -11488.498, F 4773.995)]  [G loss: -3246.671] \n",
      "38 [D loss: (-3487.441)(R -11895.638, F 4920.755)]  [G loss: -3389.687] \n",
      "38 [D loss: (-3882.918)(R -12895.007, F 5129.171)]  [G loss: -3535.896] \n",
      "39 [D loss: (-3893.134)(R -13064.526, F 5278.259)]  [G loss: -3729.866] \n",
      "39 [D loss: (-3812.404)(R -13160.320, F 5535.512)]  [G loss: -3868.451] \n",
      "39 [D loss: (-4140.360)(R -14154.703, F 5873.983)]  [G loss: -4045.562] \n",
      "40 [D loss: (-3730.022)(R -13491.289, F 6031.245)]  [G loss: -4217.538] \n",
      "40 [D loss: (-3971.467)(R -14263.342, F 6320.409)]  [G loss: -4381.849] \n",
      "40 [D loss: (-4397.614)(R -15279.139, F 6483.910)]  [G loss: -4624.901] \n",
      "41 [D loss: (-4160.199)(R -15030.060, F 6709.662)]  [G loss: -4804.466] \n",
      "41 [D loss: (-4376.616)(R -15592.645, F 6839.413)]  [G loss: -5010.386] \n",
      "41 [D loss: (-3959.573)(R -15111.756, F 7192.609)]  [G loss: -5189.212] \n",
      "42 [D loss: (-4123.145)(R -15644.876, F 7398.586)]  [G loss: -5335.845] \n",
      "42 [D loss: (-4944.073)(R -17657.264, F 7769.117)]  [G loss: -5566.887] \n",
      "42 [D loss: (-4844.471)(R -17737.428, F 8048.485)]  [G loss: -5796.622] \n",
      "43 [D loss: (-4879.615)(R -17918.857, F 8159.628)]  [G loss: -6057.558] \n",
      "43 [D loss: (-5004.288)(R -18314.994, F 8306.419)]  [G loss: -6267.433] \n",
      "43 [D loss: (-5039.700)(R -18892.672, F 8813.272)]  [G loss: -6570.495] \n",
      "44 [D loss: (-5476.006)(R -20066.023, F 9114.012)]  [G loss: -6817.365] \n",
      "44 [D loss: (-5262.599)(R -19573.588, F 9048.391)]  [G loss: -7074.274] \n",
      "44 [D loss: (-5255.658)(R -20184.777, F 9673.461)]  [G loss: -7333.019] \n",
      "45 [D loss: (-5368.729)(R -20661.744, F 9924.285)]  [G loss: -7598.115] \n",
      "45 [D loss: (-5647.609)(R -21529.934, F 10234.715)]  [G loss: -7872.621] \n",
      "45 [D loss: (-5918.241)(R -22456.270, F 10619.787)]  [G loss: -8124.614] \n",
      "46 [D loss: (-5486.591)(R -21626.412, F 10653.230)]  [G loss: -8449.276] \n",
      "46 [D loss: (-5661.374)(R -22354.959, F 11032.212)]  [G loss: -8756.289] \n",
      "46 [D loss: (-6327.630)(R -24450.514, F 11795.254)]  [G loss: -9048.444] \n",
      "47 [D loss: (-6045.834)(R -24016.049, F 11924.380)]  [G loss: -9358.371] \n",
      "47 [D loss: (-6094.910)(R -24527.514, F 12337.694)]  [G loss: -9684.820] \n",
      "47 [D loss: (-6842.968)(R -26290.766, F 12604.829)]  [G loss: -10035.899] \n",
      "48 [D loss: (-6018.434)(R -25366.533, F 13329.666)]  [G loss: -10299.762] \n",
      "48 [D loss: (-7046.093)(R -27544.920, F 13452.733)]  [G loss: -10717.133] \n",
      "48 [D loss: (-6808.968)(R -27551.158, F 13933.223)]  [G loss: -11001.798] \n",
      "49 [D loss: (-6600.885)(R -27416.996, F 14215.227)]  [G loss: -11409.627] \n",
      "49 [D loss: (-7212.123)(R -29362.721, F 14938.476)]  [G loss: -11750.848] \n",
      "49 [D loss: (-7378.652)(R -29873.998, F 15116.693)]  [G loss: -12056.626] \n",
      "50 [D loss: (-7002.864)(R -29366.801, F 15361.073)]  [G loss: -12374.369] \n",
      "50 [D loss: (-7253.675)(R -30214.135, F 15706.785)]  [G loss: -12882.557] \n",
      "50 [D loss: (-7570.924)(R -31420.463, F 16278.614)]  [G loss: -13189.415] \n",
      "51 [D loss: (-7865.469)(R -32095.859, F 16364.922)]  [G loss: -13633.568] \n",
      "51 [D loss: (-7264.159)(R -31447.383, F 16919.064)]  [G loss: -13992.509] \n",
      "51 [D loss: (-7299.220)(R -31739.896, F 17141.457)]  [G loss: -14336.211] \n",
      "52 [D loss: (-7497.534)(R -32779.559, F 17784.490)]  [G loss: -14878.189] \n",
      "52 [D loss: (-7738.698)(R -33913.984, F 18436.588)]  [G loss: -15192.914] \n",
      "52 [D loss: (-7694.250)(R -34229.918, F 18841.418)]  [G loss: -15508.860] \n",
      "53 [D loss: (-8643.575)(R -36554.164, F 19267.014)]  [G loss: -16003.244] \n",
      "53 [D loss: (-8598.502)(R -36518.922, F 19321.918)]  [G loss: -16250.990] \n",
      "53 [D loss: (-8890.537)(R -37738.738, F 19957.664)]  [G loss: -16760.824] \n",
      "54 [D loss: (-8386.101)(R -36849.945, F 20077.744)]  [G loss: -17068.578] \n",
      "54 [D loss: (-8646.580)(R -37845.867, F 20552.707)]  [G loss: -17467.738] \n",
      "54 [D loss: (-8398.913)(R -38162.523, F 21364.697)]  [G loss: -17883.289] \n",
      "55 [D loss: (-7514.137)(R -36412.828, F 21384.555)]  [G loss: -18115.160] \n",
      "55 [D loss: (-8032.932)(R -38116.102, F 22050.238)]  [G loss: -18482.607] \n",
      "55 [D loss: (-8489.504)(R -39317.145, F 22338.137)]  [G loss: -18916.961] \n",
      "56 [D loss: (-8841.979)(R -40183.656, F 22499.699)]  [G loss: -19326.965] \n",
      "56 [D loss: (-8862.424)(R -41136.223, F 23411.375)]  [G loss: -19878.141] \n",
      "56 [D loss: (-8964.898)(R -41800.301, F 23870.504)]  [G loss: -20422.850] \n",
      "57 [D loss: (-8697.104)(R -41626.156, F 24231.949)]  [G loss: -20828.076] \n",
      "57 [D loss: (-7923.713)(R -40748.742, F 24901.316)]  [G loss: -21115.721] \n",
      "57 [D loss: (-8746.966)(R -42045.520, F 24551.588)]  [G loss: -21494.734] \n",
      "58 [D loss: (-9206.023)(R -44155.258, F 25743.211)]  [G loss: -22143.723] \n",
      "58 [D loss: (-8396.748)(R -43028.402, F 26234.906)]  [G loss: -22629.184] \n",
      "58 [D loss: (-9852.664)(R -46717.699, F 27012.371)]  [G loss: -23137.143] \n",
      "59 [D loss: (-10451.127)(R -48636.242, F 27733.988)]  [G loss: -23647.561] \n",
      "59 [D loss: (-9979.855)(R -47323.219, F 27363.508)]  [G loss: -23911.994] \n",
      "59 [D loss: (-9665.068)(R -47519.230, F 28189.094)]  [G loss: -24652.201] \n",
      "60 [D loss: (-9889.037)(R -48662.355, F 28884.281)]  [G loss: -25212.506] \n",
      "60 [D loss: (-9414.150)(R -48213.184, F 29384.883)]  [G loss: -25628.750] \n",
      "60 [D loss: (-10072.399)(R -50345.684, F 30200.885)]  [G loss: -26190.258] \n",
      "61 [D loss: (-9565.114)(R -49397.770, F 30267.541)]  [G loss: -26773.750] \n",
      "61 [D loss: (-9468.219)(R -49914.895, F 30978.457)]  [G loss: -27235.062] \n",
      "61 [D loss: (-10830.646)(R -53359.375, F 31698.082)]  [G loss: -27633.160] \n",
      "62 [D loss: (-10562.380)(R -52469.047, F 31344.287)]  [G loss: -28207.012] \n",
      "62 [D loss: (-10578.557)(R -53240.059, F 32082.945)]  [G loss: -28735.443] \n",
      "62 [D loss: (-9579.055)(R -52044.109, F 32886.000)]  [G loss: -29285.688] \n",
      "63 [D loss: (-10164.646)(R -53689.719, F 33360.426)]  [G loss: -29766.965] \n",
      "63 [D loss: (-10337.057)(R -54882.102, F 34207.988)]  [G loss: -30373.865] \n",
      "63 [D loss: (-9735.701)(R -53634.582, F 34163.180)]  [G loss: -30847.764] \n",
      "64 [D loss: (-9056.850)(R -53421.457, F 35307.758)]  [G loss: -31203.609] \n",
      "64 [D loss: (-10250.623)(R -56078.258, F 35577.012)]  [G loss: -31557.469] \n",
      "64 [D loss: (-10458.154)(R -57236.754, F 36320.445)]  [G loss: -32256.061] \n",
      "65 [D loss: (-10761.965)(R -58017.957, F 36494.027)]  [G loss: -32572.029] \n",
      "65 [D loss: (-10867.555)(R -58873.293, F 37138.184)]  [G loss: -33342.074] \n",
      "65 [D loss: (-12322.312)(R -62148.562, F 37503.938)]  [G loss: -33874.684] \n",
      "66 [D loss: (-11231.379)(R -61066.094, F 38603.336)]  [G loss: -34529.027] \n",
      "66 [D loss: (-10290.270)(R -59790.691, F 39210.152)]  [G loss: -34934.441] \n",
      "66 [D loss: (-9446.812)(R -59467.293, F 40573.668)]  [G loss: -35434.613] \n",
      "67 [D loss: (-9551.742)(R -59685.137, F 40581.652)]  [G loss: -35703.207] \n",
      "67 [D loss: (-10504.066)(R -61323.555, F 40315.422)]  [G loss: -36097.797] \n",
      "67 [D loss: (-11743.535)(R -64336.820, F 40849.750)]  [G loss: -36564.367] \n",
      "68 [D loss: (-12374.576)(R -65385.703, F 40636.551)]  [G loss: -37085.516] \n",
      "68 [D loss: (-11645.234)(R -65474.086, F 42183.617)]  [G loss: -37442.738] \n",
      "68 [D loss: (-10377.748)(R -63964.699, F 43209.203)]  [G loss: -37818.824] \n",
      "69 [D loss: (-12284.246)(R -67180.859, F 42612.367)]  [G loss: -38442.996] \n",
      "69 [D loss: (-11774.445)(R -66840.492, F 43291.602)]  [G loss: -39098.332] \n",
      "69 [D loss: (-10364.730)(R -64136.453, F 43406.992)]  [G loss: -39437.555] \n",
      "70 [D loss: (-11369.598)(R -66519.508, F 43780.312)]  [G loss: -39994.562] \n",
      "70 [D loss: (-11007.064)(R -65873.445, F 43859.316)]  [G loss: -40710.012] \n",
      "70 [D loss: (-11516.387)(R -68764.727, F 45731.953)]  [G loss: -41292.074] \n",
      "71 [D loss: (-12195.568)(R -69771.281, F 45380.145)]  [G loss: -41687.805] \n",
      "71 [D loss: (-12354.693)(R -71528.758, F 46819.371)]  [G loss: -42516.328] \n",
      "71 [D loss: (-12542.445)(R -71043.594, F 45958.703)]  [G loss: -42954.699] \n",
      "72 [D loss: (-10490.412)(R -68255.312, F 47274.488)]  [G loss: -43379.547] \n",
      "72 [D loss: (-11516.699)(R -71408.758, F 48375.359)]  [G loss: -44074.012] \n",
      "72 [D loss: (-8059.004)(R -64098.266, F 47980.258)]  [G loss: -44397.086] \n",
      "73 [D loss: (-13889.320)(R -76704.359, F 48925.719)]  [G loss: -45263.598] \n",
      "73 [D loss: (-12903.436)(R -74543.719, F 48736.848)]  [G loss: -45905.477] \n",
      "73 [D loss: (-10814.500)(R -71719.367, F 50090.367)]  [G loss: -46189.359] \n",
      "74 [D loss: (-11243.980)(R -72496.906, F 50008.945)]  [G loss: -46742.781] \n",
      "74 [D loss: (-11974.465)(R -74772.312, F 50823.383)]  [G loss: -47311.867] \n",
      "74 [D loss: (-11708.488)(R -75009.555, F 51592.578)]  [G loss: -47820.438] \n",
      "75 [D loss: (-11873.016)(R -76352.680, F 52606.648)]  [G loss: -48442.633] \n",
      "75 [D loss: (-11170.256)(R -74645.000, F 52304.488)]  [G loss: -49096.738] \n",
      "75 [D loss: (-12564.625)(R -78892.898, F 53763.648)]  [G loss: -49547.812] \n",
      "76 [D loss: (-10166.943)(R -73993.891, F 53660.004)]  [G loss: -49787.305] \n",
      "76 [D loss: (-11084.840)(R -76040.391, F 53870.711)]  [G loss: -50422.477] \n",
      "76 [D loss: (-11430.734)(R -77154.953, F 54293.484)]  [G loss: -50817.113] \n",
      "77 [D loss: (-12429.992)(R -80574.859, F 55714.875)]  [G loss: -51447.602] \n",
      "77 [D loss: (-10777.480)(R -76537.359, F 54982.398)]  [G loss: -52117.070] \n",
      "77 [D loss: (-9984.773)(R -76691.875, F 56722.328)]  [G loss: -52471.547] \n",
      "78 [D loss: (-13067.104)(R -82592.141, F 56457.934)]  [G loss: -52927.695] \n",
      "78 [D loss: (-12606.383)(R -81766.234, F 56553.469)]  [G loss: -53439.609] \n",
      "78 [D loss: (-11942.689)(R -80858.508, F 56973.129)]  [G loss: -54162.355] \n",
      "79 [D loss: (-11908.830)(R -81481.250, F 57663.590)]  [G loss: -54613.906] \n",
      "79 [D loss: (-11828.705)(R -82458.547, F 58801.137)]  [G loss: -55106.121] \n",
      "79 [D loss: (-12350.172)(R -83602.938, F 58902.594)]  [G loss: -55643.699] \n",
      "80 [D loss: (-10611.857)(R -80681.555, F 59457.840)]  [G loss: -55995.816] \n",
      "80 [D loss: (-11733.195)(R -83910.258, F 60443.867)]  [G loss: -56568.719] \n",
      "80 [D loss: (-11661.484)(R -84134.422, F 60811.453)]  [G loss: -57157.852] \n",
      "81 [D loss: (-10031.996)(R -81555.016, F 61491.023)]  [G loss: -57368.184] \n",
      "81 [D loss: (-10430.092)(R -82801.688, F 61941.504)]  [G loss: -57744.594] \n",
      "81 [D loss: (-14481.996)(R -91198.844, F 62234.852)]  [G loss: -58545.703] \n",
      "82 [D loss: (-11236.615)(R -85962.375, F 63489.145)]  [G loss: -58909.426] \n",
      "82 [D loss: (-11456.619)(R -86448.641, F 63535.402)]  [G loss: -59590.047] \n",
      "82 [D loss: (-11173.539)(R -85545.797, F 63198.719)]  [G loss: -59950.727] \n",
      "83 [D loss: (-11103.469)(R -84843.641, F 62636.703)]  [G loss: -60298.047] \n",
      "83 [D loss: (-12446.484)(R -89283.156, F 64390.188)]  [G loss: -60852.863] \n",
      "83 [D loss: (-12908.598)(R -90763.875, F 64946.680)]  [G loss: -61386.406] \n",
      "84 [D loss: (-11643.232)(R -88134.141, F 64847.676)]  [G loss: -61874.477] \n",
      "84 [D loss: (-11556.906)(R -88793.953, F 65680.141)]  [G loss: -62243.613] \n",
      "84 [D loss: (-10662.555)(R -87192.547, F 65867.438)]  [G loss: -62649.703] \n",
      "85 [D loss: (-11621.602)(R -90540.039, F 67296.836)]  [G loss: -63165.910] \n",
      "85 [D loss: (-12962.461)(R -94020.531, F 68095.609)]  [G loss: -63767.883] \n",
      "85 [D loss: (-12401.617)(R -91983.672, F 67180.438)]  [G loss: -64152.332] \n",
      "86 [D loss: (-9315.660)(R -87461.953, F 68830.633)]  [G loss: -64395.137] \n",
      "86 [D loss: (-10921.883)(R -90189.078, F 68345.312)]  [G loss: -64742.316] \n",
      "86 [D loss: (-11315.695)(R -90974.547, F 68343.156)]  [G loss: -65236.812] \n",
      "87 [D loss: (-9777.051)(R -88339.500, F 68785.398)]  [G loss: -65511.414] \n",
      "87 [D loss: (-11968.836)(R -93270.672, F 69333.000)]  [G loss: -66289.555] \n",
      "87 [D loss: (-9647.547)(R -89600.398, F 70305.305)]  [G loss: -66626.570] \n",
      "88 [D loss: (-12098.480)(R -95091.297, F 70894.336)]  [G loss: -66750.477] \n",
      "88 [D loss: (-11478.910)(R -94641.453, F 71683.633)]  [G loss: -67196.914] \n",
      "88 [D loss: (-12913.328)(R -97285.570, F 71458.914)]  [G loss: -67531.266] \n",
      "89 [D loss: (-10444.340)(R -92726.891, F 71838.211)]  [G loss: -68068.688] \n",
      "89 [D loss: (-12586.660)(R -96638.562, F 71465.242)]  [G loss: -68764.359] \n",
      "89 [D loss: (-9914.305)(R -92336.523, F 72507.914)]  [G loss: -68780.250] \n",
      "90 [D loss: (-12164.109)(R -97520.242, F 73192.023)]  [G loss: -69167.828] \n",
      "90 [D loss: (-11048.398)(R -95084.953, F 72988.156)]  [G loss: -69678.195] \n",
      "90 [D loss: (-12223.930)(R -98583.906, F 74136.047)]  [G loss: -70134.102] \n",
      "91 [D loss: (-10269.719)(R -94828.289, F 74288.852)]  [G loss: -70131.938] \n",
      "91 [D loss: (-10859.930)(R -96173.453, F 74453.594)]  [G loss: -70675.984] \n",
      "91 [D loss: (-11235.180)(R -96274.703, F 73804.344)]  [G loss: -71021.578] \n",
      "92 [D loss: (-13262.914)(R -101259.703, F 74733.875)]  [G loss: -71323.188] \n",
      "92 [D loss: (-11615.594)(R -97435.891, F 74204.703)]  [G loss: -71619.664] \n",
      "92 [D loss: (-11069.953)(R -97930.953, F 75791.047)]  [G loss: -71950.141] \n",
      "93 [D loss: (-10200.812)(R -95540.000, F 75138.375)]  [G loss: -71897.320] \n",
      "93 [D loss: (-11028.793)(R -97124.938, F 75067.352)]  [G loss: -72392.953] \n",
      "93 [D loss: (-12522.254)(R -101285.945, F 76241.438)]  [G loss: -72774.289] \n",
      "94 [D loss: (-12511.105)(R -101492.195, F 76469.984)]  [G loss: -73083.078] \n",
      "94 [D loss: (-10269.008)(R -97950.977, F 77412.961)]  [G loss: -73556.648] \n",
      "94 [D loss: (-11066.496)(R -98660.422, F 76527.430)]  [G loss: -73511.953] \n",
      "95 [D loss: (-10618.555)(R -97549.016, F 76311.906)]  [G loss: -74034.117] \n",
      "95 [D loss: (-11255.551)(R -101050.484, F 78539.383)]  [G loss: -74419.633] \n",
      "95 [D loss: (-11421.281)(R -100283.719, F 77441.156)]  [G loss: -74581.609] \n",
      "96 [D loss: (-8915.613)(R -96000.336, F 78169.109)]  [G loss: -74538.516] \n",
      "96 [D loss: (-12917.480)(R -104883.133, F 79048.172)]  [G loss: -75215.711] \n",
      "96 [D loss: (-12389.664)(R -102922.453, F 78143.125)]  [G loss: -75390.102] \n",
      "97 [D loss: (-10159.918)(R -99537.352, F 79217.516)]  [G loss: -75276.609] \n",
      "97 [D loss: (-10015.363)(R -99574.484, F 79543.758)]  [G loss: -75819.977] \n",
      "97 [D loss: (-11451.727)(R -103332.320, F 80428.867)]  [G loss: -76169.102] \n",
      "98 [D loss: (-12018.816)(R -103159.922, F 79122.289)]  [G loss: -76335.781] \n",
      "98 [D loss: (-9726.574)(R -100044.195, F 80591.047)]  [G loss: -76318.820] \n",
      "98 [D loss: (-9808.898)(R -99329.953, F 79712.156)]  [G loss: -76517.195] \n",
      "99 [D loss: (-8782.695)(R -98105.234, F 80539.844)]  [G loss: -76443.445] \n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"深層学習_最終課題\")\n",
    "\n",
    "d_history = []\n",
    "g_history = []\n",
    "save_fig_path = os.path.abspath(\"gan_pokemon\")\n",
    "save_model_path = os.path.abspath(\"model\")\n",
    "\n",
    "\n",
    "def train(data, n_epochs=6000, batch_size=256, crip_threshold=0.01):\n",
    "    \n",
    "    batch_per_epoch = int(data.shape[0] / batch_size)\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = -np.ones((batch_size, 1))\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        for i in range(batch_per_epoch):\n",
    "            \n",
    "            for j in range(2):\n",
    "                idx = np.random.randint(0, len(data[0]), batch_size)\n",
    "                true_imgs = data[idx]\n",
    "\n",
    "                z = np.random.normal(0, 1, (batch_size, z_dims))\n",
    "                gen_imgs = gen.predict(z)\n",
    "\n",
    "                d_loss_real = disc.train_on_batch(true_imgs, valid)\n",
    "                d_loss_fake = disc.train_on_batch(gen_imgs, fake)\n",
    "                d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "\n",
    "                for l in disc.layers:\n",
    "                    weights = l.get_weights()\n",
    "                    weights = [np.clip(w, -crip_threshold, crip_threshold) for w in weights]\n",
    "                    l.set_weights(weights)\n",
    "            \n",
    "\n",
    "            z = np.random.normal(0, 1, (batch_size, z_dims))\n",
    "            g_loss = gan.train_on_batch(z, valid)\n",
    "            g_history.append(g_loss)\n",
    "            \n",
    "            print (\"%d [D loss: (%.3f)(R %.3f, F %.3f)]  [G loss: %.3f] \" % (epoch, d_loss, d_loss_real, d_loss_fake, g_loss))\n",
    "            \n",
    "            wandb.log({\"d_loss\": d_loss,\n",
    "                       \"g_loss\": g_loss})\n",
    "        \n",
    "        if epoch % 60 == 0:\n",
    "            sample_images(epoch)\n",
    "            save_model(epoch)\n",
    "    \n",
    "def sample_images(epoch):\n",
    "    r, c = 5, 5\n",
    "    z = np.random.normal(0, 1, (25, z_dims))\n",
    "    gen_imgs = gen.predict(z)\n",
    "\n",
    "\n",
    "    gen_imgs = 0.5 * (gen_imgs + 1)\n",
    "    gen_imgs = np.clip(gen_imgs, 0, 1)\n",
    "\n",
    "    fig, axs = plt.subplots(r, c, figsize=(15,15))\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(np.squeeze(gen_imgs[cnt, :,:,:]))\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(os.path.join(save_fig_path, f\"epoch_{epoch}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def save_model(epoch):\n",
    "    save_folder = os.path.join(save_model_path, f\"model_epoch{epoch}\")\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    disc.save(os.path.join(save_folder, 'discriminator.h5'))\n",
    "    gen.save(os.path.join(save_folder, 'generator.h5'))\n",
    "    gan.save(os.path.join(save_folder, 'gan.h5'))\n",
    "    \n",
    "train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
